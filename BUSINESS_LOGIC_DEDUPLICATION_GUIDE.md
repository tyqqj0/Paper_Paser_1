# ä¸šåŠ¡é€»è¾‘å»é‡ç³»ç»Ÿè¯¦ç»†æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»æ–‡çŒ®è§£æç³»ç»Ÿçš„ä¸šåŠ¡é€»è¾‘å»é‡åŠŸèƒ½ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å®Œå…¨ä¸šåŠ¡é€»è¾‘å»é‡çš„æ–¹æ¡ˆï¼Œç§»é™¤äº†æ•°æ®åº“å±‚é¢çš„å”¯ä¸€çº¦æŸï¼Œé€šè¿‡æ™ºèƒ½çš„ç€‘å¸ƒæµå»é‡ç­–ç•¥ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ã€‚

## ğŸ¯ è®¾è®¡ç†å¿µ

### æ ¸å¿ƒåŸåˆ™

1. **å¼‚æ­¥ä¼˜å…ˆ**: APIç«‹å³å“åº”ï¼Œæ‰€æœ‰å¤æ‚é€»è¾‘åœ¨åå°æ‰§è¡Œ
2. **ä¸šåŠ¡é€»è¾‘æ§åˆ¶**: å®Œå…¨ä¾èµ–ä¸šåŠ¡ä»£ç è¿›è¡Œå»é‡ï¼Œä¸ä¾èµ–æ•°æ®åº“çº¦æŸ
3. **ç€‘å¸ƒæµç­–ç•¥**: å¤šå±‚æ¬¡ã€å¤šç»´åº¦çš„å»é‡æ£€æŸ¥
4. **å¹¶å‘å®‰å…¨**: æ­£ç¡®å¤„ç†é«˜å¹¶å‘åœºæ™¯ä¸‹çš„ç«æ€æ¡ä»¶

### ä¼˜åŠ¿å¯¹æ¯”

| æ–¹é¢ | æ•°æ®åº“çº¦æŸå»é‡ | ä¸šåŠ¡é€»è¾‘å»é‡ |
|------|----------------|--------------|
| **æ€§èƒ½** | å†™å…¥æ—¶å¯èƒ½é˜»å¡ | é«˜å¹¶å‘å‹å¥½ |
| **çµæ´»æ€§** | çº¦æŸå›ºå®š | å¯åŠ¨æ€è°ƒæ•´ç­–ç•¥ |
| **é”™è¯¯å¤„ç†** | æŠ›å‡ºå¼‚å¸¸ | ä¼˜é›…é™çº§ |
| **å¤æ‚åœºæ™¯** | éš¾ä»¥å¤„ç† | æ”¯æŒå¤æ‚é€»è¾‘ |
| **ç»´æŠ¤æ€§** | æ•°æ®åº“ä¾èµ– | ä»£ç å¯æ§ |

## ğŸŒŠ ç€‘å¸ƒæµå»é‡ç­–ç•¥

### å››é˜¶æ®µå»é‡æµç¨‹

```mermaid
flowchart TD
    A[å¼€å§‹å»é‡] --> B[Phase 1: æ˜¾å¼æ ‡è¯†ç¬¦æ£€æŸ¥]
    B --> C{å‘ç°é‡å¤?}
    C -->|æ˜¯| D[è¿”å›é‡å¤ç»“æœ]
    C -->|å¦| E[Phase 2: æºURLæ£€æŸ¥]
    E --> F{å‘ç°é‡å¤?}
    F -->|æ˜¯| D
    F -->|å¦| G[Phase 3: å¤„ç†çŠ¶æ€æ£€æŸ¥]
    G --> H{æ­£åœ¨å¤„ç†?}
    H -->|æ˜¯| D
    H -->|å¦| I[Phase 4: å†…å®¹æŒ‡çº¹æ£€æŸ¥]
    I --> J{å‘ç°é‡å¤?}
    J -->|æ˜¯| D
    J -->|å¦| K[åˆ›å»ºæ–°æ–‡çŒ®]
```

### Phase 1: æ˜¾å¼æ ‡è¯†ç¬¦å»é‡

**æ£€æŸ¥å¯¹è±¡**: DOI, ArXiv ID, PMIDç­‰æƒå¨æ ‡è¯†ç¬¦

**å®ç°ä½ç½®**: `worker/deduplication.py` - `_check_explicit_identifiers()`

**é€»è¾‘**:
```python
async def _check_explicit_identifiers(self, source_data: Dict[str, Any]) -> Optional[str]:
    identifiers, _ = extract_authoritative_identifiers(source_data)
    
    # DOIæ£€æŸ¥
    if identifiers.doi:
        literature = await self.dao.find_by_doi(identifiers.doi)
        if literature:
            return self._handle_existing_literature(literature)
    
    # ArXiv IDæ£€æŸ¥
    if identifiers.arxiv_id:
        literature = await self.dao.find_by_arxiv_id(identifiers.arxiv_id)
        if literature:
            return self._handle_existing_literature(literature)
    
    return None
```

### Phase 2: æºURLå»é‡

**æ£€æŸ¥å¯¹è±¡**: å„ç§å½¢å¼çš„æºURL

**å®ç°ä½ç½®**: `worker/deduplication.py` - `_check_source_urls()`

**æ”¯æŒçš„URLç±»å‹**:
- ArXiv URL (`https://arxiv.org/abs/...`)
- DOI URL (`https://doi.org/...`)
- æœŸåˆŠé¡µé¢URL
- PDFç›´é“¾

### Phase 3: å¤„ç†çŠ¶æ€æ£€æŸ¥

**ç›®çš„**: é˜²æ­¢å¹¶å‘å¤„ç†åŒä¸€æ–‡çŒ®

**å®ç°ä½ç½®**: `worker/deduplication.py` - `_check_processing_state()`

**æ£€æŸ¥é€»è¾‘**:
- æŸ¥æ‰¾çŠ¶æ€ä¸º `pending`, `processing`, `in_progress` çš„æ–‡çŒ®
- åŸºäºDOIã€ArXiv IDç­‰æ ‡è¯†ç¬¦åŒ¹é…

### Phase 4: å†…å®¹æŒ‡çº¹å»é‡

**æ£€æŸ¥å¯¹è±¡**: PDFå†…å®¹MD5ã€æ ‡é¢˜+ä½œè€…æŒ‡çº¹

**å®ç°ä½ç½®**: `worker/deduplication.py` - `_check_content_fingerprint()`

**ç‰¹ç‚¹**:
- éœ€è¦å…ˆä¸‹è½½å’Œè§£æPDF
- ç”Ÿæˆå¤šç§æŒ‡çº¹è¿›è¡ŒåŒ¹é…
- æ”¯æŒæ¨¡ç³ŠåŒ¹é…

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | æ–‡ä»¶è·¯å¾„ | ä¸»è¦åŠŸèƒ½ |
|------|----------|----------|
| **WaterfallDeduplicator** | `worker/deduplication.py` | ç€‘å¸ƒæµå»é‡æ ¸å¿ƒé€»è¾‘ |
| **LiteratureDAO** | `db/dao.py` | æ•°æ®åº“æŸ¥è¯¢æ“ä½œ |
| **ä»»åŠ¡ç¼–æ’** | `worker/tasks.py` | å»é‡æµç¨‹é›†æˆ |
| **APIå±‚** | `web/api/literature.py` | å¼‚æ­¥ä»»åŠ¡åˆ›å»º |

### æ•°æ®åº“ç´¢å¼•è®¾è®¡

**ä¼˜åŒ–å‰** (å­˜åœ¨å”¯ä¸€çº¦æŸé—®é¢˜):
```javascript
doi_unique_index: {"identifiers.doi": 1} [UNIQUE] [PARTIAL]
arxiv_unique_index: {"identifiers.arxiv_id": 1} [UNIQUE] [PARTIAL]
fingerprint_unique_index: {"identifiers.fingerprint": 1} [UNIQUE] [PARTIAL]
```

**ä¼˜åŒ–å** (çº¯æŸ¥è¯¢ç´¢å¼•):
```javascript
// æ ¸å¿ƒæŸ¥è¯¢ç´¢å¼• (æ— å”¯ä¸€çº¦æŸ)
doi_query_index: {"identifiers.doi": 1} [PARTIAL]
arxiv_query_index: {"identifiers.arxiv_id": 1} [PARTIAL]
fingerprint_query_index: {"identifiers.fingerprint": 1} [PARTIAL]
task_id_query_index: {"task_info.task_id": 1}
title_text_search_index: {"metadata.title": "text"}
```

## ğŸ”§ é…ç½®å’Œéƒ¨ç½²

### ç´¢å¼•ä¼˜åŒ–è„šæœ¬

```bash
# ç§»é™¤å”¯ä¸€çº¦æŸï¼Œåˆ›å»ºæŸ¥è¯¢ç´¢å¼•
python scripts/optimize_business_logic_indexes.py

# ç®€åŒ–ç´¢å¼•ç»“æ„ï¼Œåªä¿ç•™æ ¸å¿ƒç´¢å¼•
python scripts/simplify_index_structure.py
```

### ç¯å¢ƒå˜é‡

æ— éœ€é¢å¤–é…ç½®ï¼Œä½¿ç”¨ç°æœ‰çš„æ•°æ®åº“å’ŒRedisé…ç½®å³å¯ã€‚

## ğŸ§ª æµ‹è¯•éªŒè¯

### è‡ªåŠ¨åŒ–æµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´çš„å»é‡æµ‹è¯•
python3 test_business_logic_deduplication.py
```

**æµ‹è¯•è¦†ç›–**:
- âœ… DOIå»é‡æµ‹è¯•
- âœ… ArXiv IDå»é‡æµ‹è¯•  
- âœ… å¹¶å‘æäº¤æµ‹è¯•
- âœ… è·¨æ ‡è¯†ç¬¦å»é‡æµ‹è¯•

### æµ‹è¯•ç»“æœç¤ºä¾‹

```
ğŸš€ å¼€å§‹ä¸šåŠ¡é€»è¾‘å»é‡å…¨é¢æµ‹è¯•...
============================================================

ğŸ”¬ æµ‹è¯•DOIå»é‡...
âœ… DOIå»é‡: PASS
   æˆåŠŸæ£€æµ‹åˆ°DOIé‡å¤

ğŸ”¬ æµ‹è¯•ArXiv IDå»é‡...
âœ… ArXivå»é‡: PASS
   æˆåŠŸæ£€æµ‹åˆ°ArXivé‡å¤

ğŸ”¬ æµ‹è¯•å¹¶å‘æäº¤...
âœ… å¹¶å‘æäº¤: PASS
   æ­£ç¡®å¤„ç†å¹¶å‘: 1ä¸ªåˆ›å»º, 2ä¸ªé‡å¤

ğŸ”¬ æµ‹è¯•è·¨æ ‡è¯†ç¬¦å»é‡...
âœ… è·¨æ ‡è¯†ç¬¦å»é‡: PASS
   æˆåŠŸæ£€æµ‹åˆ°è·¨æ ‡è¯†ç¬¦é‡å¤ (DOI vs ArXiv)

============================================================
ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:
âœ… é€šè¿‡: 4
âŒ å¤±è´¥: 0
âš ï¸  è­¦å‘Š: 0
ğŸ“ˆ æ€»è®¡: 4

ğŸ‰ æ‰€æœ‰æ ¸å¿ƒæµ‹è¯•é€šè¿‡ï¼ä¸šåŠ¡é€»è¾‘å»é‡å·¥ä½œæ­£å¸¸ï¼
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### å…³é”®æŒ‡æ ‡

1. **å»é‡å‡†ç¡®ç‡**: åº”æ¥è¿‘100%
2. **å“åº”æ—¶é—´**: APIå“åº” < 100ms
3. **å¤„ç†æ—¶é—´**: åå°å»é‡ < 30s
4. **å¹¶å‘å¤„ç†**: æ”¯æŒé«˜å¹¶å‘æäº¤

### ç›‘æ§æ–¹æ³•

```python
# åœ¨ä»£ç ä¸­æ·»åŠ ç›‘æ§ç‚¹
import time
from loguru import logger

async def deduplicate_literature(self, source_data):
    start_time = time.time()
    
    # æ‰§è¡Œå»é‡é€»è¾‘
    result = await self._execute_deduplication(source_data)
    
    # è®°å½•æ€§èƒ½æŒ‡æ ‡
    duration = time.time() - start_time
    logger.info(f"å»é‡å®Œæˆ: è€—æ—¶{duration:.2f}s, ç»“æœ: {result}")
    
    return result
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **é‡å¤æ–‡çŒ®æœªè¢«æ£€æµ‹åˆ°**
   - æ£€æŸ¥æ ‡è¯†ç¬¦æå–é€»è¾‘
   - éªŒè¯æ•°æ®åº“ç´¢å¼•çŠ¶æ€
   - æŸ¥çœ‹å»é‡æ—¥å¿—

2. **å¹¶å‘å¤„ç†å¼‚å¸¸**
   - æ£€æŸ¥Redisè¿æ¥çŠ¶æ€
   - éªŒè¯ä»»åŠ¡é˜Ÿåˆ—é…ç½®
   - æŸ¥çœ‹Workeræ—¥å¿—

3. **æ€§èƒ½é—®é¢˜**
   - æ£€æŸ¥æ•°æ®åº“ç´¢å¼•æ•ˆç‡
   - ç›‘æ§æŸ¥è¯¢æ‰§è¡Œæ—¶é—´
   - ä¼˜åŒ–å»é‡ç­–ç•¥

### è°ƒè¯•å·¥å…·

```bash
# æŸ¥çœ‹å»é‡è¯¦ç»†æ—¥å¿—
docker logs paper_paser_1-worker-1 | grep "deduplication"

# æ£€æŸ¥æ•°æ®åº“ç´¢å¼•çŠ¶æ€
python scripts/check_index_status.py

# æ‰‹åŠ¨æµ‹è¯•å»é‡é€»è¾‘
python -c "
from literature_parser_backend.worker.deduplication import WaterfallDeduplicator
# æµ‹è¯•ä»£ç ...
"
```

## ğŸš€ æœ€ä½³å®è·µ

### å¼€å‘å»ºè®®

1. **æ‰©å±•å»é‡ç­–ç•¥**
   ```python
   # åœ¨WaterfallDeduplicatorä¸­æ·»åŠ æ–°çš„æ£€æŸ¥é˜¶æ®µ
   async def _check_custom_logic(self, source_data):
       # è‡ªå®šä¹‰å»é‡é€»è¾‘
       pass
   ```

2. **æ€§èƒ½ä¼˜åŒ–**
   - ç¼“å­˜å¸¸ç”¨æŸ¥è¯¢ç»“æœ
   - æ‰¹é‡å¤„ç†ç›¸ä¼¼è¯·æ±‚
   - å¼‚æ­¥å¹¶è¡Œæ£€æŸ¥

3. **ç›‘æ§é›†æˆ**
   - æ·»åŠ PrometheusæŒ‡æ ‡
   - é›†æˆAPMå·¥å…·
   - è®¾ç½®å‘Šè­¦è§„åˆ™

### è¿ç»´å»ºè®®

1. **å®šæœŸç»´æŠ¤**
   ```bash
   # æ¸…ç†å¤±è´¥çš„æ–‡çŒ®è®°å½•
   python scripts/cleanup_failed_literature.py
   
   # é‡å»ºç´¢å¼•
   python scripts/rebuild_indexes.py
   ```

2. **å¤‡ä»½ç­–ç•¥**
   - å®šæœŸå¤‡ä»½MongoDBæ•°æ®
   - ä¿ç•™å»é‡æ—¥å¿—
   - ç›‘æ§å­˜å‚¨ä½¿ç”¨é‡

## ğŸ“ˆ æœªæ¥æ‰©å±•

### è®¡åˆ’åŠŸèƒ½

1. **æ™ºèƒ½å»é‡**
   - åŸºäºæœºå™¨å­¦ä¹ çš„ç›¸ä¼¼åº¦æ£€æµ‹
   - è¯­ä¹‰ç›¸ä¼¼æ€§åˆ†æ
   - è‡ªåŠ¨åˆå¹¶é‡å¤æ–‡çŒ®

2. **æ€§èƒ½ä¼˜åŒ–**
   - åˆ†å¸ƒå¼å»é‡ç¼“å­˜
   - é¢„è®¡ç®—æŒ‡çº¹ç´¢å¼•
   - å®æ—¶å»é‡å»ºè®®

3. **ç®¡ç†åŠŸèƒ½**
   - å»é‡è§„åˆ™é…ç½®ç•Œé¢
   - é‡å¤æ–‡çŒ®ç®¡ç†å·¥å…·
   - å»é‡ç»Ÿè®¡æŠ¥è¡¨

### æ‰©å±•æ¥å£

```python
class DeduplicationStrategy:
    """å»é‡ç­–ç•¥æ¥å£"""
    
    async def check_duplicate(self, source_data: Dict) -> Optional[str]:
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        raise NotImplementedError
    
    def get_priority(self) -> int:
        """è·å–ç­–ç•¥ä¼˜å…ˆçº§"""
        return 0

# æ³¨å†Œè‡ªå®šä¹‰ç­–ç•¥
deduplicator.register_strategy(CustomStrategy())
```
