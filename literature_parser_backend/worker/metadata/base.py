"""
Base classes and interfaces for metadata processors.

Defines the unified interface that all metadata processors must implement.
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, List, Optional
from enum import Enum

from ...models.literature import MetadataModel

logger = logging.getLogger(__name__)


class ProcessorType(Enum):
    """Type of metadata processor."""
    API = "api"                    # External API (CrossRef, Semantic Scholar)
    SITE_PARSER = "site_parser"    # Website content parsing
    PDF_PARSER = "pdf_parser"      # PDF content extraction
    FALLBACK = "fallback"          # Last resort fallback


# ç§»é™¤äº†PaperTypeæšä¸¾ï¼Œæ”¹ç”¨ç®€å•çš„å¿…éœ€å­—æ®µæ£€æŸ¥


@dataclass
class IdentifierData:
    """Standardized input data for processors."""
    # Primary identifiers
    doi: Optional[str] = None
    arxiv_id: Optional[str] = None
    pmid: Optional[str] = None
    semantic_scholar_id: Optional[str] = None
    
    # URL-based identifiers
    url: Optional[str] = None
    pdf_url: Optional[str] = None
    
    # Extracted metadata (from URL mapping)
    title: Optional[str] = None
    year: Optional[int] = None
    venue: Optional[str] = None
    authors: Optional[List[str]] = None  # ğŸ†• æ·»åŠ ä½œè€…å­—æ®µæ”¯æŒ
    
    # Additional context
    source_data: Optional[Dict[str, Any]] = None
    pdf_content: Optional[bytes] = None
    file_path: Optional[str] = None  # Local file path


@dataclass
class ProcessorResult:
    """Standardized output from processors."""
    success: bool
    metadata: Optional[MetadataModel] = None
    raw_data: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    confidence: float = 0.0  # 0.0 to 1.0
    source: str = ""
    new_identifiers: Optional[Dict[str, str]] = None  # æ‰¿è½½æ–°å‘ç°çš„æ ‡è¯†ç¬¦
    
    @property
    def is_valid(self) -> bool:
        """Check if result contains valid metadata."""
        return self.success and self.metadata is not None
    
    def extract_new_identifiers(self) -> Dict[str, str]:
        """
        ä»metadataä¸­æå–æ–°å‘ç°çš„æ ‡è¯†ç¬¦ï¼Œç‰¹åˆ«æ˜¯DOIã€‚
        
        Returns:
            Dict[str, str]: æ–°å‘ç°çš„æ ‡è¯†ç¬¦å­—å…¸
        """
        if not self.metadata:
            return {}
        
        identifiers = {}
        
        # æå–DOI
        if hasattr(self.metadata, 'doi') and self.metadata.doi:
            doi_value = self.metadata.doi.strip()
            if doi_value and doi_value.lower() != 'none':
                identifiers['doi'] = doi_value
        
        # æå–ArXiv IDï¼ˆå¦‚æœæœ‰external_idsï¼‰
        if hasattr(self.metadata, 'external_ids') and self.metadata.external_ids:
            if isinstance(self.metadata.external_ids, dict):
                arxiv_id = self.metadata.external_ids.get('ArXiv') or self.metadata.external_ids.get('arxiv')
                if arxiv_id:
                    identifiers['arxiv_id'] = arxiv_id.strip()
        
        # åˆå¹¶å·²æœ‰çš„new_identifiers
        if self.new_identifiers:
            identifiers.update(self.new_identifiers)
        
        return identifiers
    
    def get_parsing_score(self) -> float:
        """
        è®¡ç®—è§£æçš„åˆ†æ•°ï¼ˆ0.0-1.0+ï¼‰
        
        è¯„åˆ†ç®—æ³•ï¼š
        - å¿…è¦å­—æ®µï¼ˆtitle, authors, yearï¼‰ï¼šç¼ºä¸€ä¸ªå°±ä¹˜ä»¥0.3æƒ©ç½šç³»æ•°
        - å¯é€‰å­—æ®µï¼ˆabstract, journal, doiï¼‰ï¼šæœ‰å°±å¥–åŠ±ï¼Œæ²¡æœ‰å°±è½»å¾®æƒ©ç½š
        - è¿”å›æœ€ç»ˆåˆ†æ•°ï¼Œæ»¡åˆ†å¯èƒ½è¶…è¿‡1.0
        
        Returns:
            float: è§£æåˆ†æ•°ï¼Œ0.0è¡¨ç¤ºé›¶åˆ†ï¼ˆæ— æ•ˆï¼‰ï¼Œ>0.0è¡¨ç¤ºæœ‰ä»·å€¼ï¼Œ>=1.0è¡¨ç¤ºæ»¡åˆ†
        """
        if not self.is_valid:
            logger.debug("ğŸ” [è¯„åˆ†] ç»“æœæ— æ•ˆï¼Œè¿”å›0åˆ†")
            return 0.0
            
        metadata = self.metadata
        
        # åŸºç¡€åˆ†æ•°ï¼Œä»1.0å¼€å§‹
        score = 1.0
        
        # === å¿…è¦å­—æ®µæ£€æŸ¥ï¼ˆç¼ºä¸€ä¸ªå°±ä¸¥é‡æƒ©ç½šï¼‰ ===
        required_fields_missing = 0
        
        # æ£€æŸ¥æ ‡é¢˜
        if not (metadata.title and metadata.title.strip() and metadata.title != "Unknown Title"):
            required_fields_missing += 1
            title_info = metadata.title[:50] if metadata.title else "None"
            logger.debug(f"ğŸ” [è¯„åˆ†] æ ‡é¢˜æ— æ•ˆ: '{title_info}'")
        else:
            logger.debug(f"ğŸ” [è¯„åˆ†] æ ‡é¢˜æœ‰æ•ˆ: '{metadata.title[:50]}'")
            
        # æ£€æŸ¥ä½œè€…
        valid_authors = []
        if metadata.authors and len(metadata.authors) > 0:
            valid_authors = [a for a in metadata.authors if a.name and a.name.strip()]
        if not valid_authors:
            required_fields_missing += 1
            author_count = len(metadata.authors) if metadata.authors else 0
            logger.debug(f"ğŸ” [è¯„åˆ†] ä½œè€…æ— æ•ˆ: æ€»æ•°{author_count}ï¼Œæœ‰æ•ˆ0ä¸ª")
        else:
            author_names = [a.name for a in valid_authors[:3]]
            logger.debug(f"ğŸ” [è¯„åˆ†] ä½œè€…æœ‰æ•ˆ: {len(valid_authors)}ä¸ª {author_names}")
                
        # æ£€æŸ¥å¹´ä»½
        valid_year = False
        if metadata.year and str(metadata.year).isdigit():
            year_int = int(metadata.year)
            if 1900 <= year_int <= 2030:  # åˆç†çš„å¹´ä»½èŒƒå›´
                valid_year = True
        if not valid_year:
            required_fields_missing += 1
            logger.debug(f"ğŸ” [è¯„åˆ†] å¹´ä»½æ— æ•ˆ: '{metadata.year}'")
        else:
            logger.debug(f"ğŸ” [è¯„åˆ†] å¹´ä»½æœ‰æ•ˆ: {metadata.year}")
        
        # å¿…è¦å­—æ®µæƒ©ç½šï¼šæ¯ç¼ºä¸€ä¸ªå­—æ®µï¼Œåˆ†æ•°ä¹˜ä»¥0.3
        if required_fields_missing > 0:
            old_score = score
            score *= (0 ** required_fields_missing)
            logger.debug(f"ğŸ” [è¯„åˆ†] å¿…è¦å­—æ®µç¼ºå¤±{required_fields_missing}ä¸ªï¼Œåˆ†æ•°ä»{old_score:.3f}é™è‡³{score:.3f}")
        else:
            logger.debug("ğŸ” [è¯„åˆ†] å¿…è¦å­—æ®µå®Œæ•´ï¼Œä¿æŒåŸºç¡€åˆ†æ•°1.0")
        
        # === å¯é€‰å­—æ®µæ£€æŸ¥ï¼ˆæœ‰å¥–åŠ±ï¼Œæ²¡æœ‰è½»å¾®æƒ©ç½šï¼‰ ===
        optional_bonus = 1.0
        
        # æ£€æŸ¥æ‘˜è¦ï¼ˆæœ‰ä»·å€¼çš„å¯é€‰å­—æ®µï¼‰
        if metadata.abstract and len(metadata.abstract.strip()) > 50:
            optional_bonus *= 1  # 20% å¥–åŠ±
            logger.debug(f"ğŸ” [è¯„åˆ†] æœ‰æ•ˆæ‘˜è¦({len(metadata.abstract.strip())}å­—ç¬¦)")
        else:
            optional_bonus *= 0.5  # 5% æƒ©ç½š
            abstract_len = len(metadata.abstract.strip()) if metadata.abstract else 0
            logger.debug(f"ğŸ” [è¯„åˆ†] æ‘˜è¦ä¸è¶³({abstract_len}å­—ç¬¦)ï¼Œæƒ©ç½š5%")
            
        # æ£€æŸ¥æœŸåˆŠ/ä¼šè®®ï¼ˆæœ‰ä»·å€¼çš„å¯é€‰å­—æ®µï¼‰
        if metadata.journal and metadata.journal.strip():
            optional_bonus *= 1  # 15% å¥–åŠ±
            logger.debug(f"ğŸ” [è¯„åˆ†] æœ‰æœŸåˆŠä¿¡æ¯({metadata.journal[:30]})")
        else:
            optional_bonus *= 0.5  # 5% æƒ©ç½š
            logger.debug("ğŸ” [è¯„åˆ†] æ— æœŸåˆŠä¿¡æ¯ï¼Œæƒ©ç½š5%")
            
        # æ£€æŸ¥æ ‡è¯†ç¬¦ï¼ˆDOIã€ArXiv IDç­‰ - å¯¹åç»­å¤„ç†å™¨å¾ˆé‡è¦ï¼‰
        has_identifiers = False
        identifier_bonus = 1.0
        
        # æ£€æŸ¥DOIï¼ˆä»new_identifiersä¸­è·å–ï¼‰
        doi_found = False
        if self.new_identifiers and self.new_identifiers.get('doi'):
            doi_found = True
            has_identifiers = True
            identifier_bonus *= 4.0  # ğŸ†• æœ‰DOIæ˜¯å·¨å¤§ä¼˜åŠ¿ï¼Œ4å€å¥–åŠ±
            logger.debug(f"ğŸ” [è¯„åˆ†] æœ‰DOI({self.new_identifiers['doi']}) - 4å€å¥–åŠ±ï¼")
        else:
            logger.debug("ğŸ” [è¯„åˆ†] æ— DOI")
        
        # æ£€æŸ¥ArXiv IDï¼ˆä»new_identifiersä¸­è·å–ï¼‰
        arxiv_id_found = False
        if self.new_identifiers and self.new_identifiers.get('arxiv_id'):
            arxiv_id_found = True
            has_identifiers = True
            identifier_bonus *= 4.0  # ğŸ†• æœ‰ArXiv IDä¹Ÿæ˜¯å·¨å¤§ä¼˜åŠ¿ï¼Œ4å€å¥–åŠ±
            logger.debug(f"ğŸ” [è¯„åˆ†] æœ‰ArXiv ID({self.new_identifiers['arxiv_id']}) - 4å€å¥–åŠ±ï¼")
        else:
            logger.debug("ğŸ” [è¯„åˆ†] æ— ArXiv ID")
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•æ ‡è¯†ç¬¦ï¼Œç»™äºˆæƒ©ç½šï¼ˆå› ä¸ºå…¶ä»–å¤„ç†å™¨æ— æ³•å·¥ä½œï¼‰
        if not has_identifiers:
            identifier_bonus *= 0.25   # 75% æƒ©ç½š
            logger.debug("ğŸ” [è¯„åˆ†] æ— ä»»ä½•æ ‡è¯†ç¬¦(DOI/ArXiv)ï¼Œæƒ©ç½š75% - å…¶ä»–å¤„ç†å™¨éš¾ä»¥å·¥ä½œ")
        else:
            logger.debug(f"ğŸ” [è¯„åˆ†] æœ‰é‡è¦æ ‡è¯†ç¬¦ - DOI:{doi_found}, ArXiv:{arxiv_id_found}")
        
        # æœ€ç»ˆåˆ†æ•°
        final_score = score * optional_bonus * identifier_bonus
        logger.debug(f"ğŸ” [è¯„åˆ†] æœ€ç»ˆå¾—åˆ†: {score:.3f} Ã— {optional_bonus:.3f} Ã— {identifier_bonus:.3f} = {final_score:.3f}")
        return final_score
    
    def is_complete_parsing(self, completeness_threshold: float = 0.7) -> bool:
        """
        åˆ¤æ–­è§£ææ˜¯å¦è¶³å¤Ÿå®Œæ•´ï¼Œå¯ä»¥åœæ­¢åç»­å¤„ç†å™¨çš„å°è¯•ã€‚
        
        åŸºäºget_parsing_score()çš„ç»“æœï¼š
        - æ»¡åˆ†(>=1.0) â†’ ç«‹å³åœæ­¢
        - è¾¾åˆ°é˜ˆå€¼ â†’ å¯ä»¥åœæ­¢ï¼Œä½†å¯èƒ½ç»§ç»­å¯»æ‰¾æ›´å¥½çš„
        
        Args:
            completeness_threshold: å®Œæ•´æ€§é˜ˆå€¼ï¼Œé»˜è®¤0.7
            
        Returns:
            bool: Trueè¡¨ç¤ºè§£æè¶³å¤Ÿå®Œæ•´ï¼Œå¯ä»¥åœæ­¢ï¼›Falseè¡¨ç¤ºéœ€è¦ç»§ç»­å°è¯•å…¶ä»–å¤„ç†å™¨
        """
        score = self.get_parsing_score()
        
        # æ»¡åˆ†æƒ…å†µï¼šæ‰€æœ‰å¿…è¦+å¯é€‰å­—æ®µéƒ½æœ‰
        if score >= 1.0:
            return True  # æ»¡åˆ†ç›´æ¥é€šè¿‡
        
        # é«˜ç½®ä¿¡åº¦æ—¶é™ä½è¦æ±‚
        adjusted_threshold = completeness_threshold
        if self.confidence > 0.8:
            adjusted_threshold = max(0.5, completeness_threshold - 0.2)
            
        return score >= adjusted_threshold
    
    def is_zero_score(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé›¶åˆ†ï¼ˆå®Œå…¨æ— æ•ˆçš„ç»“æœï¼‰"""
        return self.get_parsing_score() <= 0.0
    



class MetadataProcessor(ABC):
    """
    Abstract base class for all metadata processors.
    
    All processors (API clients, site parsers, PDF parsers) must implement
    this interface to ensure consistent behavior and easy integration.
    """
    
    def __init__(self, settings=None):
        """Initialize processor with optional settings."""
        self.settings = settings
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Human-readable name of this processor."""
        pass
    
    @property
    @abstractmethod
    def processor_type(self) -> ProcessorType:
        """Type of this processor."""
        pass
    
    @property
    @abstractmethod
    def priority(self) -> int:
        """
        Priority of this processor (lower = higher priority).
        
        Suggested ranges:
        - 1-10: Primary APIs (CrossRef, Semantic Scholar)
        - 11-20: Secondary APIs (arXiv Official)
        - 21-30: Site parsers (NeurIPS, ACM)
        - 31-40: PDF parsers (GROBID)
        - 91-99: Fallbacks
        """
        pass
    
    @abstractmethod
    def can_handle(self, identifiers: IdentifierData) -> bool:
        """
        Check if this processor can handle the given identifiers.
        
        Args:
            identifiers: Standardized identifier data
            
        Returns:
            True if this processor can attempt to fetch metadata
        """
        pass
    
    @abstractmethod
    async def process(self, identifiers: IdentifierData) -> ProcessorResult:
        """
        Process identifiers and return metadata.
        
        Args:
            identifiers: Standardized identifier data
            
        Returns:
            ProcessorResult with success status and metadata
        """
        pass
    
    def __str__(self) -> str:
        """String representation for debugging."""
        return f"{self.name} (priority: {self.priority})"
    
    def __repr__(self) -> str:
        """Detailed representation for debugging."""
        return f"{self.__class__.__name__}(name='{self.name}', priority={self.priority})"
