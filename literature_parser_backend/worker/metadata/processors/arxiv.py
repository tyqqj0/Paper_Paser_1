#!/usr/bin/env python3
"""
ArXivå…ƒæ•°æ®å¤„ç†å™¨ - Paper Parser 0.2

ä¸“é—¨å¤„ç†ArXivè®ºæ–‡çš„å…ƒæ•°æ®è·å–å’Œå¢å¼ºã€‚
æ”¯æŒç›´æ¥è·å–å’Œç°æœ‰metadataçš„å¢å¼ºæ¨¡å¼ã€‚
"""

import logging
from typing import Any, Dict, List, Optional, Tuple

from ....models.literature import AuthorModel, MetadataModel
from ....services.arxiv_api import ArXivAPIClient
from ....utils.title_matching import TitleMatchingUtils
from ..base import IdentifierData, MetadataProcessor, ProcessorResult, ProcessorType

logger = logging.getLogger(__name__)


class ArXivProcessor(MetadataProcessor):
    """
    ArXivå…ƒæ•°æ®å¤„ç†å™¨ã€‚
    
    ä¸“é—¨å¤„ç†ArXivè®ºæ–‡ï¼Œæ”¯æŒç›´æ¥è·å–å’Œå¢å¼ºæ¨¡å¼ã€‚
    ä¼˜å…ˆçº§ï¼š10ï¼ˆä¸­ç­‰ä¼˜å…ˆçº§ï¼Œä¸»è¦ç”¨ä½œå¢å¼ºï¼‰
    """
    
    def __init__(self, settings=None):
        """åˆå§‹åŒ–ArXivå¤„ç†å™¨"""
        super().__init__(settings)
        self.arxiv_client = ArXivAPIClient(settings)
    
    @property
    def name(self) -> str:
        """å¤„ç†å™¨åç§°"""
        return "ArXiv Official API"
    
    @property
    def processor_type(self) -> ProcessorType:
        """å¤„ç†å™¨ç±»å‹"""
        return ProcessorType.API
    
    @property
    def priority(self) -> int:
        """å¤„ç†å™¨ä¼˜å…ˆçº§ï¼ˆä¸­ç­‰ä¼˜å…ˆçº§ï¼Œä¸»è¦ç”¨ä½œå¢å¼ºï¼‰"""
        return 10
    
    def can_handle(self, identifiers: IdentifierData) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†ç»™å®šçš„æ ‡è¯†ç¬¦ã€‚
        
        å¯ä»¥å¤„ç†ArXiv IDæˆ–æ ‡é¢˜æœç´¢ã€‚
        
        Args:
            identifiers: æ ‡å‡†åŒ–çš„æ ‡è¯†ç¬¦æ•°æ®
            
        Returns:
            True if æœ‰ArXiv IDæˆ–æ ‡é¢˜å¯ä»¥å¤„ç†
        """
        return bool(identifiers.arxiv_id) or bool(identifiers.title)
    
    async def process(self, identifiers: IdentifierData) -> ProcessorResult:
        """
        å¤„ç†æ ‡è¯†ç¬¦å¹¶è¿”å›å…ƒæ•°æ®ã€‚
        
        ArXivå¤„ç†å™¨æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        1. ArXiv IDç›´æ¥æŸ¥è¯¢
        2. æ ‡é¢˜æœç´¢ï¼ˆé€‚ç”¨äºNeurIPSç­‰åœºæ™¯ï¼‰
        
        Args:
            identifiers: æ ‡å‡†åŒ–çš„æ ‡è¯†ç¬¦æ•°æ®
            
        Returns:
            ProcessorResult with æˆåŠŸçŠ¶æ€å’Œå…ƒæ•°æ®
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨ArXiv ID
            if identifiers.arxiv_id:
                return await self._process_by_arxiv_id(identifiers)
            elif identifiers.title:
                return await self._process_by_title(identifiers)
            else:
                return ProcessorResult(
                    success=False,
                    error="ArXiv: No ArXiv ID or title provided",
                    source=self.name
                )
            
        except Exception as e:
            logger.error(f"ArXivå¤„ç†å™¨å¼‚å¸¸: {e}")
            return ProcessorResult(
                success=False,
                error=f"ArXivå¤„ç†å™¨å¼‚å¸¸: {str(e)}",
                source=self.name
            )
    
    async def _process_by_arxiv_id(self, identifiers: IdentifierData) -> ProcessorResult:
        """é€šè¿‡ArXiv IDå¤„ç†"""
        logger.info(f"ğŸ” ArXiv APIæŸ¥è¯¢: {identifiers.arxiv_id}")
        
        # è·å–ArXivæ•°æ®
        arxiv_data = self.arxiv_client.get_metadata(identifiers.arxiv_id)
        
        if not arxiv_data:
            return ProcessorResult(
                success=False,
                error="ArXiv: No metadata found",
                source=self.name
            )
        
        # è½¬æ¢ä¸ºæ ‡å‡†MetadataModel
        arxiv_metadata = self.arxiv_client.convert_to_metadata_model(arxiv_data)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰metadataéœ€è¦å¢å¼º
        existing_metadata = self._extract_existing_metadata(identifiers)
        
        if existing_metadata:
            # å¢å¼ºæ¨¡å¼ï¼šåˆå¹¶ç°æœ‰metadataå’ŒArXivæ•°æ®
            enhanced_metadata = self._enhance_metadata(existing_metadata, arxiv_metadata)
            confidence = 0.75  # å¢å¼ºæ¨¡å¼ç½®ä¿¡åº¦ç¨ä½
            logger.info("âœ… ArXivæ•°æ®ç”¨äºå¢å¼ºç°æœ‰metadata")
        else:
            # ç›´æ¥æ¨¡å¼ï¼šä½¿ç”¨ArXivæ•°æ®ä½œä¸ºä¸»è¦æ¥æº
            enhanced_metadata = arxiv_metadata
            confidence = 0.85  # ç›´æ¥ä½¿ç”¨ArXivæ•°æ®ç½®ä¿¡åº¦è¾ƒé«˜
            logger.info("âœ… ArXivæ•°æ®ä½œä¸ºä¸»è¦metadataæ¥æº")
        
        return ProcessorResult(
            success=True,
            metadata=enhanced_metadata,
            raw_data=arxiv_data,
            confidence=confidence,
            source=self.name
        )
    
    async def _process_by_title(self, identifiers: IdentifierData) -> ProcessorResult:
        """é€šè¿‡æ ‡é¢˜æœç´¢ArXiv"""
        logger.info(f"ğŸ” ArXivæ ‡é¢˜æœç´¢: {identifiers.title}")
        
        # æœç´¢ArXiv
        search_results = self.arxiv_client.search_by_title(identifiers.title, max_results=5)
        
        if not search_results:
            return ProcessorResult(
                success=False,
                error="ArXiv: No search results found",
                source=self.name
            )
        
        logger.info(f"ğŸ” ArXivè¿”å›{len(search_results)}ä¸ªå€™é€‰ç»“æœ")
        
        # ä½¿ç”¨æ™ºèƒ½åŒ¹é…æ‰¾åˆ°æœ€ä½³ç»“æœ
        best_match, similarity_score = self._find_best_title_match(
            target_title=identifiers.title,
            target_year=identifiers.year,
            candidates=search_results
        )
        
        if not best_match or similarity_score < 0.7:  # ç›¸å¯¹ä¸¥æ ¼çš„é˜ˆå€¼
            return ProcessorResult(
                success=False,
                error="ArXiv: No results passed similarity filter",
                source=self.name
            )
        
        logger.info(f"âœ… é€‰æ‹©æœ€ä½³åŒ¹é…: ç›¸ä¼¼åº¦={similarity_score:.3f}")
        
        # è½¬æ¢ä¸ºæ ‡å‡†å…ƒæ•°æ®æ ¼å¼
        arxiv_metadata = self.arxiv_client.convert_to_metadata_model(best_match)
        
        # è°ƒæ•´ç½®ä¿¡åº¦ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰
        confidence = min(0.8, similarity_score * 0.7)  # æœç´¢æ¨¡å¼ç½®ä¿¡åº¦ç¨ä½
        
        return ProcessorResult(
            success=True,
            metadata=arxiv_metadata,
            raw_data=best_match,
            confidence=confidence,
            source=self.name
        )
    
    def _extract_existing_metadata(self, identifiers: IdentifierData) -> Optional[MetadataModel]:
        """
        ä»æ ‡è¯†ç¬¦æ•°æ®ä¸­æå–ç°æœ‰çš„metadataï¼ˆå¦‚æœæœ‰ï¼‰ã€‚
        
        è¿™ä¸ªæ–¹æ³•æ£€æŸ¥source_dataä¸­æ˜¯å¦æœ‰ç°æœ‰çš„metadataéœ€è¦å¢å¼ºã€‚
        
        Args:
            identifiers: æ ‡è¯†ç¬¦æ•°æ®
            
        Returns:
            ç°æœ‰çš„MetadataModelæˆ–None
        """
        # æ£€æŸ¥source_dataä¸­æ˜¯å¦æœ‰ç°æœ‰çš„metadata
        if identifiers.source_data:
            # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µæ£€æŸ¥æ˜¯å¦æœ‰é¢„å¤„ç†çš„metadata
            # ç›®å‰ç®€åŒ–å¤„ç†ï¼Œä¸»è¦åŸºäºæ ‡é¢˜å’Œå¹´ä»½åˆ¤æ–­
            if identifiers.title:
                # æ„å»ºä¸€ä¸ªç®€å•çš„ç°æœ‰metadata
                return MetadataModel(
                    title=identifiers.title,
                    year=identifiers.year,
                    authors=[],  # ç©ºçš„ä½œè€…åˆ—è¡¨ï¼Œç­‰å¾…å¢å¼º
                    abstract=None,  # ç©ºçš„æ‘˜è¦ï¼Œç­‰å¾…å¢å¼º
                    journal=identifiers.venue
                )
        
        return None
    
    def _enhance_metadata(
        self, 
        existing_metadata: MetadataModel, 
        arxiv_metadata: MetadataModel
    ) -> MetadataModel:
        """
        ä½¿ç”¨ArXivæ•°æ®å¢å¼ºç°æœ‰metadataã€‚
        
        å¢å¼ºç­–ç•¥ï¼š
        1. å¦‚æœç°æœ‰å­—æ®µä¸ºç©ºï¼Œä½¿ç”¨ArXivæ•°æ®å¡«å……
        2. å¦‚æœç°æœ‰æ ‡é¢˜è´¨é‡å·®ï¼Œæ›¿æ¢ä¸ºArXivæ ‡é¢˜
        3. å§‹ç»ˆä½¿ç”¨ArXivçš„æ‘˜è¦ï¼ˆé€šå¸¸è´¨é‡å¾ˆé«˜ï¼‰
        4. åˆå¹¶å…³é”®è¯
        
        Args:
            existing_metadata: ç°æœ‰çš„metadata
            arxiv_metadata: ArXivçš„metadata
            
        Returns:
            å¢å¼ºåçš„MetadataModel
        """
        logger.info("ğŸ”§ å¼€å§‹å¢å¼ºç°æœ‰metadata...")
        
        # å¤åˆ¶ç°æœ‰metadataä½œä¸ºåŸºç¡€
        enhanced = MetadataModel(
            title=existing_metadata.title,
            authors=existing_metadata.authors.copy() if existing_metadata.authors else [],
            year=existing_metadata.year,
            journal=existing_metadata.journal,
            abstract=existing_metadata.abstract,
            keywords=existing_metadata.keywords.copy() if existing_metadata.keywords else [],
            source_priority=existing_metadata.source_priority.copy() if existing_metadata.source_priority else []
        )
        
        # 1. å¢å¼ºæ ‡é¢˜ï¼ˆå¦‚æœç°æœ‰æ ‡é¢˜è´¨é‡å·®ï¼‰
        if self._needs_title_enhancement(existing_metadata.title, arxiv_metadata.title):
            enhanced.title = arxiv_metadata.title
            logger.info(f"âœ… æ ‡é¢˜å¢å¼º: '{existing_metadata.title}' -> '{arxiv_metadata.title}'")
        
        # 2. å¢å¼ºæ‘˜è¦ï¼ˆArXivæ‘˜è¦é€šå¸¸è´¨é‡å¾ˆé«˜ï¼‰
        if not enhanced.abstract and arxiv_metadata.abstract:
            enhanced.abstract = arxiv_metadata.abstract
            logger.info("âœ… æ‘˜è¦å¢å¼º: æ·»åŠ ArXivæ‘˜è¦")
        elif arxiv_metadata.abstract and len(arxiv_metadata.abstract) > len(enhanced.abstract or ""):
            # å¦‚æœArXivæ‘˜è¦æ›´é•¿ï¼Œå¯èƒ½è´¨é‡æ›´å¥½
            enhanced.abstract = arxiv_metadata.abstract
            logger.info("âœ… æ‘˜è¦å¢å¼º: æ›¿æ¢ä¸ºæ›´è¯¦ç»†çš„ArXivæ‘˜è¦")
        
        # 3. å¢å¼ºä½œè€…ï¼ˆå¦‚æœç°æœ‰ä½œè€…ä¸ºç©ºï¼‰
        if not enhanced.authors and arxiv_metadata.authors:
            enhanced.authors = arxiv_metadata.authors
            logger.info(f"âœ… ä½œè€…å¢å¼º: æ·»åŠ {len(arxiv_metadata.authors)}ä¸ªä½œè€…")
        
        # 4. å¢å¼ºå¹´ä»½ï¼ˆå¦‚æœç°æœ‰å¹´ä»½ä¸ºç©ºï¼‰
        if not enhanced.year and arxiv_metadata.year:
            enhanced.year = arxiv_metadata.year
            logger.info(f"âœ… å¹´ä»½å¢å¼º: æ·»åŠ å¹´ä»½{arxiv_metadata.year}")
        
        # 5. å¢å¼ºæœŸåˆŠä¿¡æ¯ï¼ˆå¦‚æœç°æœ‰æœŸåˆŠä¸ºç©ºï¼Œå¹¶ä¸”ArXivæœ‰æœŸåˆŠå¼•ç”¨ï¼‰
        if not enhanced.journal and arxiv_metadata.journal:
            enhanced.journal = arxiv_metadata.journal
            logger.info(f"âœ… æœŸåˆŠå¢å¼º: æ·»åŠ æœŸåˆŠä¿¡æ¯")
        
        # 6. åˆå¹¶å…³é”®è¯
        if arxiv_metadata.keywords:
            existing_keywords = set(enhanced.keywords)
            new_keywords = [kw for kw in arxiv_metadata.keywords if kw not in existing_keywords]
            if new_keywords:
                enhanced.keywords.extend(new_keywords)
                logger.info(f"âœ… å…³é”®è¯å¢å¼º: æ·»åŠ {len(new_keywords)}ä¸ªæ–°å…³é”®è¯")
        
        # 7. æ›´æ–°source_priority
        enhanced.source_priority.append(f"{self.name} (enhancement)")
        
        return enhanced
    
    def _needs_title_enhancement(self, existing_title: str, arxiv_title: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦æ ‡é¢˜å¢å¼ºã€‚
        
        å¢å¼ºæ¡ä»¶ï¼š
        1. ç°æœ‰æ ‡é¢˜ä¸ºç©ºæˆ–é»˜è®¤å€¼
        2. ç°æœ‰æ ‡é¢˜åŒ…å«"Processing:"ç­‰ä¸´æ—¶æ–‡æœ¬
        3. ç°æœ‰æ ‡é¢˜æ˜æ˜¾è´¨é‡å·®ï¼ˆå¤ªçŸ­ç­‰ï¼‰
        
        Args:
            existing_title: ç°æœ‰æ ‡é¢˜
            arxiv_title: ArXivæ ‡é¢˜
            
        Returns:
            æ˜¯å¦éœ€è¦å¢å¼º
        """
        if not existing_title or not arxiv_title:
            return bool(arxiv_title)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é»˜è®¤æˆ–ä¸´æ—¶æ ‡é¢˜
        poor_title_indicators = [
            "Unknown Title",
            "Processing:",
            "Extracting:",
            "Loading...",
            "Error:",
            "N/A"
        ]
        
        for indicator in poor_title_indicators:
            if indicator in existing_title:
                return True
        
        # æ£€æŸ¥æ ‡é¢˜é•¿åº¦ï¼ˆå¤ªçŸ­å¯èƒ½è´¨é‡å·®ï¼‰
        if len(existing_title.strip()) < 10:
            return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯ç¬¦å·æˆ–æ•°å­—ï¼ˆå¯èƒ½æ˜¯è§£æé”™è¯¯ï¼‰
        import re
        if re.match(r'^[\d\s\-\.]+$', existing_title.strip()):
            return True
        
        return False
    
    def _find_best_title_match(
        self, 
        target_title: str, 
        target_year: Optional[int], 
        candidates: List[Dict[str, Any]]
    ) -> Tuple[Optional[Dict[str, Any]], float]:
        """
        ä»å€™é€‰ç»“æœä¸­æ‰¾åˆ°æœ€ä½³æ ‡é¢˜åŒ¹é…
        
        Args:
            target_title: ç›®æ ‡æ ‡é¢˜
            target_year: ç›®æ ‡å¹´ä»½ï¼ˆå¯é€‰ï¼‰
            candidates: å€™é€‰è®ºæ–‡åˆ—è¡¨
            
        Returns:
            (æœ€ä½³åŒ¹é…çš„è®ºæ–‡, ç›¸ä¼¼åº¦åˆ†æ•°)
        """
        if not candidates:
            return None, 0.0
        
        best_match = None
        best_score = 0.0
        
        for candidate in candidates:
            candidate_title = candidate.get('title', '')
            candidate_year = candidate.get('year')
            
            if not candidate_title:
                continue
            
            # è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
            title_similarity = TitleMatchingUtils.calculate_similarity(
                target_title, 
                candidate_title
            )
            
            # å¹´ä»½åŒ¹é…åŠ åˆ†
            year_bonus = 0.0
            if target_year and candidate_year:
                if target_year == candidate_year:
                    year_bonus = 0.1  # å®Œå…¨åŒ¹é…
                elif abs(target_year - candidate_year) <= 1:
                    year_bonus = 0.05  # ç›¸è¿‘å¹´ä»½
            
            total_score = title_similarity + year_bonus
            
            logger.debug(f"å€™é€‰åŒ¹é…: '{candidate_title}' - ç›¸ä¼¼åº¦: {title_similarity:.3f}, å¹´ä»½åŠ åˆ†: {year_bonus:.3f}, æ€»åˆ†: {total_score:.3f}")
            
            if total_score > best_score:
                best_score = total_score
                best_match = candidate
        
        return best_match, best_score


# è‡ªåŠ¨æ³¨å†Œå¤„ç†å™¨
from ..registry import register_processor
register_processor(ArXivProcessor)


