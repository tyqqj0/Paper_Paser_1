#!/usr/bin/env python3
"""
ç½‘ç«™ç‰¹å®šè§£æå¤„ç†å™¨ - Paper Parser 0.2

æ•´åˆç°æœ‰çš„URLé€‚é…å™¨ç³»ç»Ÿï¼Œä¸ºç‰¹å®šç½‘ç«™ï¼ˆNeurIPSã€ACMã€ArXivç­‰ï¼‰
æä¾›ç›´æ¥çš„å…ƒæ•°æ®è§£æèƒ½åŠ›ã€‚å½“APIå¤±è´¥æ—¶ä½œä¸ºfallbackã€‚
"""

import logging
from typing import Any, Dict, List, Optional, Tuple

from ....models.literature import AuthorModel, MetadataModel
from ....services.url_mapping.core.service import URLMappingService
from ....services.url_mapping.core.result import URLMappingResult
from ..base import IdentifierData, MetadataProcessor, ProcessorResult, ProcessorType

logger = logging.getLogger(__name__)


class SiteParserProcessor(MetadataProcessor):
    """
    ç½‘ç«™ç‰¹å®šè§£æå¤„ç†å™¨ã€‚
    
    åˆ©ç”¨ç°æœ‰çš„URLé€‚é…å™¨ç³»ç»Ÿç›´æ¥ä»ç½‘ç«™é¡µé¢è§£æå…ƒæ•°æ®ã€‚
    ä¼˜å…ˆçº§ï¼š20ï¼ˆä¸­ç­‰åä½ï¼Œä¸»è¦ä½œä¸ºAPIå¤±è´¥åçš„fallbackï¼‰
    """
    
    def __init__(self, settings=None):
        """åˆå§‹åŒ–ç½‘ç«™è§£æå¤„ç†å™¨"""
        super().__init__(settings)
        self.url_mapping_service = URLMappingService()
    
    @property
    def name(self) -> str:
        """å¤„ç†å™¨åç§°"""
        return "Site Parser"
    
    @property
    def processor_type(self) -> ProcessorType:
        """å¤„ç†å™¨ç±»å‹"""
        return ProcessorType.SITE_PARSER
    
    @property
    def priority(self) -> int:
        """å¤„ç†å™¨ä¼˜å…ˆçº§ï¼ˆä¸­ç­‰åä½ï¼Œä¸»è¦ä½œä¸ºfallbackï¼‰"""
        return 20
    
    def can_handle(self, identifiers: IdentifierData) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†ç»™å®šçš„æ ‡è¯†ç¬¦ã€‚
        
        åªæœ‰åœ¨æœ‰URLä¸”ä¸æ˜¯PDF URLçš„æƒ…å†µä¸‹æ‰èƒ½å¤„ç†ã€‚
        
        Args:
            identifiers: æ ‡å‡†åŒ–çš„æ ‡è¯†ç¬¦æ•°æ®
            
        Returns:
            True if æœ‰å¯å¤„ç†çš„URL
        """
        if not identifiers.url:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ”¯æŒçš„ç½‘ç«™URL
        supported_sites = [
            'proceedings.neurips.cc', 'papers.nips.cc',  # NeurIPS
            'dl.acm.org',  # ACM
            'arxiv.org',   # ArXiv
            'openaccess.thecvf.com',  # CVF
            'ieeexplore.ieee.org',  # IEEE
            'www.nature.com', 'nature.com',  # Nature
            'journals.plos.org',  # PLOS
            'science.sciencemag.org',  # Science
            'link.springer.com',  # Springer
            'www.cell.com'  # Cell
        ]
        
        url_lower = identifiers.url.lower()
        is_supported = any(site in url_lower for site in supported_sites)
        
        # æ’é™¤PDF URLï¼ˆPDFè§£æç”±GROBIDå¤„ç†å™¨è´Ÿè´£ï¼‰
        is_pdf = url_lower.endswith('.pdf')
        
        return is_supported and not is_pdf
    
    async def process(self, identifiers: IdentifierData) -> ProcessorResult:
        """
        å¤„ç†æ ‡è¯†ç¬¦å¹¶è¿”å›å…ƒæ•°æ®ã€‚
        
        ä½¿ç”¨URLé€‚é…å™¨ç³»ç»Ÿè§£æç½‘ç«™é¡µé¢ï¼Œæå–æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ç­‰ä¿¡æ¯ã€‚
        
        Args:
            identifiers: æ ‡å‡†åŒ–çš„æ ‡è¯†ç¬¦æ•°æ®
            
        Returns:
            ProcessorResult with æˆåŠŸçŠ¶æ€å’Œå…ƒæ•°æ®
        """
        try:
            if not identifiers.url:
                return ProcessorResult(
                    success=False,
                    error="Site Parser: No URL provided",
                    source=self.name
                )
            
            logger.info(f"ğŸ” ç½‘ç«™è§£æ: {identifiers.url}")
            
            # ä½¿ç”¨URLé€‚é…å™¨ç³»ç»Ÿè§£æURL
            url_mapping_result = await self.url_mapping_service.map_url(
                identifiers.url,
                enable_validation=False,  # è·³è¿‡URLéªŒè¯ä»¥æé«˜é€Ÿåº¦
                skip_url_validation=True
            )
            
            if not url_mapping_result.has_useful_info():
                return ProcessorResult(
                    success=False,
                    error="Site Parser: No useful information extracted from URL",
                    source=self.name
                )
            
            # è½¬æ¢URLMappingResultä¸ºMetadataModel
            metadata = self._convert_url_mapping_to_metadata(url_mapping_result)
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºæå–åˆ°çš„ä¿¡æ¯è´¨é‡ï¼‰
            confidence = self._calculate_confidence(url_mapping_result)
            
            logger.info(f"âœ… ç½‘ç«™è§£ææˆåŠŸ: title='{metadata.title}', venue='{metadata.journal}', confidence={confidence:.2f}")
            
            return ProcessorResult(
                success=True,
                metadata=metadata,
                raw_data=self._url_mapping_result_to_dict(url_mapping_result),
                confidence=confidence,
                source=self.name
            )
            
        except Exception as e:
            logger.error(f"ç½‘ç«™è§£æå¤„ç†å™¨å¼‚å¸¸: {e}")
            return ProcessorResult(
                success=False,
                error=f"ç½‘ç«™è§£æå¤„ç†å™¨å¼‚å¸¸: {str(e)}",
                source=self.name
            )
    
    def _convert_url_mapping_to_metadata(self, url_result: URLMappingResult) -> MetadataModel:
        """
        å°†URLMappingResultè½¬æ¢ä¸ºæ ‡å‡†çš„MetadataModelã€‚
        
        Args:
            url_result: URLæ˜ å°„ç»“æœ
            
        Returns:
            æ ‡å‡†åŒ–çš„MetadataModel
        """
        # æå–æ ‡é¢˜
        title = url_result.title or "Unknown Title"
        
        # æå–ä½œè€…ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        authors = []
        if hasattr(url_result, 'authors') and url_result.authors:
            for author_name in url_result.authors:
                if isinstance(author_name, str) and author_name.strip():
                    authors.append(AuthorModel(name=author_name.strip()))
        
        # æå–å‘è¡¨å¹´ä»½
        year = url_result.year
        
        # æå–æœŸåˆŠ/ä¼šè®®ä¿¡æ¯
        journal = url_result.venue
        
        # æå–æ‘˜è¦ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        abstract = None
        if hasattr(url_result, 'abstract'):
            abstract = url_result.abstract
        elif url_result.metadata.get('abstract'):
            abstract = url_result.metadata['abstract']
        
        # æå–å…³é”®è¯ï¼ˆä»metadataä¸­ï¼‰
        keywords = []
        if url_result.metadata.get('keywords'):
            keywords = url_result.metadata['keywords']
        elif url_result.metadata.get('categories'):
            keywords = url_result.metadata['categories']
        
        return MetadataModel(
            title=title,
            authors=authors,
            year=year,
            journal=journal,
            abstract=abstract,
            keywords=keywords,
            source_priority=[self.name]
        )
    
    def _calculate_confidence(self, url_result: URLMappingResult) -> float:
        """
        åŸºäºæå–ä¿¡æ¯çš„è´¨é‡è®¡ç®—ç½®ä¿¡åº¦ã€‚
        
        Args:
            url_result: URLæ˜ å°„ç»“æœ
            
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•° (0.0 - 1.0)
        """
        confidence = 0.3  # åŸºç¡€åˆ†æ•°
        
        # æœ‰æ ‡é¢˜ +0.3
        if url_result.title and len(url_result.title.strip()) > 10:
            confidence += 0.3
        
        # æœ‰DOI +0.2
        if url_result.doi:
            confidence += 0.2
        
        # æœ‰ArXiv ID +0.15
        if url_result.arxiv_id:
            confidence += 0.15
        
        # æœ‰å¹´ä»½ +0.1
        if url_result.year:
            confidence += 0.1
        
        # æœ‰venue +0.1
        if url_result.venue:
            confidence += 0.1
        
        # æœ‰æ‘˜è¦ +0.1
        if (hasattr(url_result, 'abstract') and url_result.abstract) or \
           url_result.metadata.get('abstract'):
            confidence += 0.1
        
        # ä½¿ç”¨åŸå§‹é€‚é…å™¨çš„ç½®ä¿¡åº¦ä½œä¸ºå‚è€ƒ
        if hasattr(url_result, 'confidence') and url_result.confidence:
            # å–ä¸¤è€…çš„åŠ æƒå¹³å‡
            confidence = (confidence * 0.7) + (url_result.confidence * 0.3)
        
        return min(confidence, 1.0)
    
    def _url_mapping_result_to_dict(self, url_result: URLMappingResult) -> Dict[str, Any]:
        """
        å°†URLMappingResultè½¬æ¢ä¸ºå­—å…¸æ ¼å¼çš„raw_dataã€‚
        
        Args:
            url_result: URLæ˜ å°„ç»“æœ
            
        Returns:
            å­—å…¸æ ¼å¼çš„åŸå§‹æ•°æ®
        """
        return {
            'doi': url_result.doi,
            'arxiv_id': url_result.arxiv_id,
            'pmid': url_result.pmid,
            'title': url_result.title,
            'venue': url_result.venue,
            'year': url_result.year,
            'source_page_url': url_result.source_page_url,
            'pdf_url': url_result.pdf_url,
            'source_adapter': url_result.source_adapter,
            'strategy_used': url_result.strategy_used,
            'confidence': url_result.confidence,
            'identifiers': url_result.identifiers,
            'metadata': url_result.metadata,
            'has_identifiers': url_result.has_identifiers(),
            'has_useful_info': url_result.has_useful_info()
        }


# è‡ªåŠ¨æ³¨å†Œå¤„ç†å™¨
from ..registry import register_processor
register_processor(SiteParserProcessor)


