"""
å¤‡é€‰æ–‡çŒ®å¤„ç†å™¨

ç”¨äºå¤„ç†æ™ºèƒ½è·¯ç”±æ— æ³•å¤„ç†çš„æƒ…å†µï¼Œå¦‚çº¯DOIã€ArXiv IDç­‰æ ‡è¯†ç¬¦ã€‚
è¯¥å¤„ç†å™¨å°†åˆ›å»ºåŸºç¡€çš„æ–‡çŒ®æ•°æ®ç»“æ„ï¼Œç„¶åé€šè¿‡DataPipelineè¿›è¡Œç»Ÿä¸€å¤„ç†å’Œå»é‡ã€‚
"""

import logging
from typing import Dict, Any, Optional
from ...models.literature import MetadataModel, IdentifiersModel
from ...utils.external_api.crossref_client import CrossRefClient
from ...utils.external_api.arxiv_client import ArxivClient

logger = logging.getLogger(__name__)


class FallbackProcessor:
    """å¤‡é€‰æ–‡çŒ®å¤„ç†å™¨ - å¤„ç†æ™ºèƒ½è·¯ç”±æ— æ³•å¤„ç†çš„æƒ…å†µ"""
    
    def __init__(self):
        self.crossref_client = CrossRefClient()
        self.arxiv_client = ArxivClient()
    
    async def process(self, source_data: Dict[str, Any], task_id: str) -> Dict[str, Any]:
        """
        å¤„ç†éæ™ºèƒ½è·¯ç”±æƒ…å†µçš„æ–‡çŒ®æ•°æ®
        
        Args:
            source_data: æºæ•°æ®ï¼ˆåŒ…å«DOIã€ArXiv IDç­‰æ ‡è¯†ç¬¦ï¼‰
            task_id: ä»»åŠ¡ID
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å«æ ‡è¯†ç¬¦ã€å…ƒæ•°æ®ç­‰ä¿¡æ¯
        """
        logger.info(f"ğŸ”„ [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: å¼€å§‹å¤„ç†æºæ•°æ®")
        
        try:
            # 1. ä»æºæ•°æ®ä¸­æå–æ ‡è¯†ç¬¦
            identifiers = await self._extract_identifiers(source_data, task_id)
            
            # 2. å°è¯•è·å–å…ƒæ•°æ®
            metadata = await self._fetch_metadata(identifiers, task_id)
            
            # 3. æ„å»ºå¤„ç†ç»“æœ
            result = {
                'success': True,
                'identifiers': identifiers,
                'metadata': metadata,
                'source_data': source_data,
                'processor_type': 'fallback'
            }
            
            logger.info(f"âœ… [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: å¤„ç†å®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: å¤„ç†å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'error_type': 'fallback_processing_failed',
                'source_data': source_data
            }
    
    async def _extract_identifiers(self, source_data: Dict[str, Any], task_id: str) -> IdentifiersModel:
        """ä»æºæ•°æ®ä¸­æå–æ ‡è¯†ç¬¦"""
        identifiers = IdentifiersModel()
        
        # ä»source_dataä¸­æå–DOI
        if 'doi' in source_data:
            identifiers.doi = source_data['doi']
            logger.info(f"ğŸ”— [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: æå–DOI: {identifiers.doi}")
        
        # ä»source_dataä¸­æå–ArXiv ID
        if 'arxiv_id' in source_data:
            identifiers.arxiv_id = source_data['arxiv_id']
            logger.info(f"ğŸ”— [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: æå–ArXiv ID: {identifiers.arxiv_id}")
        
        # ä»URLä¸­å°è¯•æå–æ ‡è¯†ç¬¦
        if 'url' in source_data:
            url = source_data['url']
            # å°è¯•ä»URLä¸­æå–DOI
            if 'doi.org' in url and not identifiers.doi:
                # æå–doi.org/åé¢çš„éƒ¨åˆ†
                import re
                doi_match = re.search(r'doi\.org/(.+)', url)
                if doi_match:
                    identifiers.doi = doi_match.group(1)
                    logger.info(f"ğŸ”— [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: ä»URLæå–DOI: {identifiers.doi}")
            
            # å°è¯•ä»URLä¸­æå–ArXiv ID
            if 'arxiv.org' in url and not identifiers.arxiv_id:
                arxiv_match = re.search(r'arxiv\.org/(?:abs/|pdf/)?(\d{4}\.\d{4,5})', url)
                if arxiv_match:
                    identifiers.arxiv_id = arxiv_match.group(1)
                    logger.info(f"ğŸ”— [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: ä»URLæå–ArXiv ID: {identifiers.arxiv_id}")
        
        return identifiers
    
    async def _fetch_metadata(self, identifiers: IdentifiersModel, task_id: str) -> Optional[MetadataModel]:
        """æ ¹æ®æ ‡è¯†ç¬¦è·å–å…ƒæ•°æ®"""
        metadata = None
        
        # 1. ä¼˜å…ˆä½¿ç”¨DOIè·å–å…ƒæ•°æ®
        if identifiers.doi:
            try:
                logger.info(f"ğŸ“š [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: ä½¿ç”¨CrossRefè·å–DOIå…ƒæ•°æ®: {identifiers.doi}")
                crossref_data = await self.crossref_client.get_work_by_doi(identifiers.doi)
                if crossref_data:
                    metadata = self._convert_crossref_to_metadata(crossref_data, task_id)
                    logger.info(f"âœ… [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: CrossRefå…ƒæ•°æ®è·å–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: CrossRefè·å–å¤±è´¥: {e}")
        
        # 2. å¦‚æœDOIå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ArXiv IDè·å–å…ƒæ•°æ®
        if not metadata and identifiers.arxiv_id:
            try:
                logger.info(f"ğŸ“š [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: ä½¿ç”¨ArXivè·å–å…ƒæ•°æ®: {identifiers.arxiv_id}")
                arxiv_data = await self.arxiv_client.get_paper_by_id(identifiers.arxiv_id)
                if arxiv_data:
                    metadata = self._convert_arxiv_to_metadata(arxiv_data, task_id)
                    logger.info(f"âœ… [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: ArXivå…ƒæ•°æ®è·å–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: ArXivè·å–å¤±è´¥: {e}")
        
        # 3. å¦‚æœéƒ½å¤±è´¥äº†ï¼Œåˆ›å»ºåŸºç¡€å…ƒæ•°æ®
        if not metadata:
            logger.warning(f"âš ï¸ [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: æ— æ³•è·å–å…ƒæ•°æ®ï¼Œåˆ›å»ºåŸºç¡€å…ƒæ•°æ®")
            metadata = MetadataModel()
            # ä½¿ç”¨æ ‡è¯†ç¬¦ä½œä¸ºæ ‡é¢˜çš„å¤‡é€‰æ–¹æ¡ˆ
            if identifiers.doi:
                metadata.title = f"Literature with DOI: {identifiers.doi}"
            elif identifiers.arxiv_id:
                metadata.title = f"Literature with ArXiv ID: {identifiers.arxiv_id}"
            else:
                metadata.title = "Unknown Literature"
        
        return metadata
    
    def _convert_crossref_to_metadata(self, crossref_data: Dict[str, Any], task_id: str) -> MetadataModel:
        """å°†CrossRefæ•°æ®è½¬æ¢ä¸ºMetadataModel"""
        metadata = MetadataModel()
        
        # åŸºç¡€ä¿¡æ¯
        if 'title' in crossref_data and crossref_data['title']:
            metadata.title = crossref_data['title'][0] if isinstance(crossref_data['title'], list) else crossref_data['title']
        
        if 'author' in crossref_data:
            authors = []
            for author in crossref_data['author']:
                given = author.get('given', '')
                family = author.get('family', '')
                if given and family:
                    authors.append(f"{given} {family}")
                elif family:
                    authors.append(family)
            metadata.authors = authors
        
        # å‘è¡¨ä¿¡æ¯
        if 'published-print' in crossref_data:
            date_parts = crossref_data['published-print'].get('date-parts')
            if date_parts and date_parts[0]:
                metadata.year = str(date_parts[0][0])
        elif 'published-online' in crossref_data:
            date_parts = crossref_data['published-online'].get('date-parts')
            if date_parts and date_parts[0]:
                metadata.year = str(date_parts[0][0])
        
        if 'container-title' in crossref_data and crossref_data['container-title']:
            metadata.journal = crossref_data['container-title'][0] if isinstance(crossref_data['container-title'], list) else crossref_data['container-title']
        
        logger.info(f"ğŸ“š [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: CrossRefè½¬æ¢å®Œæˆ - æ ‡é¢˜: {metadata.title}")
        return metadata
    
    def _convert_arxiv_to_metadata(self, arxiv_data: Dict[str, Any], task_id: str) -> MetadataModel:
        """å°†ArXivæ•°æ®è½¬æ¢ä¸ºMetadataModel"""
        metadata = MetadataModel()
        
        # åŸºç¡€ä¿¡æ¯
        if 'title' in arxiv_data:
            metadata.title = arxiv_data['title']
        
        if 'authors' in arxiv_data:
            metadata.authors = arxiv_data['authors']
        
        if 'published' in arxiv_data:
            # ArXivæ—¥æœŸæ ¼å¼é€šå¸¸æ˜¯YYYY-MM-DD
            try:
                metadata.year = str(arxiv_data['published'][:4])
            except:
                pass
        
        if 'summary' in arxiv_data:
            metadata.abstract = arxiv_data['summary']
        
        # ArXivç‰¹æœ‰ä¿¡æ¯
        metadata.journal = "arXiv preprint"
        
        logger.info(f"ğŸ“š [å¤‡é€‰å¤„ç†å™¨] Task {task_id}: ArXivè½¬æ¢å®Œæˆ - æ ‡é¢˜: {metadata.title}")
        return metadata



