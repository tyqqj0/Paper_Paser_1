"""
æ™ºèƒ½è·¯ç”±å™¨ - çº¯ç²¹çš„è·¯ç”±é€‰æ‹©å’ŒHookç¼–æ’

åªè´Ÿè´£ä¸¤ä»¶äº‹ï¼š
1. æ ¹æ®URL/æ ‡è¯†ç¬¦é€‰æ‹©æœ€ä¼˜å¤„ç†è·¯å¾„  
2. ç¼–æ’Hookç³»ç»Ÿçš„è‡ªåŠ¨è§¦å‘
"""

import logging
from typing import Dict, List, Any, Optional
from datetime import datetime

from ...services.url_mapping import get_url_mapping_service
from ..metadata.registry import get_global_registry
from ..metadata.base import IdentifierData
from .routing import RouteManager
from .data_pipeline import DataPipeline

logger = logging.getLogger(__name__)


class SmartRouter:
    """æ™ºèƒ½è·¯ç”±å™¨ - ä¸“æ³¨è·¯ç”±é€‰æ‹©å’ŒHookç¼–æ’"""
    
    def __init__(self, dao=None):
        # è·¯ç”±ç»„ä»¶
        self.url_mapping_service = get_url_mapping_service()
        self.metadata_registry = get_global_registry()
        self.route_manager = RouteManager()
        
        # æ•°æ®ç®¡é“ (è´Ÿè´£æ‰€æœ‰æ•°æ®åº“æ“ä½œ)
        self.data_pipeline = DataPipeline(dao) if dao else None
        
    async def route_and_process(self, url: str, source_data: Dict[str, Any], task_id: str) -> Dict[str, Any]:
        """
        æ™ºèƒ½è·¯ç”±å’Œå¤„ç†å…¥å£
        
        Args:
            url: è¾“å…¥URL
            source_data: æºæ•°æ®
            task_id: ä»»åŠ¡ID
            
        Returns:
            å¤„ç†ç»“æœ
        """
        logger.info(f"ğŸš€ [æ™ºèƒ½è·¯ç”±] å¼€å§‹å¤„ç†: {url}")
        start_time = datetime.now()
        
        try:
            # é˜¶æ®µ1: URLæ˜ å°„ - æå–åŸºç¡€æ ‡è¯†ç¬¦
            mapping_result = await self._perform_url_mapping(url)
            
            # é˜¶æ®µ2: è·¯ç”±å†³ç­– - é€‰æ‹©æœ€ä¼˜å¤„ç†è·¯å¾„
            route = self.route_manager.determine_route(url, mapping_result)
            logger.info(f"ğŸ¯ [æ™ºèƒ½è·¯ç”±] é€‰æ‹©è·¯ç”±: {route.name} (å¤„ç†å™¨: {route.processors})")
            
            # é˜¶æ®µ3: æ‰§è¡Œé€‰å®šçš„å¤„ç†å™¨è·å–åŸå§‹æ•°æ®
            raw_data = await self._execute_processors(route, source_data, mapping_result)
            
            # ğŸ”§ æ£€æŸ¥raw_dataæ˜¯å¦æœ‰æ•ˆ
            if not raw_data:
                logger.error(f"[æ™ºèƒ½è·¯ç”±] å¤„ç†å™¨æ‰§è¡Œè¿”å›ç©ºæ•°æ®")
                return {
                    'status': 'failed',
                    'error': 'Processor execution returned no data',
                    'execution_time': (datetime.now() - start_time).total_seconds(),
                    'fallback_to_legacy': True
                }
            
            # é˜¶æ®µ4: æ•°æ®ç®¡é“å¤„ç† - ç»Ÿä¸€çš„å»é‡ã€å†™å…¥ã€Hookè§¦å‘
            if self.data_pipeline:
                pipeline_result = await self.data_pipeline.process_data(
                    raw_data=raw_data,
                    source_data=source_data,
                    mapping_result=mapping_result,
                    route_info={'name': route.name, 'processors': route.processors},
                    task_id=task_id
                )
                
                execution_time = (datetime.now() - start_time).total_seconds()
                pipeline_result['execution_time'] = execution_time
                pipeline_result['route_used'] = route.name
                
                logger.info(f"âœ… [æ™ºèƒ½è·¯ç”±] å¤„ç†å®Œæˆ: {execution_time:.2f}s")
                return pipeline_result
            else:
                # æ²¡æœ‰æ•°æ®ç®¡é“æ—¶çš„ç®€åŒ–è¿”å›
                execution_time = (datetime.now() - start_time).total_seconds()
                return {
                    'status': 'completed',
                    'route_used': route.name,
                    'execution_time': execution_time,
                    'raw_data': raw_data,
                    'note': 'No data pipeline configured'
                }
                
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ [æ™ºèƒ½è·¯ç”±] å¤„ç†å¤±è´¥: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'execution_time': execution_time,
                'fallback_to_legacy': True
            }
    
    async def _perform_url_mapping(self, url: str) -> Optional[Dict]:
        """æ‰§è¡ŒURLæ˜ å°„ - å¤ç”¨ç°æœ‰æœåŠ¡"""
        try:
            logger.debug(f"ğŸ” [æ™ºèƒ½è·¯ç”±] URLæ˜ å°„: {url}")
            # ğŸ”§ ä¿®å¤ï¼šURLæ˜ å°„æœåŠ¡çš„map_urlæ˜¯å¼‚æ­¥æ–¹æ³•ï¼Œéœ€è¦await
            mapping_result = await self.url_mapping_service.map_url(url)
            
            if mapping_result and mapping_result.is_successful():
                result_dict = mapping_result.to_dict()
                logger.info(f"âœ… [æ™ºèƒ½è·¯ç”±] URLæ˜ å°„æˆåŠŸ: {len([k for k, v in result_dict.items() if v])} ä¸ªå­—æ®µ")
                return result_dict
            else:
                logger.warning(f"âš ï¸ [æ™ºèƒ½è·¯ç”±] URLæ˜ å°„æ— ç»“æœ: {url}")
                return None
                
        except Exception as e:
            logger.warning(f"âš ï¸ [æ™ºèƒ½è·¯ç”±] URLæ˜ å°„å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None
    
    async def _execute_processors(self, route, source_data: Dict, mapping_result: Optional[Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œå¤„ç†å™¨è·å–åŸå§‹æ•°æ® - ä¸æ¶‰åŠæ•°æ®åº“æ“ä½œ"""
        
        # å‡†å¤‡æ ‡è¯†ç¬¦æ•°æ®
        identifier_data = self._prepare_identifier_data(source_data, mapping_result)
        
        # è·å–å¯ç”¨çš„å¤„ç†å™¨
        available_processors = self._get_available_processors(route.processors, identifier_data)
        
        if not available_processors:
            logger.warning(f"[æ™ºèƒ½è·¯ç”±] æœªæ‰¾åˆ°å¯ç”¨å¤„ç†å™¨ï¼Œè·¯ç”±: {route.name}")
            return {'processors_attempted': route.processors, 'available_processors': 0}
        
        logger.info(f"ğŸª [æ™ºèƒ½è·¯ç”±] å¯ç”¨å¤„ç†å™¨: {[p.name for p in available_processors]}")
        
        # æ ¹æ®è·¯ç”±ç±»å‹æ‰§è¡Œå¤„ç†å™¨
        if self.route_manager.is_fast_path(route):
            return await self._execute_fast_path(available_processors, identifier_data)
        else:
            return await self._execute_standard_path(available_processors, identifier_data)
    
    async def _execute_fast_path(self, processors, identifier_data: IdentifierData) -> Dict[str, Any]:
        """å¿«é€Ÿè·¯å¾„ - åªç”¨ç¬¬ä¸€ä¸ªå¤„ç†å™¨"""
        processor = processors[0]
        logger.info(f"âš¡ [æ™ºèƒ½è·¯ç”±] å¿«é€Ÿè·¯å¾„: {processor.name}")
        
        try:
            # å…¼å®¹åŒæ­¥/å¼‚æ­¥å¤„ç†å™¨
            if hasattr(processor.process, '__call__'):
                import inspect
                if inspect.iscoroutinefunction(processor.process):
                    result = await processor.process(identifier_data)
                else:
                    import asyncio
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(None, processor.process, identifier_data)
            else:
                raise AttributeError(f"Processor {processor.name} has no process method")
            
            if result and result.is_valid:
                logger.info(f"âœ… [æ™ºèƒ½è·¯ç”±] å¿«é€Ÿè·¯å¾„æˆåŠŸ: {processor.name}")
                return {
                    'processor_used': processor.name,
                    'confidence': result.confidence,
                    'metadata': result.metadata,
                    'new_identifiers': result.new_identifiers,
                    'success': True
                }
            else:
                logger.warning(f"âš ï¸ [æ™ºèƒ½è·¯ç”±] å¿«é€Ÿè·¯å¾„å¤±è´¥: {processor.name}")
                return {
                    'processor_used': processor.name,
                    'error': result.error if result else 'No result',
                    'success': False
                }
                
        except Exception as e:
            logger.error(f"âŒ [æ™ºèƒ½è·¯ç”±] å¿«é€Ÿè·¯å¾„å¼‚å¸¸: {processor.name}, {e}")
            return {
                'processor_used': processor.name,
                'error': str(e),
                'success': False
            }
    
    async def _execute_standard_path(self, processors, identifier_data: IdentifierData) -> Dict[str, Any]:
        """æ ‡å‡†è·¯å¾„ - å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå¤„ç†å™¨"""
        logger.info(f"ğŸ”„ [æ™ºèƒ½è·¯ç”±] æ ‡å‡†è·¯å¾„: {[p.name for p in processors]}")
        
        # ç®€åŒ–ç‰ˆæœ¬ï¼šä¾æ¬¡å°è¯•å¤„ç†å™¨ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªæˆåŠŸçš„
        for processor in processors:
            try:
                # å…¼å®¹åŒæ­¥/å¼‚æ­¥å¤„ç†å™¨
                import inspect
                if inspect.iscoroutinefunction(processor.process):
                    result = await processor.process(identifier_data)
                else:
                    import asyncio
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(None, processor.process, identifier_data)
                
                if result and result.is_valid:
                    logger.info(f"âœ… [æ™ºèƒ½è·¯ç”±] æ ‡å‡†è·¯å¾„æˆåŠŸ: {processor.name}")
                    return {
                        'processor_used': processor.name,
                        'confidence': result.confidence,
                        'metadata': result.metadata,
                        'new_identifiers': result.new_identifiers,
                        'success': True
                    }
                else:
                    logger.debug(f"[æ™ºèƒ½è·¯ç”±] å¤„ç†å™¨å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª: {processor.name}")
                    
            except Exception as e:
                logger.debug(f"[æ™ºèƒ½è·¯ç”±] å¤„ç†å™¨å¼‚å¸¸ï¼Œå°è¯•ä¸‹ä¸€ä¸ª: {processor.name}, {e}")
                continue
        
        # æ‰€æœ‰å¤„ç†å™¨éƒ½å¤±è´¥
        return {
            'processors_attempted': [p.name for p in processors],
            'error': 'All processors failed',
            'success': False
        }
    
    def _prepare_identifier_data(self, source_data: Dict, mapping_result: Optional[Dict]) -> IdentifierData:
        """å‡†å¤‡æ ‡è¯†ç¬¦æ•°æ®"""
        
        # åŸºç¡€æ ‡è¯†ç¬¦
        identifier_data = IdentifierData(
            doi=source_data.get("doi"),
            arxiv_id=source_data.get("arxiv_id"),
            pmid=source_data.get("pmid"),
            url=source_data.get("url"),
            source_data=source_data
        )
        
        # ä»URLæ˜ å°„ç»“æœä¸­æå–å¢å¼ºä¿¡æ¯
        if mapping_result:
            identifier_data.title = mapping_result.get("title")
            identifier_data.year = mapping_result.get("year")
            identifier_data.venue = mapping_result.get("venue")
            identifier_data.authors = mapping_result.get("authors")
            
            # æ›´æ–°æ ‡è¯†ç¬¦
            if mapping_result.get("doi"):
                identifier_data.doi = mapping_result["doi"]
            if mapping_result.get("arxiv_id"):
                identifier_data.arxiv_id = mapping_result["arxiv_id"]
                
        return identifier_data
    
    def _get_available_processors(self, processor_names: List[str], identifier_data: IdentifierData):
        """è·å–å¯ç”¨çš„å¤„ç†å™¨å®ä¾‹"""
        available = []
        
        for name in processor_names:
            try:
                processor = self.metadata_registry.get_processor(name)
                if processor.can_handle(identifier_data):
                    available.append(processor)
                else:
                    logger.debug(f"[æ™ºèƒ½è·¯ç”±] å¤„ç†å™¨æ— æ³•å¤„ç†: {name}")
            except KeyError:
                logger.warning(f"[æ™ºèƒ½è·¯ç”±] å¤„ç†å™¨æœªæ³¨å†Œ: {name}")
            except Exception as e:
                logger.warning(f"[æ™ºèƒ½è·¯ç”±] è·å–å¤„ç†å™¨å¤±è´¥: {name}, é”™è¯¯: {e}")
                
        return available
