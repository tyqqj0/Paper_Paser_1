"""
æ™ºèƒ½è·¯ç”±å™¨ - çº¯ç²¹çš„è·¯ç”±é€‰æ‹©å’ŒHookç¼–æŽ’

åªè´Ÿè´£ä¸¤ä»¶äº‹ï¼š
1. æ ¹æ®URL/æ ‡è¯†ç¬¦é€‰æ‹©æœ€ä¼˜å¤„ç†è·¯å¾„  
2. ç¼–æŽ’Hookç³»ç»Ÿçš„è‡ªåŠ¨è§¦å‘
"""

import logging
from typing import Dict, List, Any, Optional
from datetime import datetime

from ...services.url_mapping import get_url_mapping_service
from ..metadata.registry import get_global_registry
from ..metadata.base import IdentifierData
from .routing import RouteManager
from .data_pipeline import DataPipeline
from .hooks import HookManager

logger = logging.getLogger(__name__)


class SmartRouter:
    """æ™ºèƒ½è·¯ç”±å™¨ - ä¸“æ³¨è·¯ç”±é€‰æ‹©å’ŒHookç¼–æŽ’"""
    
    def __init__(self, dao=None):
        # è·¯ç”±ç»„ä»¶
        self.url_mapping_service = get_url_mapping_service()
        self.metadata_registry = get_global_registry()
        self.route_manager = RouteManager.get_instance()
        
        # ðŸ†• Hookç®¡ç†å™¨
        self.hook_manager = HookManager(dao) if dao else None
        
        # æ•°æ®ç®¡é“ (è´Ÿè´£æ‰€æœ‰æ•°æ®åº“æ“ä½œ)
        self.data_pipeline = DataPipeline(dao, self.hook_manager) if dao else None
        
    def can_handle(self, url: str) -> bool:
        """
        åˆ¤æ–­SmartRouteræ˜¯å¦èƒ½å¤„ç†æ­¤URL
        
        Args:
            url: è¾“å…¥URL
            
        Returns:
            True if å¯ä»¥é€šè¿‡æ™ºèƒ½è·¯ç”±å¤„ç†ï¼ŒFalse if éœ€è¦å›žé€€åˆ°legacyå¤„ç†
        """
        if not url:
            return False
            
        try:
            # ä½¿ç”¨RouteManageråˆ¤æ–­æ˜¯å¦æœ‰åˆé€‚çš„è·¯ç”±
            route = self.route_manager.determine_route(url)
            
            # å¦‚æžœæ‰¾åˆ°äº†éžå…œåº•è·¯ç”±ï¼Œè¯´æ˜Žå¯ä»¥å¤„ç†
            if route and route.name != "fallback_route":
                logger.debug(f"ðŸŽ¯ SmartRouterå¯ä»¥å¤„ç†: {url} -> {route.name}")
                return True
            else:
                logger.debug(f"âš ï¸ SmartRouteræ— æ³•å¤„ç†: {url} -> å›žé€€åˆ°legacy")
                return False
                
        except Exception as e:
            logger.warning(f"âŒ SmartRouterè·¯ç”±åˆ¤æ–­å¼‚å¸¸: {url} -> {e}")
            return False

    async def route_and_process(self, url: str, source_data: Dict[str, Any], task_id: str) -> Dict[str, Any]:
        """
        æ™ºèƒ½è·¯ç”±å’Œå¤„ç†å…¥å£
        
        Args:
            url: è¾“å…¥URL
            source_data: æºæ•°æ®
            task_id: ä»»åŠ¡ID
            
        Returns:
            å¤„ç†ç»“æžœ
        """
        logger.info(f"ðŸš€ [æ™ºèƒ½è·¯ç”±] å¼€å§‹å¤„ç†: {url}")
        start_time = datetime.now()
        
        try:
            # é˜¶æ®µ1: URLæ˜ å°„ - æå–åŸºç¡€æ ‡è¯†ç¬¦
            mapping_result = await self._perform_url_mapping(url)
            
            # é˜¶æ®µ2: è·¯ç”±å†³ç­– - é€‰æ‹©æœ€ä¼˜å¤„ç†è·¯å¾„
            route = self.route_manager.determine_route(url, mapping_result)
            logger.info(f"ðŸŽ¯ [æ™ºèƒ½è·¯ç”±] é€‰æ‹©è·¯ç”±: {route.name} (å¤„ç†å™¨: {route.processors})")
            
            # é˜¶æ®µ3: æ‰§è¡Œé€‰å®šçš„å¤„ç†å™¨èŽ·å–åŽŸå§‹æ•°æ®
            raw_data = await self._execute_processors(route, source_data, mapping_result)
            
            # ðŸ”§ æ£€æŸ¥raw_dataæ˜¯å¦æœ‰æ•ˆ
            if not raw_data:
                logger.error(f"[æ™ºèƒ½è·¯ç”±] å¤„ç†å™¨æ‰§è¡Œè¿”å›žç©ºæ•°æ®")
                return {
                    'status': 'failed',
                    'error': 'Processor execution returned no data',
                    'execution_time': (datetime.now() - start_time).total_seconds(),
                    'fallback_to_legacy': True
                }
            
            # é˜¶æ®µ4: æ•°æ®ç®¡é“å¤„ç† - ç»Ÿä¸€çš„åŽ»é‡ã€å†™å…¥ã€Hookè§¦å‘
            if self.data_pipeline:
                pipeline_result = await self.data_pipeline.process_data(
                    raw_data=raw_data,
                    source_data=source_data,
                    mapping_result=mapping_result,
                    route_info={'name': route.name, 'processors': route.processors},
                    task_id=task_id
                )
                
                execution_time = (datetime.now() - start_time).total_seconds()
                pipeline_result['execution_time'] = execution_time
                pipeline_result['route_used'] = route.name
                
                logger.info(f"âœ… [æ™ºèƒ½è·¯ç”±] å¤„ç†å®Œæˆ: {execution_time:.2f}s")
                return pipeline_result
            else:
                # æ²¡æœ‰æ•°æ®ç®¡é“æ—¶çš„ç®€åŒ–è¿”å›ž
                execution_time = (datetime.now() - start_time).total_seconds()
                return {
                    'status': 'completed',
                    'route_used': route.name,
                    'execution_time': execution_time,
                    'raw_data': raw_data,
                    'note': 'No data pipeline configured'
                }
                
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ [æ™ºèƒ½è·¯ç”±] å¤„ç†å¤±è´¥: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'execution_time': execution_time,
                'fallback_to_legacy': True
            }
    
    async def _perform_url_mapping(self, url: str) -> Optional[Dict]:
        """æ‰§è¡ŒURLæ˜ å°„ - å¤ç”¨çŽ°æœ‰æœåŠ¡"""
        try:
            logger.debug(f"ðŸ” [æ™ºèƒ½è·¯ç”±] URLæ˜ å°„: {url}")
            # ðŸ”§ ä¿®å¤ï¼šURLæ˜ å°„æœåŠ¡çš„map_urlæ˜¯å¼‚æ­¥æ–¹æ³•ï¼Œéœ€è¦await
            mapping_result = await self.url_mapping_service.map_url(url)
            
            if mapping_result and mapping_result.is_successful():
                result_dict = mapping_result.to_dict()
                logger.info(f"âœ… [æ™ºèƒ½è·¯ç”±] URLæ˜ å°„æˆåŠŸ: {len([k for k, v in result_dict.items() if v])} ä¸ªå­—æ®µ")
                return result_dict
            else:
                logger.warning(f"âš ï¸ [æ™ºèƒ½è·¯ç”±] URLæ˜ å°„æ— ç»“æžœ: {url}")
                return None
                
        except Exception as e:
            logger.warning(f"âš ï¸ [æ™ºèƒ½è·¯ç”±] URLæ˜ å°„å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None
    
    async def _execute_processors(self, route, source_data: Dict, mapping_result: Optional[Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œå¤„ç†å™¨èŽ·å–åŽŸå§‹æ•°æ® - ä¸æ¶‰åŠæ•°æ®åº“æ“ä½œ"""
        
        # å‡†å¤‡æ ‡è¯†ç¬¦æ•°æ®
        identifier_data = self._prepare_identifier_data(source_data, mapping_result)
        
        # èŽ·å–å¯ç”¨çš„å¤„ç†å™¨
        # available_processors = self._get_available_processors(route.processors, identifier_data)
        available_processors = []
        for processor_name in route.processors:
            processor = self.metadata_registry.get_processor(processor_name)
            if processor:
                available_processors.append(processor)
            else:
                logger.warning(f"[æ™ºèƒ½è·¯ç”±] æœªæ‰¾åˆ°å¤„ç†å™¨: {processor_name}")
        
        if not available_processors:
            logger.warning(f"[æ™ºèƒ½è·¯ç”±] æœªæ‰¾åˆ°å¯ç”¨å¤„ç†å™¨ï¼Œè·¯ç”±: {route.name}")
            return {'processors_attempted': route.processors, 'available_processors': 0}
        
        logger.info(f"ðŸŽª [æ™ºèƒ½è·¯ç”±] å¯ç”¨å¤„ç†å™¨: {[p.name for p in available_processors]}")
        
        # æ ¹æ®è·¯ç”±ç±»åž‹æ‰§è¡Œå¤„ç†å™¨
        if self.route_manager.is_fast_path(route):
            return await self._execute_fast_path(available_processors, identifier_data)
        else:
            return await self._execute_standard_path(available_processors, identifier_data)
    
    async def _execute_fast_path(self, processors, identifier_data: IdentifierData) -> Dict[str, Any]:
        """å¿«é€Ÿè·¯å¾„ - ä½¿ç”¨ç»Ÿä¸€çš„å¤„ç†é€»è¾‘"""
        logger.info(f"âš¡ [æ™ºèƒ½è·¯ç”±] å¿«é€Ÿè·¯å¾„: {[p.name for p in processors]}")
        return await self._execute_processors_unified(processors, identifier_data, is_fast_path=True)
    
    async def _execute_standard_path(self, processors, identifier_data: IdentifierData) -> Dict[str, Any]:
        """æ ‡å‡†è·¯å¾„ - ä½¿ç”¨ç»Ÿä¸€çš„å¤„ç†é€»è¾‘"""
        logger.info(f"ðŸ”„ [æ™ºèƒ½è·¯ç”±] æ ‡å‡†è·¯å¾„: {[p.name for p in processors]}")
        return await self._execute_processors_unified(processors, identifier_data, is_fast_path=False)
    
    async def _execute_processors_unified(self, processors, identifier_data: IdentifierData, is_fast_path: bool = False) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„å¤„ç†å™¨æ‰§è¡Œé€»è¾‘ï¼Œæ”¯æŒmetadataå’Œidentifiersç´¯ç§¯åˆå¹¶
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. æŒ‰é¡ºåºé€‰æ‹©åˆ—è¡¨ä¸­æœ€é å‰çš„å¯ç”¨ä¸”æœªç”¨è¿‡çš„å¤„ç†å™¨
        2. æ‰§è¡Œå¤„ç†å™¨èŽ·å¾—ç»“æžœ
        3. ç´¯ç§¯åˆå¹¶metadataå’Œnew_identifiers
        4. å¦‚æžœç»“æžœæœ‰æ•ˆä¸”å®Œæ•´(is_complete_parsing)ï¼Œåˆ™åœæ­¢
        5. å¦åˆ™ç»§ç»­ä¸‹ä¸€ä¸ªå¤„ç†å™¨
        6. è¿”å›žæœ€ä½³ç»“æžœé…åˆç´¯ç§¯çš„metadataå’Œidentifiers
        """
        path_type = "å¿«é€Ÿè·¯å¾„" if is_fast_path else "æ ‡å‡†è·¯å¾„"
        attempted_processors = []
        used_processors = set()  # è·Ÿè¸ªå·²ä½¿ç”¨çš„å¤„ç†å™¨
        
        # ç´¯ç§¯æ•°æ®å­˜å‚¨
        accumulated_metadata = {}
        accumulated_identifiers = []
        best_result = None
        best_confidence = 0.0
        
        while True:
            # é€‰æ‹©ä¸‹ä¸€ä¸ªå¯ç”¨ä¸”æœªç”¨è¿‡çš„å¤„ç†å™¨
            next_processor = self._get_next_available_processor(processors, used_processors, identifier_data)
            
            if not next_processor:
                # æ²¡æœ‰æ›´å¤šå¯ç”¨å¤„ç†å™¨
                logger.info(f"ðŸ [{path_type}] æ‰€æœ‰å¯ç”¨å¤„ç†å™¨å·²å°è¯•å®Œæ¯•")
                break
            
            # æ ‡è®°ä¸ºå·²ä½¿ç”¨
            used_processors.add(next_processor.name)
            attempted_processors.append(next_processor.name)
            
            logger.info(f"ðŸ” [{path_type}] å°è¯•å¤„ç†å™¨: {next_processor.name}")
            
            try:
                # æ‰§è¡Œå¤„ç†å™¨
                result = await self._execute_single_processor(next_processor, identifier_data)
                
                if result:
                    # è®¡ç®—è§£æžåˆ†æ•°
                    parsing_score = result.get_parsing_score()
                    
                    # ðŸ†• è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                    logger.info(f"ðŸ” [{path_type}] å¤„ç†å™¨ç»“æžœè¯¦æƒ…: {next_processor.name}")
                    logger.info(f"  ðŸ“Š åˆ†æ•°: {parsing_score:.3f}, ç½®ä¿¡åº¦: {result.confidence:.3f}")
                    
                    # ä¿®å¤ï¼šMetadataModelå¯¹è±¡æ²¡æœ‰len()æ–¹æ³•ï¼Œæ”¹ä¸ºæ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    metadata_status = "æœ‰æ•ˆ" if result.metadata and result.metadata.title else "æ— æ•ˆ"
                    logger.info(f"  ðŸ“ MetadataçŠ¶æ€: {metadata_status}")
                    logger.info(f"  ðŸ†” æ–°Identifiersæ•°: {len(result.new_identifiers) if result.new_identifiers else 0}")
                    
                    if result.metadata:
                        # ä¿®å¤ï¼šMetadataModelå¯¹è±¡ä¸èƒ½ç”¨get()æ–¹æ³•ï¼Œç›´æŽ¥è®¿é—®å±žæ€§
                        title = result.metadata.title or 'N/A'
                        logger.info(f"  ðŸ“– æ ‡é¢˜: {title[:50]}{'...' if len(title) > 50 else ''}")
                        
                        authors = result.metadata.authors or []
                        author_names = []
                        for a in authors[:3]:
                            if isinstance(a, dict):
                                author_names.append(a.get('name', 'N/A'))
                            else:
                                author_names.append(str(a))
                        logger.info(f"  ðŸ‘¥ ä½œè€…æ•°: {len(authors)} - {author_names}")
                        logger.info(f"  ðŸ“… å¹´ä»½: {result.metadata.year or 'N/A'}")
                        logger.info(f"  ðŸ“š æœŸåˆŠ: {result.metadata.journal or 'N/A'}")
                    
                    if parsing_score > 0.0:
                        # éžé›¶åˆ†ï¼šæœ‰ä»·å€¼çš„ç»“æžœ
                        logger.info(f"âœ… [{path_type}] å¤„ç†å™¨äº§ç”Ÿæœ‰æ•ˆç»“æžœ: {next_processor.name}")
                        
                        # ç´¯ç§¯åˆå¹¶metadataå’Œidentifiersï¼ˆæ‰€æœ‰éžé›¶åˆ†ç»“æžœéƒ½è¦åˆå¹¶ï¼‰
                        self._merge_metadata(accumulated_metadata, result.metadata, next_processor.name)
                        self._merge_identifiers(accumulated_identifiers, result.new_identifiers, next_processor.name)
                        
                        # ðŸ†• æ›´æ–°identifier_dataï¼Œä¸ºåŽç»­å¤„ç†å™¨æä¾›æ›´å¤šä¿¡æ¯
                        identifier_data = self._update_identifier_data_from_result(identifier_data, result)
                        
                        # æ›´æ–°æœ€ä½³ä¸»è¦ç»“æžœçš„æ¡ä»¶
                        is_better_result = (
                            best_result is None or 
                            parsing_score > best_result.get_parsing_score() or
                            (parsing_score == best_result.get_parsing_score() and result.confidence > best_confidence)
                        )
                        
                        # ðŸ” è°ƒè¯•ï¼šæ£€æŸ¥best_resultæ›´æ–°é€»è¾‘
                        logger.debug(f"ðŸ” [{path_type}] best_resultæ›´æ–°æ£€æŸ¥:")
                        logger.debug(f"  å½“å‰best_result: {best_result is not None}")
                        logger.debug(f"  å½“å‰åˆ†æ•°: {parsing_score:.3f}")
                        logger.debug(f"  is_better_result: {is_better_result}")
                        
                        if is_better_result:
                            best_result = result
                            best_confidence = result.confidence
                            logger.info(f"ðŸ“ˆ [{path_type}] æ›´æ–°æœ€ä½³ä¸»ç»“æžœ: {next_processor.name} (åˆ†æ•°: {parsing_score:.3f})")
                        else:
                            logger.debug(f"ðŸ”„ [{path_type}] æœªæ›´æ–°best_result: {next_processor.name}")
                        
                        # æ»¡åˆ†æ£€æŸ¥ - å¦‚æžœæ»¡åˆ†å°±ç«‹å³åœæ­¢
                        if parsing_score >= 1.0:
                            logger.info(f"ðŸš€ [{path_type}] æ»¡åˆ†è§£æžï¼Œç«‹å³åœæ­¢: {next_processor.name} (åˆ†æ•°: {parsing_score:.3f})")
                            break
                        elif is_fast_path and best_result:
                            # å¿«é€Ÿè·¯å¾„ï¼šå¾—åˆ°æœ‰æ•ˆç»“æžœå°±è¿”å›žï¼ˆå³ä½¿ä¸æ»¡åˆ†ï¼‰
                            logger.info(f"âš¡ [{path_type}] å¿«é€Ÿè·¯å¾„èŽ·å¾—æœ‰æ•ˆç»“æžœï¼Œç›´æŽ¥è¿”å›ž (åˆ†æ•°: {parsing_score:.3f})")
                            break
                        else:
                            # æ ‡å‡†è·¯å¾„ï¼šéžæ»¡åˆ†ä½†æœ‰æ•ˆï¼Œç»§ç»­å¯»æ‰¾æ›´å¥½çš„ç»“æžœ
                            logger.info(f"ðŸ”„ [{path_type}] éžæ»¡åˆ†ä½†æœ‰æ•ˆï¼Œç»§ç»­å¯»æ‰¾æ›´å¥½ç»“æžœ (å½“å‰åˆ†æ•°: {parsing_score:.3f})")
                    else:
                        # é›¶åˆ†ï¼šæ— æ•ˆç»“æžœï¼Œç»§ç»­å°è¯•
                        logger.debug(f"âŒ [{path_type}] å¤„ç†å™¨é›¶åˆ†ï¼Œç»§ç»­å°è¯•: {next_processor.name}")
                        
                else:
                    logger.debug(f"âŒ [{path_type}] å¤„ç†å™¨è¿”å›žç©ºç»“æžœ: {next_processor.name}")
                    
            except Exception as e:
                logger.debug(f"ðŸ’¥ [{path_type}] å¤„ç†å™¨å¼‚å¸¸ï¼Œç»§ç»­å°è¯•: {next_processor.name}, {e}")
                continue
        
        # æž„å»ºæœ€ç»ˆç»“æžœ
        if best_result:
            final_parsing_score = best_result.get_parsing_score()
            
            final_result = {
                'processor_used': best_result.processor_name if hasattr(best_result, 'processor_name') else 'unknown',
                'confidence': best_result.confidence,
                'parsing_score': final_parsing_score,  # æ·»åŠ è§£æžåˆ†æ•°
                'metadata': best_result.metadata,  # ðŸ”§ ä½¿ç”¨ä¸»å¤„ç†å™¨çš„åŽŸå§‹MetadataModelå¯¹è±¡ï¼Œè€Œä¸æ˜¯ç´¯ç§¯å­—å…¸
                'accumulated_metadata': accumulated_metadata,  # ä¿ç•™ç´¯ç§¯æ•°æ®ç”¨äºŽè°ƒè¯•
                'new_identifiers': accumulated_identifiers,  # ä½¿ç”¨ç´¯ç§¯çš„identifiers
                'success': True,
                'attempted_processors': attempted_processors,
                'metadata_sources': list(set([meta.get('source_processor', 'unknown') for meta in accumulated_metadata.values() if isinstance(meta, dict)])),
                'is_complete': final_parsing_score >= 1.0,  # æ˜¯å¦ä¸ºæ»¡åˆ†
                'accumulation_summary': {
                    'total_metadata_fields': len(accumulated_metadata),
                    'total_identifiers': len(accumulated_identifiers),
                    'contributing_processors': len(set(attempted_processors))
                }
            }
            
            logger.info(f"ðŸ† [{path_type}] æœ€ç»ˆç»“æžœ: ä¸»å¤„ç†å™¨={final_result['processor_used']}, "
                       f"è§£æžåˆ†æ•°={final_parsing_score:.3f}, "
                       f"metadataæ¥æº={len(final_result['metadata_sources'])}ä¸ªå¤„ç†å™¨, "
                       f"ç´¯ç§¯identifiers={len(accumulated_identifiers)}ä¸ª")
            
            return final_result
        else:
            # æ‰€æœ‰å¤„ç†å™¨éƒ½äº§ç”Ÿé›¶åˆ†ç»“æžœ
            logger.warning(f"âŒ [{path_type}] æ‰€æœ‰å¤„ç†å™¨éƒ½äº§ç”Ÿé›¶åˆ†ç»“æžœ")
            return {
                'processors_attempted': attempted_processors,
                'error': 'All processors produced zero-score results',
                'success': False,
                'parsing_score': 0.0
            }
    
    def _merge_metadata(self, accumulated_metadata: Dict, new_metadata: Dict, processor_name: str):
        """
        åˆå¹¶metadataï¼Œé¿å…è¦†ç›–å·²æœ‰æ•°æ®
        
        ç­–ç•¥ï¼š
        1. å¯¹äºŽç›¸åŒçš„keyï¼Œå¦‚æžœå€¼ä¸åŒï¼Œä¿ç•™æ›´è¯¦ç»†çš„æˆ–åˆ›å»ºåˆ—è¡¨
        2. ä¸ºæ¯ä¸ªæ•°æ®æ·»åŠ æ¥æºæ ‡è®°
        3. ä¼˜å…ˆä¿ç•™æ›´å®Œæ•´çš„æ•°æ®
        """
        if not new_metadata:
            return
        
        # ðŸ”§ ä¿®å¤ï¼šå¤„ç†MetadataModelå¯¹è±¡
        if hasattr(new_metadata, '__dict__') and not isinstance(new_metadata, dict):
            # MetadataModelå¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸
            metadata_dict = {}
            for attr_name in dir(new_metadata):
                if not attr_name.startswith('_') and not callable(getattr(new_metadata, attr_name)):
                    value = getattr(new_metadata, attr_name)
                    if value is not None:
                        metadata_dict[attr_name] = value
            new_metadata = metadata_dict
        
        # çŽ°åœ¨new_metadataè‚¯å®šæ˜¯å­—å…¸äº†
        logger.debug(f"ðŸ”„ [åˆå¹¶] {processor_name} è´¡çŒ® {len(new_metadata)} ä¸ªå­—æ®µåˆ°å·²æœ‰çš„{len(accumulated_metadata)}ä¸ªå­—æ®µä¸­")
        
        for key, value in new_metadata.items():
            if key not in accumulated_metadata:
                # æ–°å­—æ®µï¼Œç›´æŽ¥æ·»åŠ 
                accumulated_metadata[key] = {
                    'value': value,
                    'source_processor': processor_name,
                    'confidence': getattr(value, 'confidence', 1.0) if hasattr(value, 'confidence') else 1.0
                }
                logger.debug(f"ðŸ†• [åˆå¹¶] æ–°å¢žå­—æ®µ '{key}': {self._format_value_for_debug(value)} (æ¥æº: {processor_name})")
            else:
                # å·²å­˜åœ¨çš„å­—æ®µï¼Œéœ€è¦æ™ºèƒ½åˆå¹¶
                existing = accumulated_metadata[key]
                existing_value = existing.get('value') if isinstance(existing, dict) else existing
                
                # æ¯”è¾ƒå€¼çš„å®Œæ•´æ€§å’Œè´¨é‡
                if self._is_better_metadata_value(value, existing_value):
                    logger.debug(f"ðŸ“ˆ [åˆå¹¶] æ›´æ–°å­—æ®µ '{key}': {self._format_value_for_debug(value)} "
                                f"({processor_name}) æ›¿æ¢ {self._format_value_for_debug(existing_value)} "
                                f"({existing.get('source_processor', 'unknown')})")
                    accumulated_metadata[key] = {
                        'value': value,
                        'source_processor': processor_name,
                        'confidence': getattr(value, 'confidence', 1.0) if hasattr(value, 'confidence') else 1.0,
                        'previous_value': existing_value,
                        'previous_source': existing.get('source_processor', 'unknown')
                    }
                elif value != existing_value:
                    # å€¼ä¸åŒä½†æ–°å€¼ä¸ä¸€å®šæ›´å¥½ï¼Œè®°å½•ä¸ºalternative
                    if 'alternatives' not in accumulated_metadata[key]:
                        accumulated_metadata[key]['alternatives'] = []
                    accumulated_metadata[key]['alternatives'].append({
                        'value': value,
                        'source_processor': processor_name
                    })
                    logger.debug(f"ðŸ”„ [åˆå¹¶] ä¿ç•™åŽŸå€¼ '{key}': {self._format_value_for_debug(existing_value)} "
                                f"({existing.get('source_processor', 'unknown')})ï¼Œ"
                                f"æ–°å€¼ä½œä¸ºå¤‡é€‰: {self._format_value_for_debug(value)} ({processor_name})")
                else:
                    logger.debug(f"ðŸ”„ [åˆå¹¶] å­—æ®µ '{key}' å€¼ç›¸åŒï¼Œè·³è¿‡")
    
    def _merge_identifiers(self, accumulated_identifiers: List, new_identifiers: List, processor_name: str):
        """
        åˆå¹¶new_identifiersï¼Œé¿å…é‡å¤
        
        ç­–ç•¥ï¼š
        1. æŒ‰identifierçš„å€¼åŽ»é‡
        2. ä¿ç•™æ¥æºä¿¡æ¯
        3. åˆå¹¶ç›¸åŒidentifierçš„ä¸åŒå±žæ€§
        """
        if not new_identifiers:
            return
            
        logger.debug(f"ðŸ”„ [åˆå¹¶] {processor_name} è´¡çŒ® {len(new_identifiers)} ä¸ªæ ‡è¯†ç¬¦åˆ°å·²æœ‰çš„{len(accumulated_identifiers)}ä¸ªä¸­")
        
        # åˆ›å»ºçŽ°æœ‰identifiersçš„ç´¢å¼•ï¼ˆæŒ‰å€¼ï¼‰
        existing_index = {}
        for i, existing_id in enumerate(accumulated_identifiers):
            key = self._get_identifier_key(existing_id)
            existing_index[key] = i
        
        # å¤„ç†æ–°çš„identifiers
        for new_id in new_identifiers:
            key = self._get_identifier_key(new_id)
            
            if key in existing_index:
                # å·²å­˜åœ¨ï¼Œåˆå¹¶å±žæ€§
                existing_idx = existing_index[key]
                existing_id = accumulated_identifiers[existing_idx]
                
                # åˆå¹¶å±žæ€§ï¼ˆå¦‚confidence, sourceç­‰ï¼‰
                merged_id = self._merge_identifier_attributes(existing_id, new_id, processor_name)
                accumulated_identifiers[existing_idx] = merged_id
                logger.debug(f"ðŸ”— [åˆå¹¶] åˆå¹¶é‡å¤æ ‡è¯†ç¬¦: {key} (æ¥æº: {processor_name})")
            else:
                # æ–°identifierï¼Œæ·»åŠ æ¥æºä¿¡æ¯
                enhanced_id = dict(new_id) if isinstance(new_id, dict) else {'value': new_id}
                enhanced_id['discovered_by'] = processor_name
                accumulated_identifiers.append(enhanced_id)
                existing_index[key] = len(accumulated_identifiers) - 1
                logger.debug(f"âž• [åˆå¹¶] æ–°å¢žæ ‡è¯†ç¬¦: {key} (æ¥æº: {processor_name})")
    
    def _is_better_metadata_value(self, new_value, existing_value):
        """åˆ¤æ–­æ–°çš„metadataå€¼æ˜¯å¦æ¯”çŽ°æœ‰å€¼æ›´å¥½"""
        # å¦‚æžœçŽ°æœ‰å€¼ä¸ºç©ºæˆ–Noneï¼Œæ–°å€¼æ€»æ˜¯æ›´å¥½
        if not existing_value:
            return bool(new_value)
        
        # å¦‚æžœæ–°å€¼ä¸ºç©ºï¼Œä¿æŒçŽ°æœ‰å€¼
        if not new_value:
            return False
        
        # å­—ç¬¦ä¸²é•¿åº¦æ¯”è¾ƒï¼ˆæ›´é•¿é€šå¸¸æ„å‘³ç€æ›´è¯¦ç»†ï¼‰
        if isinstance(new_value, str) and isinstance(existing_value, str):
            return len(new_value) > len(existing_value)
        
        # åˆ—è¡¨é•¿åº¦æ¯”è¾ƒ
        if isinstance(new_value, list) and isinstance(existing_value, list):
            return len(new_value) > len(existing_value)
        
        # å­—å…¸å­—æ®µæ•°é‡æ¯”è¾ƒ
        if isinstance(new_value, dict) and isinstance(existing_value, dict):
            return len(new_value) > len(existing_value)
        
        # å¦‚æžœæœ‰confidenceå±žæ€§ï¼Œæ¯”è¾ƒconfidence
        new_conf = getattr(new_value, 'confidence', 0)
        existing_conf = getattr(existing_value, 'confidence', 0)
        if new_conf != existing_conf:
            return new_conf > existing_conf
        
        # é»˜è®¤ä¿æŒçŽ°æœ‰å€¼
        return False
    
    def _get_identifier_key(self, identifier):
        """èŽ·å–identifierçš„å”¯ä¸€é”®ç”¨äºŽåŽ»é‡"""
        if isinstance(identifier, dict):
            # ä½¿ç”¨typeå’Œvalueç»„åˆä½œä¸ºkey
            return f"{identifier.get('type', 'unknown')}:{identifier.get('value', '')}"
        else:
            # ç®€å•å€¼ç›´æŽ¥ä½¿ç”¨
            return str(identifier)
    
    def _merge_identifier_attributes(self, existing_id, new_id, processor_name):
        """åˆå¹¶ä¸¤ä¸ªç›¸åŒidentifierçš„å±žæ€§"""
        if isinstance(existing_id, dict) and isinstance(new_id, dict):
            merged = existing_id.copy()
            
            # åˆå¹¶å±žæ€§
            for key, value in new_id.items():
                if key not in merged:
                    merged[key] = value
                elif key == 'confidence':
                    # å–æ›´é«˜çš„confidence
                    merged[key] = max(merged.get(key, 0), value)
                elif key == 'discovered_by':
                    # è®°å½•å¤šä¸ªå‘çŽ°è€…
                    existing_sources = merged.get('discovered_by', [])
                    if isinstance(existing_sources, str):
                        existing_sources = [existing_sources]
                    if processor_name not in existing_sources:
                        existing_sources.append(processor_name)
                    merged['discovered_by'] = existing_sources
            
            # æ·»åŠ å½“å‰å¤„ç†å™¨ä¸ºå‘çŽ°è€…
            if 'discovered_by' not in merged:
                merged['discovered_by'] = [existing_id.get('discovered_by', 'unknown'), processor_name]
            
            return merged
        else:
            # ç®€å•å€¼ï¼Œè¿”å›žçŽ°æœ‰çš„
            return existing_id
    
    def _get_next_available_processor(self, processors, used_processors, identifier_data):
        """èŽ·å–ä¸‹ä¸€ä¸ªå¯ç”¨ä¸”æœªç”¨è¿‡çš„å¤„ç†å™¨ï¼ˆæŒ‰åˆ—è¡¨é¡ºåºï¼‰"""
        logger.debug(f"ðŸ” å¯»æ‰¾å¯ç”¨å¤„ç†å™¨: æ€»æ•°={len(processors)}, å·²ç”¨={list(used_processors)}")
        
        for processor in processors:
            if processor.name in used_processors:
                logger.debug(f"â­ï¸ è·³è¿‡å·²ä½¿ç”¨å¤„ç†å™¨: {processor.name}")
                continue
                
            can_handle = processor.can_handle(identifier_data)
            logger.debug(f"ðŸ¤” æ£€æŸ¥å¤„ç†å™¨ {processor.name}: can_handle={can_handle}")
            
            # ðŸ†• è¯¦ç»†è°ƒè¯•ï¼šæ˜¾ç¤ºå½“å‰identifier_dataçŠ¶æ€
            logger.debug(f"  ðŸ” å½“å‰identifier_dataçŠ¶æ€:")
            logger.debug(f"    ðŸ“– title: {identifier_data.title[:50] if identifier_data.title else 'None'}{'...' if identifier_data.title and len(identifier_data.title) > 50 else ''}")
            logger.debug(f"    ðŸ‘¥ authors: {len(identifier_data.authors) if identifier_data.authors else 0} ä¸ª")
            logger.debug(f"    ðŸ”— doi: {identifier_data.doi or 'None'}")
            logger.debug(f"    ðŸ“„ arxiv_id: {identifier_data.arxiv_id or 'None'}")
            logger.debug(f"    ðŸŒ url: {identifier_data.url or 'None'}")
            
            if can_handle:
                logger.info(f"âœ… é€‰æ‹©å¤„ç†å™¨: {processor.name}")
                return processor
            else:
                logger.debug(f"âŒ å¤„ç†å™¨æ— æ³•å¤„ç†å½“å‰æ ‡è¯†ç¬¦: {processor.name}")
        
        logger.warning(f"âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„å¤„ç†å™¨ (å‰©ä½™æœªç”¨: {[p.name for p in processors if p.name not in used_processors]})")
        return None
    
    async def _execute_single_processor(self, processor, identifier_data: IdentifierData):
        """æ‰§è¡Œå•ä¸ªå¤„ç†å™¨ï¼ˆå…¼å®¹åŒæ­¥/å¼‚æ­¥ï¼‰"""
        if hasattr(processor.process, '__call__'):
            import inspect
            if inspect.iscoroutinefunction(processor.process):
                result = await processor.process(identifier_data)
            else:
                import asyncio
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, processor.process, identifier_data)
            
            # ç¡®ä¿ç»“æžœåŒ…å«å¤„ç†å™¨åç§°
            if result:
                result.processor_name = processor.name
                result.source = processor.name  # ä¹Ÿè®¾ç½®sourceå­—æ®µ
            
            return result
        else:
            raise AttributeError(f"Processor {processor.name} has no process method")
    
    def _update_identifier_data_from_result(self, identifier_data: IdentifierData, result) -> IdentifierData:
        """åŸºäºŽå¤„ç†å™¨ç»“æžœæ›´æ–°identifier_dataï¼Œä¸ºåŽç»­å¤„ç†å™¨æä¾›æ›´å¤šä¿¡æ¯"""
        if not result or not result.metadata:
            logger.debug(f"ðŸ”„ [æ›´æ–°] æ— ç»“æžœæˆ–metadataï¼Œä¿æŒåŽŸidentifier_data")
            return identifier_data
            
        logger.debug(f"ðŸ”„ [æ›´æ–°] å¼€å§‹ä»Žå¤„ç†å™¨ç»“æžœæ›´æ–°identifier_data")
            
        # åˆ›å»ºæ–°çš„identifier_dataå‰¯æœ¬
        updated_data = IdentifierData(
            doi=identifier_data.doi,
            arxiv_id=identifier_data.arxiv_id,
            pmid=identifier_data.pmid,
            semantic_scholar_id=identifier_data.semantic_scholar_id,
            url=identifier_data.url,
            pdf_url=identifier_data.pdf_url,
            title=identifier_data.title,
            year=identifier_data.year,
            venue=identifier_data.venue,
            authors=identifier_data.authors,
            source_data=identifier_data.source_data,
            pdf_content=identifier_data.pdf_content,
            file_path=identifier_data.file_path
        )
        
        # ä»Žç»“æžœä¸­æ›´æ–°å­—æ®µï¼ˆå¦‚æžœidentifier_dataä¸­è¿˜æ²¡æœ‰è¿™äº›ä¿¡æ¯ï¼‰
        metadata = result.metadata
        
        # ðŸ”§ ä¿®å¤ï¼šMetadataModelå¯¹è±¡æ²¡æœ‰get()æ–¹æ³•ï¼Œç›´æŽ¥è®¿é—®å±žæ€§
        if not updated_data.title and metadata and hasattr(metadata, 'title') and metadata.title:
            updated_data.title = metadata.title
            logger.debug(f"ðŸ“ ä»Žå¤„ç†å™¨ç»“æžœæ›´æ–°title: {metadata.title}")
            
        if not updated_data.authors and metadata and hasattr(metadata, 'authors') and metadata.authors:
            updated_data.authors = metadata.authors
            logger.debug(f"ðŸ‘¥ ä»Žå¤„ç†å™¨ç»“æžœæ›´æ–°authors: {len(metadata.authors)} ä¸ªä½œè€…")
            
        if not updated_data.year and metadata and hasattr(metadata, 'year') and metadata.year:
            updated_data.year = metadata.year
            logger.debug(f"ðŸ“… ä»Žå¤„ç†å™¨ç»“æžœæ›´æ–°year: {metadata.year}")
            
        if not updated_data.venue and metadata and hasattr(metadata, 'journal') and metadata.journal:
            updated_data.venue = metadata.journal
            logger.debug(f"ðŸ“ ä»Žå¤„ç†å™¨ç»“æžœæ›´æ–°venue: {metadata.journal}")
            
        # ä»Žnew_identifiersä¸­æ›´æ–°æ ‡è¯†ç¬¦
        if result.new_identifiers:
            for identifier in result.new_identifiers:
                if hasattr(identifier, 'doi') and identifier.doi and not updated_data.doi:
                    updated_data.doi = identifier.doi
                    logger.debug(f"ðŸ”— ä»Žå¤„ç†å™¨ç»“æžœæ›´æ–°DOI: {identifier.doi}")
                elif hasattr(identifier, 'arxiv_id') and identifier.arxiv_id and not updated_data.arxiv_id:
                    updated_data.arxiv_id = identifier.arxiv_id
                    logger.debug(f"ðŸ”— ä»Žå¤„ç†å™¨ç»“æžœæ›´æ–°ArXiv ID: {identifier.arxiv_id}")
                elif hasattr(identifier, 'pmid') and identifier.pmid and not updated_data.pmid:
                    updated_data.pmid = identifier.pmid
                    logger.debug(f"ðŸ”— ä»Žå¤„ç†å™¨ç»“æžœæ›´æ–°PMID: {identifier.pmid}")
        
        logger.debug(f"ðŸ”„ [æ›´æ–°] å®Œæˆidentifier_dataæ›´æ–°")
        logger.debug(f"  ðŸ“– æ›´æ–°åŽtitle: {updated_data.title[:50] if updated_data.title else 'None'}{'...' if updated_data.title and len(updated_data.title) > 50 else ''}")
        logger.debug(f"  ðŸ‘¥ æ›´æ–°åŽauthors: {len(updated_data.authors) if updated_data.authors else 0} ä¸ª")
        logger.debug(f"  ðŸ”— æ›´æ–°åŽdoi: {updated_data.doi or 'None'}")
        logger.debug(f"  ðŸ“„ æ›´æ–°åŽarxiv_id: {updated_data.arxiv_id or 'None'}")
        
        return updated_data

    def _prepare_identifier_data(self, source_data: Dict, mapping_result: Optional[Dict]) -> IdentifierData:
        """å‡†å¤‡æ ‡è¯†ç¬¦æ•°æ®"""
        
        # åŸºç¡€æ ‡è¯†ç¬¦
        identifier_data = IdentifierData(
            doi=source_data.get("doi"),
            arxiv_id=source_data.get("arxiv_id"),
            pmid=source_data.get("pmid"),
            url=source_data.get("url"),
            source_data=source_data
        )
        
        # ä»ŽURLæ˜ å°„ç»“æžœä¸­æå–å¢žå¼ºä¿¡æ¯
        if mapping_result:
            identifier_data.title = mapping_result.get("title")
            identifier_data.year = mapping_result.get("year")
            identifier_data.venue = mapping_result.get("venue")
            identifier_data.authors = mapping_result.get("authors")
            
            # æ›´æ–°æ ‡è¯†ç¬¦
            if mapping_result.get("doi"):
                identifier_data.doi = mapping_result["doi"]
            if mapping_result.get("arxiv_id"):
                identifier_data.arxiv_id = mapping_result["arxiv_id"]
                
        return identifier_data
    
    def _get_available_processors(self, processor_names: List[str], identifier_data: IdentifierData):
        """èŽ·å–å¯ç”¨çš„å¤„ç†å™¨å®žä¾‹"""
        available = []
        
        for name in processor_names:
            try:
                processor = self.metadata_registry.get_processor(name)
                if processor.can_handle(identifier_data):
                    available.append(processor)
                    logger.debug(f"[æ™ºèƒ½è·¯ç”±] âœ… å¤„ç†å™¨å¯ç”¨: {name}")
                else:
                    logger.debug(f"[æ™ºèƒ½è·¯ç”±] å¤„ç†å™¨æ— æ³•å¤„ç†: {name}")
            except KeyError:
                logger.warning(f"[æ™ºèƒ½è·¯ç”±] å¤„ç†å™¨æœªæ³¨å†Œ: {name}")
            except Exception as e:
                logger.warning(f"[æ™ºèƒ½è·¯ç”±] å¤„ç†å™¨å®žä¾‹åŒ–å¤±è´¥: {name}, é”™è¯¯: {e}")
                
        return available
    
    def _format_value_for_debug(self, value) -> str:
        """æ ¼å¼åŒ–å€¼ç”¨äºŽè°ƒè¯•è¾“å‡º"""
        if value is None:
            return "None"
        elif isinstance(value, str):
            return f"'{value[:50]}{'...' if len(value) > 50 else ''}'"
        elif isinstance(value, list):
            return f"[{len(value)} items]"
        elif isinstance(value, dict):
            return f"{{{len(value)} keys}}"
        else:
            return str(value)[:50]
