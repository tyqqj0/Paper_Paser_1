"""
æ™ºèƒ½æ‰§è¡Œå™¨

åŸºäºè·¯ç”±çš„æ™ºèƒ½å¤„ç†å™¨æ‰§è¡Œç³»ç»Ÿï¼Œè‡ªåŠ¨åˆ¤æ–­å¹¶è¡Œæ‰§è¡Œã€‚
"""

import asyncio
import inspect
import logging
from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime

from ...services.url_mapping import get_url_mapping_service
from ..metadata.registry import get_global_registry
from ..metadata.base import IdentifierData
from .routing import RouteManager
from .hooks import HookManager

logger = logging.getLogger(__name__)


class ExecutionContext:
    """æ‰§è¡Œä¸Šä¸‹æ–‡ - è·Ÿè¸ªæ‰§è¡ŒçŠ¶æ€å’Œæ•°æ®"""
    
    def __init__(self, task_id: str, source_data: Dict[str, Any]):
        self.task_id = task_id
        self.source_data = source_data
        self.url = source_data.get('url', '')
        
        # æ‰§è¡ŒçŠ¶æ€
        self.executed_processors: Set[str] = set()
        self.results: Dict[str, Any] = {}
        self.metadata = None
        self.identifiers = {}
        
        # æ—¶é—´è·Ÿè¸ª
        self.start_time = datetime.now()
        self.processor_times: Dict[str, float] = {}
        
    def mark_processor_executed(self, processor_name: str, execution_time: float):
        """æ ‡è®°å¤„ç†å™¨å·²æ‰§è¡Œ"""
        self.executed_processors.add(processor_name)
        self.processor_times[processor_name] = execution_time
        logger.debug(f"å¤„ç†å™¨ {processor_name} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}s")
        
    def update_metadata(self, metadata, source: str):
        """æ›´æ–°å…ƒæ•°æ®"""
        if metadata:
            self.metadata = metadata
            self.results[f'metadata_from_{source}'] = metadata
            logger.info(f"âœ… å…ƒæ•°æ®å·²æ›´æ–°ï¼Œæ¥æº: {source}")
            
    def update_identifiers(self, identifiers: Dict[str, Any]):
        """æ›´æ–°æ ‡è¯†ç¬¦"""
        if identifiers:
            self.identifiers.update(identifiers)
            logger.debug(f"æ ‡è¯†ç¬¦å·²æ›´æ–°: {identifiers}")


class SmartExecutor:
    """æ™ºèƒ½æ‰§è¡Œå™¨ - æ ¸å¿ƒæ‰§è¡Œé€»è¾‘"""
    
    def __init__(self, dao=None):
        # å¤ç”¨ç°æœ‰ç»„ä»¶
        self.url_mapping_service = get_url_mapping_service()
        self.metadata_registry = get_global_registry()
        self.route_manager = RouteManager.get_instance()
        
        # ğŸ†• Hookç³»ç»Ÿ (å¦‚æœæä¾›äº†DAO)
        self.hook_manager = HookManager(dao) if dao else None
        
    async def execute_by_route(self, url: str, source_data: Dict[str, Any], task_id: str) -> Dict[str, Any]:
        """
        åŸºäºè·¯ç”±çš„æ™ºèƒ½æ‰§è¡Œå…¥å£
        
        Args:
            url: è¾“å…¥URL
            source_data: æºæ•°æ®
            task_id: ä»»åŠ¡ID
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        logger.info(f"ğŸš€ æ™ºèƒ½æ‰§è¡Œå™¨å¯åŠ¨: {url}")
        start_time = datetime.now()
        
        # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
        context = ExecutionContext(task_id, source_data)
        
        try:
            # é˜¶æ®µ1: URLæ˜ å°„ (å¤ç”¨ç°æœ‰æœåŠ¡)
            mapping_result = await self._perform_url_mapping(url)
            
            # é˜¶æ®µ2: è·¯ç”±å†³ç­–
            route = self.route_manager.determine_route(url, mapping_result)
            
            # é˜¶æ®µ3: æ™ºèƒ½æ‰§è¡Œå¤„ç†å™¨
            execution_results = await self._execute_processors_smart(route, context, mapping_result)
            
            # é˜¶æ®µ4: ç»“æœæ•´åˆ
            final_result = self._build_final_result(context, execution_results, route)
            
            # ğŸ†• é˜¶æ®µ5: Hookåå¤„ç†
            if self.hook_manager and final_result.get('status') == 'completed':
                await self._trigger_post_processing_hooks(final_result, context)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            final_result['execution_time'] = execution_time
            
            logger.info(f"âœ… æ™ºèƒ½æ‰§è¡Œå®Œæˆ: {url}, è€—æ—¶: {execution_time:.2f}s, è·¯ç”±: {route.name}")
            
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½æ‰§è¡Œå¤±è´¥: {url}, é”™è¯¯: {e}")
            # è¿”å›é”™è¯¯ä¿¡æ¯ï¼Œè®©ä¸Šå±‚å†³å®šæ˜¯å¦å›é€€åˆ°åŸæœ‰é€»è¾‘
            return {
                'status': 'failed',
                'error': str(e),
                'fallback_to_legacy': True,
                'execution_time': (datetime.now() - start_time).total_seconds()
            }
    
    async def _perform_url_mapping(self, url: str) -> Optional[Dict]:
        """æ‰§è¡ŒURLæ˜ å°„ (å¤ç”¨ç°æœ‰æœåŠ¡)"""
        try:
            logger.debug(f"ğŸ¯ æ‰§è¡ŒURLæ˜ å°„: {url}")
            mapping_result = self.url_mapping_service.map_url(url)  # âœ… ç§»é™¤awaitï¼Œè¿™æ˜¯åŒæ­¥æ–¹æ³•
            
            if mapping_result and mapping_result.is_successful():
                result_dict = mapping_result.to_dict()
                logger.info(f"âœ… URLæ˜ å°„æˆåŠŸ: æ‰¾åˆ° {len([k for k, v in result_dict.items() if v])} ä¸ªæœ‰æ•ˆå­—æ®µ")
                return result_dict
            else:
                logger.warning(f"âš ï¸ URLæ˜ å°„æœªæ‰¾åˆ°æ ‡è¯†ç¬¦: {url}")
                return None
                
        except Exception as e:
            logger.warning(f"âš ï¸ URLæ˜ å°„å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None
    
    async def _execute_processors_smart(self, route, context: ExecutionContext, mapping_result: Optional[Dict]) -> List[Dict]:
        """æ™ºèƒ½æ‰§è¡Œå¤„ç†å™¨ - è‡ªåŠ¨åˆ¤æ–­å¹¶è¡Œ"""
        
        # å‡†å¤‡æ ‡è¯†ç¬¦æ•°æ®
        identifier_data = self._prepare_identifier_data(context.source_data, mapping_result)
        
        # è·å–å¯ç”¨çš„å¤„ç†å™¨
        available_processors = self._get_available_processors(route.processors, identifier_data)
        
        if not available_processors:
            logger.warning(f"æœªæ‰¾åˆ°å¯ç”¨çš„å¤„ç†å™¨ï¼Œè·¯ç”±: {route.name}")
            return []
        
        logger.info(f"ğŸª å¯ç”¨å¤„ç†å™¨: {[p.name for p in available_processors]} (è·¯ç”±: {route.name})")
        
        # æ ¹æ®è·¯ç”±ç±»å‹å†³å®šæ‰§è¡Œç­–ç•¥
        if self.route_manager.is_fast_path(route):
            return await self._execute_fast_path(available_processors, identifier_data, context)
        else:
            return await self._execute_parallel_processors(available_processors, identifier_data, context)
    
    async def _execute_fast_path(self, processors, identifier_data: IdentifierData, context: ExecutionContext) -> List[Dict]:
        """å¿«é€Ÿè·¯å¾„æ‰§è¡Œ - åªä½¿ç”¨ç¬¬ä¸€ä¸ªå¤„ç†å™¨"""
        if not processors:
            return []
            
        processor = processors[0]  # å¿«é€Ÿè·¯å¾„åªç”¨æœ€ä¼˜å¤„ç†å™¨
        logger.info(f"âš¡ å¿«é€Ÿè·¯å¾„æ‰§è¡Œ: {processor.name}")
        
        start_time = datetime.now()
        try:
            # ğŸ”§ æ£€æŸ¥å¤„ç†å™¨çš„processæ–¹æ³•æ˜¯å¦æ˜¯å¼‚æ­¥çš„
            if inspect.iscoroutinefunction(processor.process):
                # å¼‚æ­¥å¤„ç†å™¨
                result = await processor.process(identifier_data)
            else:
                # åŒæ­¥å¤„ç†å™¨ï¼Œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, processor.process, identifier_data)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            context.mark_processor_executed(processor.name, execution_time)
            
            if result.is_valid:
                context.update_metadata(result.metadata, processor.name)
                if result.new_identifiers:
                    context.update_identifiers(result.new_identifiers)
                return [{'processor': processor.name, 'result': result, 'success': True}]
            else:
                logger.warning(f"âš ï¸ å¿«é€Ÿè·¯å¾„å¤„ç†å™¨å¤±è´¥: {processor.name}, é”™è¯¯: {result.error}")
                return [{'processor': processor.name, 'result': result, 'success': False}]
                
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            context.mark_processor_executed(processor.name, execution_time)
            logger.error(f"âŒ å¿«é€Ÿè·¯å¾„å¤„ç†å™¨å¼‚å¸¸: {processor.name}, é”™è¯¯: {e}")
            return [{'processor': processor.name, 'error': str(e), 'success': False}]
    
    async def _execute_parallel_processors(self, processors, identifier_data: IdentifierData, context: ExecutionContext) -> List[Dict]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå¤„ç†å™¨"""
        logger.info(f"ğŸ”„ å¹¶è¡Œæ‰§è¡Œå¤„ç†å™¨: {[p.name for p in processors]}")
        
        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        tasks = {}
        for processor in processors:
            task = asyncio.create_task(self._execute_single_processor(processor, identifier_data, context))
            tasks[processor.name] = task
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ (æˆ–å¿«é€Ÿå¤±è´¥)
        results = []
        completed_tasks = await asyncio.gather(*tasks.values(), return_exceptions=True)
        
        for i, (processor_name, result) in enumerate(zip(tasks.keys(), completed_tasks)):
            if isinstance(result, Exception):
                logger.error(f"âŒ å¤„ç†å™¨å¼‚å¸¸: {processor_name}, é”™è¯¯: {result}")
                results.append({'processor': processor_name, 'error': str(result), 'success': False})
            else:
                results.append(result)
        
        # ç»Ÿè®¡æˆåŠŸçš„å¤„ç†å™¨
        successful = [r for r in results if r.get('success')]
        logger.info(f"ğŸ“Š å¤„ç†å™¨æ‰§è¡Œå®Œæˆ: {len(successful)}/{len(processors)} æˆåŠŸ")
        
        return results
    
    async def _execute_single_processor(self, processor, identifier_data: IdentifierData, context: ExecutionContext) -> Dict:
        """æ‰§è¡Œå•ä¸ªå¤„ç†å™¨ - è‡ªåŠ¨å¤„ç†async/syncå…¼å®¹æ€§"""
        start_time = datetime.now()
        
        try:
            # ğŸ”§ æ£€æŸ¥å¤„ç†å™¨çš„processæ–¹æ³•æ˜¯å¦æ˜¯å¼‚æ­¥çš„
            if inspect.iscoroutinefunction(processor.process):
                # å¼‚æ­¥å¤„ç†å™¨
                result = await processor.process(identifier_data)
            else:
                # åŒæ­¥å¤„ç†å™¨ï¼Œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, processor.process, identifier_data)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            context.mark_processor_executed(processor.name, execution_time)
            
            if result.is_valid:
                context.update_metadata(result.metadata, processor.name)
                if result.new_identifiers:
                    context.update_identifiers(result.new_identifiers)
                    
                logger.info(f"âœ… å¤„ç†å™¨æˆåŠŸ: {processor.name} (ç½®ä¿¡åº¦: {result.confidence:.2f})")
                return {'processor': processor.name, 'result': result, 'success': True}
            else:
                logger.warning(f"âš ï¸ å¤„ç†å™¨å¤±è´¥: {processor.name}, é”™è¯¯: {result.error}")
                return {'processor': processor.name, 'result': result, 'success': False}
                
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            context.mark_processor_executed(processor.name, execution_time)
            logger.error(f"âŒ å¤„ç†å™¨å¼‚å¸¸: {processor.name}, é”™è¯¯: {e}")
            return {'processor': processor.name, 'error': str(e), 'success': False}
    
    def _prepare_identifier_data(self, source_data: Dict, mapping_result: Optional[Dict]) -> IdentifierData:
        """å‡†å¤‡æ ‡è¯†ç¬¦æ•°æ® (å¤ç”¨ç°æœ‰é€»è¾‘)"""
        
        # åŸºç¡€æ ‡è¯†ç¬¦
        identifier_data = IdentifierData(
            doi=source_data.get("doi"),
            arxiv_id=source_data.get("arxiv_id"),
            pmid=source_data.get("pmid"),
            url=source_data.get("url"),
            source_data=source_data
        )
        
        # ä»URLæ˜ å°„ç»“æœä¸­æå–å¢å¼ºä¿¡æ¯
        if mapping_result:
            identifier_data.title = mapping_result.get("title")
            identifier_data.year = mapping_result.get("year")
            identifier_data.venue = mapping_result.get("venue")
            identifier_data.authors = mapping_result.get("authors")
            
            # æ›´æ–°æ ‡è¯†ç¬¦
            if mapping_result.get("doi"):
                identifier_data.doi = mapping_result["doi"]
            if mapping_result.get("arxiv_id"):
                identifier_data.arxiv_id = mapping_result["arxiv_id"]
                
        return identifier_data
    
    def _get_available_processors(self, processor_names: List[str], identifier_data: IdentifierData):
        """è·å–å¯ç”¨çš„å¤„ç†å™¨å®ä¾‹"""
        available = []
        
        for name in processor_names:
            try:
                processor = self.metadata_registry.get_processor(name)
                if processor.can_handle(identifier_data):
                    available.append(processor)
                else:
                    logger.debug(f"å¤„ç†å™¨ {name} æ— æ³•å¤„ç†å½“å‰æ ‡è¯†ç¬¦")
            except KeyError:
                logger.warning(f"æœªæ‰¾åˆ°å¤„ç†å™¨: {name}")
            except Exception as e:
                logger.warning(f"è·å–å¤„ç†å™¨å¤±è´¥: {name}, é”™è¯¯: {e}")
                
        return available
    
    def _build_final_result(self, context: ExecutionContext, execution_results: List[Dict], route) -> Dict[str, Any]:
        """æ„å»ºæœ€ç»ˆç»“æœ"""
        
        # é€‰æ‹©æœ€ä½³ç»“æœ
        successful_results = [r for r in execution_results if r.get('success')]
        
        if successful_results:
            # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„ç»“æœ
            best_result = max(successful_results, 
                            key=lambda r: r.get('result', {}).get('confidence', 0))
            
            total_time = (datetime.now() - context.start_time).total_seconds()
            
            return {
                'status': 'completed',
                'result_type': 'created',  # æš‚æ—¶ç¡¬ç¼–ç ï¼Œåç»­é€šè¿‡Hookå¤„ç†
                'literature_id': f"smart-route-{context.task_id[:8]}",  # ä¸´æ—¶IDï¼Œç”¨äºæ¼”ç¤º
                'metadata': context.metadata,
                'processor_used': best_result['processor'],
                'confidence': best_result['result'].confidence,
                'route_used': route.name,
                'execution_time': total_time,
                'processor_times': context.processor_times,
                'url_validation_status': 'success',
                'original_url': context.url
            }
        else:
            # æ‰€æœ‰å¤„ç†å™¨éƒ½å¤±è´¥
            return {
                'status': 'failed',
                'error': 'All processors failed',
                'attempted_processors': [r.get('processor') for r in execution_results],
                'route_used': route.name,
                'execution_time': (datetime.now() - context.start_time).total_seconds(),
                'fallback_to_legacy': True  # å»ºè®®å›é€€åˆ°åŸæœ‰é€»è¾‘
            }
    
    async def _trigger_post_processing_hooks(self, final_result: Dict, context: ExecutionContext):
        """è§¦å‘åå¤„ç†Hook"""
        try:
            if not self.hook_manager:
                return
            
            # å‡†å¤‡Hookä¸Šä¸‹æ–‡
            hook_context = {
                'literature_id': final_result.get('literature_id'),
                'metadata': context.metadata,
                'url_info': {'url': context.url},
                'processor_used': final_result.get('processor_used'),
                'confidence': final_result.get('confidence'),
                'task_id': context.task_id
            }
            
            # è§¦å‘æ–‡çŒ®åˆ›å»ºäº‹ä»¶
            if final_result.get('result_type') == 'created':
                logger.info(f"ğŸ¯ [Hook] è§¦å‘æ–‡çŒ®åˆ›å»ºäº‹ä»¶: {hook_context['literature_id']}")
                hook_results = await self.hook_manager.trigger_event('literature_created', hook_context)
                final_result['hook_results'] = hook_results
            
            # è§¦å‘å…ƒæ•°æ®æ›´æ–°äº‹ä»¶
            if context.metadata:
                logger.info(f"ğŸ¯ [Hook] è§¦å‘å…ƒæ•°æ®æ›´æ–°äº‹ä»¶: {hook_context['literature_id']}")
                hook_results = await self.hook_manager.trigger_event('metadata_updated', hook_context)
                final_result['hook_results'] = hook_results
                
        except Exception as e:
            logger.error(f"âŒ Hookåå¤„ç†å¤±è´¥: {e}")
            # Hookå¤±è´¥ä¸åº”è¯¥å½±å“ä¸»æµç¨‹
