"""
æ•°æ®ç®¡é“ - ç»Ÿä¸€çš„æ•°æ®åº“æ“ä½œå’ŒHookç³»ç»Ÿ

è´Ÿè´£ï¼š
1. çŠ¶æ€æ£€æŸ¥ - åˆ¤æ–­æ•°æ®æ˜¯å¦å¯ä»¥å¤„ç†
2. å»é‡æ£€æŸ¥ - ç»Ÿä¸€çš„å»é‡é€»è¾‘ 
3. æ•°æ®å†™å…¥ - ç»Ÿä¸€çš„æ•°æ®åº“æ“ä½œ
4. Hookè§¦å‘ - äº‹ä»¶é©±åŠ¨çš„åå¤„ç†
"""

import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
from enum import Enum

logger = logging.getLogger(__name__)


class DataEvent(Enum):
    """æ•°æ®äº‹ä»¶ç±»å‹"""
    METADATA_EXTRACTED = "metadata_extracted"
    DUPLICATE_FOUND = "duplicate_found"
    LITERATURE_CREATED = "literature_created"
    IDENTIFIERS_UPDATED = "identifiers_updated"


class DataPipeline:
    """æ•°æ®ç®¡é“ - ç»Ÿä¸€çš„æ•°æ®å¤„ç†æµç¨‹"""
    
    def __init__(self, dao):
        self.dao = dao
        self.hooks = []  # Hookåˆ—è¡¨
        
    async def process_data(self, raw_data: Dict[str, Any], source_data: Dict[str, Any], 
                          mapping_result: Optional[Dict], route_info: Dict, task_id: str) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„æ•°æ®å¤„ç†å…¥å£
        
        æµç¨‹ï¼šåŸå§‹æ•°æ® â†’ çŠ¶æ€æ£€æŸ¥ â†’ å»é‡æ£€æŸ¥ â†’ æ•°æ®å†™å…¥/è¿”å›é‡å¤
        """
        logger.info(f"ğŸ“‹ [æ•°æ®ç®¡é“] å¼€å§‹å¤„ç†æ•°æ®ï¼Œä»»åŠ¡: {task_id}")
        
        try:
            # é˜¶æ®µ1: çŠ¶æ€æ£€æŸ¥ - åˆ¤æ–­æ•°æ®æ˜¯å¦å¯ä»¥å¤„ç†
            if not self._can_process_data(raw_data):
                return {
                    'status': 'failed',
                    'error': 'Data quality insufficient for processing',
                    'data_quality': self._evaluate_data_quality(raw_data)
                }
            
            # é˜¶æ®µ2: æ„å»ºæ ‡å‡†åŒ–çš„æ–‡çŒ®æ•°æ®
            literature_data = await self._build_literature_data(raw_data, source_data, mapping_result)
            
            # é˜¶æ®µ3: ç»Ÿä¸€å»é‡æ£€æŸ¥
            duplicate_result = await self._unified_deduplication(literature_data, task_id)
            if duplicate_result['is_duplicate']:
                logger.info(f"ğŸ” [æ•°æ®ç®¡é“] å‘ç°é‡å¤æ–‡çŒ®: {duplicate_result['existing_lid']}")
                
                # è§¦å‘é‡å¤å‘ç°äº‹ä»¶
                await self._trigger_event(DataEvent.DUPLICATE_FOUND, {
                    'existing_lid': duplicate_result['existing_lid'],
                    'new_source': source_data,
                    'task_id': task_id
                })
                
                return {
                    'status': 'completed',
                    'result_type': 'duplicate',
                    'literature_id': duplicate_result['existing_lid'],
                    'duplicate_reason': duplicate_result['reason']
                }
            
            # é˜¶æ®µ4: å†™å…¥æ–°æ–‡çŒ®æ•°æ®
            new_literature = await self._create_literature(literature_data, task_id)
            
            # é˜¶æ®µ5: è§¦å‘åˆ›å»ºå®Œæˆäº‹ä»¶
            await self._trigger_event(DataEvent.LITERATURE_CREATED, {
                'literature': new_literature,
                'source_data': source_data,
                'task_id': task_id
            })
            
            logger.info(f"âœ… [æ•°æ®ç®¡é“] æ–‡çŒ®åˆ›å»ºå®Œæˆ: {new_literature['lid']}")
            
            return {
                'status': 'completed',
                'result_type': 'created', 
                'literature_id': new_literature['lid'],
                'processor_used': raw_data.get('processor_used'),
                'confidence': raw_data.get('confidence')
            }
            
        except Exception as e:
            logger.error(f"âŒ [æ•°æ®ç®¡é“] å¤„ç†å¤±è´¥: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'fallback_to_legacy': True
            }
    
    def _can_process_data(self, raw_data: Dict[str, Any]) -> bool:
        """çŠ¶æ€æ£€æŸ¥ - åˆ¤æ–­æ•°æ®æ˜¯å¦å¯ä»¥å¤„ç†"""
        
        # ğŸ”§ é˜²æŠ¤æ€§æ£€æŸ¥
        if not raw_data or not isinstance(raw_data, dict):
            logger.warning(f"[æ•°æ®ç®¡é“] raw_dataä¸ºç©ºæˆ–ä¸æ˜¯å­—å…¸: {type(raw_data)}")
            return False
        
        # åŸºæœ¬æˆåŠŸæ£€æŸ¥
        if not raw_data.get('success'):
            logger.warning(f"[æ•°æ®ç®¡é“] å¤„ç†å™¨æ ‡è®°ä¸ºå¤±è´¥: {raw_data.get('error', 'Unknown error')}")
            return False
        
        # å…ƒæ•°æ®è´¨é‡æ£€æŸ¥
        metadata = raw_data.get('metadata')
        if not metadata:
            logger.warning(f"[æ•°æ®ç®¡é“] æ²¡æœ‰å…ƒæ•°æ®")
            return False
        
        # æ£€æŸ¥æ ‡é¢˜ - MetadataModelå¯¹è±¡åº”è¯¥æœ‰titleå±æ€§
        try:
            title = getattr(metadata, 'title', None)
            if not title or title in ['Unknown Title', 'Processing...']:
                logger.warning(f"[æ•°æ®ç®¡é“] æ ‡é¢˜æ— æ•ˆ: {title}")
                return False
            
        except Exception as e:
            logger.warning(f"[æ•°æ®ç®¡é“] æ ‡é¢˜æ£€æŸ¥å¼‚å¸¸: {e}, metadataç±»å‹: {type(metadata)}")
            return False
        
        logger.debug(f"[æ•°æ®ç®¡é“] æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡: {metadata.title}")
        return True
    
    def _evaluate_data_quality(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        # ğŸ”§ é˜²æŠ¤æ€§æ£€æŸ¥
        if not raw_data or not isinstance(raw_data, dict):
            return {'score': 0, 'issues': ['Invalid raw_data']}
        
        metadata = raw_data.get('metadata')
        if not metadata:
            return {'score': 0, 'issues': ['No metadata']}
        
        score = 0
        issues = []
        
        # æ ‡é¢˜æ£€æŸ¥ (30åˆ†)
        title = getattr(metadata, 'title', None)
        if title and title not in ['Unknown Title', 'Processing...']:
            score += 30
        else:
            issues.append('Missing or invalid title')
        
        # ä½œè€…æ£€æŸ¥ (25åˆ†)
        authors = getattr(metadata, 'authors', None)
        if authors and len(authors) > 0:
            score += 25
        else:
            issues.append('Missing authors')
        
        # å¹´ä»½æ£€æŸ¥ (20åˆ†)
        year = getattr(metadata, 'year', None)
        if year:
            score += 20
        else:
            issues.append('Missing year')
        
        # æœŸåˆŠæ£€æŸ¥ (15åˆ†)
        journal = getattr(metadata, 'journal', None)
        if journal:
            score += 15
        else:
            issues.append('Missing journal')
        
        # æ‘˜è¦æ£€æŸ¥ (10åˆ†)
        abstract = getattr(metadata, 'abstract', None)
        if abstract:
            score += 10
        else:
            issues.append('Missing abstract')
        
        return {'score': score, 'issues': issues}
    
    async def _build_literature_data(self, raw_data: Dict, source_data: Dict, mapping_result: Optional[Dict]) -> Dict:
        """æ„å»ºæ ‡å‡†åŒ–çš„æ–‡çŒ®æ•°æ®"""
        
        # ğŸ”§ é˜²æŠ¤æ€§æ£€æŸ¥ï¼šç¡®ä¿raw_dataä¸ä¸ºNone
        if not raw_data:
            raise ValueError("raw_data cannot be None")
        
        metadata = raw_data.get('metadata')
        new_identifiers = raw_data.get('new_identifiers') or {}
        
        # ğŸ”§ ç¡®ä¿source_dataå’Œmapping_resultä¸ä¸ºNone
        source_data = source_data or {}
        mapping_result = mapping_result or {}
        
        # æ„å»ºæ ‡è¯†ç¬¦
        identifiers = {
            'doi': source_data.get('doi') or new_identifiers.get('doi'),
            'arxiv_id': source_data.get('arxiv_id') or new_identifiers.get('arxiv_id'),
            'pmid': source_data.get('pmid') or new_identifiers.get('pmid'),
            'url': source_data.get('url')
        }
        
        # ä»URLæ˜ å°„ç»“æœè¡¥å……æ ‡è¯†ç¬¦
        if mapping_result:
            if mapping_result.get('doi') and not identifiers['doi']:
                identifiers['doi'] = mapping_result['doi']
            if mapping_result.get('arxiv_id') and not identifiers['arxiv_id']:
                identifiers['arxiv_id'] = mapping_result['arxiv_id']

        
        return {
            'identifiers': identifiers,
            'metadata': metadata,
            'source_data': source_data,
            'processor_info': {
                'processor_used': raw_data.get('processor_used'),
                'confidence': raw_data.get('confidence')
            }
        }
    
    async def _unified_deduplication(self, literature_data: Dict, task_id: str) -> Dict[str, Any]:
        """ç»Ÿä¸€å»é‡æ£€æŸ¥ - æ›¿ä»£åŸæœ‰çš„5æ¬¡å»é‡"""
        
        # ğŸ”§ é˜²æŠ¤æ€§æ£€æŸ¥
        if not literature_data or not isinstance(literature_data, dict):
            logger.warning(f"[æ•°æ®ç®¡é“] literature_dataæ— æ•ˆ: {type(literature_data)}")
            return {'is_duplicate': False}
        
        identifiers = literature_data.get('identifiers', {})
        metadata = literature_data.get('metadata')
        
        if not metadata:
            logger.warning(f"[æ•°æ®ç®¡é“] æ²¡æœ‰å…ƒæ•°æ®ï¼Œè·³è¿‡å»é‡æ£€æŸ¥")
            return {'is_duplicate': False}
        
        logger.info(f"ğŸ” [æ•°æ®ç®¡é“] å¼€å§‹ç»Ÿä¸€å»é‡æ£€æŸ¥ï¼Œä»»åŠ¡: {task_id}")
        
        try:
            # ä¼˜å…ˆçº§1: DOIå»é‡ (æœ€å¯é )
            if identifiers.get('doi'):
                existing = await self.dao.find_by_doi(identifiers['doi'])
                if existing:
                    return {
                        'is_duplicate': True,
                        'existing_lid': existing.lid,
                        'reason': f"DOIé‡å¤: {identifiers['doi']}"
                    }
            
            # ä¼˜å…ˆçº§2: ArXiv IDå»é‡
            if identifiers.get('arxiv_id'):
                existing = await self.dao.find_by_arxiv_id(identifiers['arxiv_id'])
                if existing:
                    return {
                        'is_duplicate': True,
                        'existing_lid': existing.lid,
                        'reason': f"ArXiv IDé‡å¤: {identifiers['arxiv_id']}"
                    }
            
            # ä¼˜å…ˆçº§3: æ ‡é¢˜+ä½œè€…å»é‡ (æ¨¡ç³ŠåŒ¹é…)
            if hasattr(metadata, 'title') and metadata.title:
                candidates = await self.dao.find_by_title_fuzzy(metadata.title, limit=5)
                for candidate in candidates:
                    if candidate and candidate.metadata and candidate.metadata.title:
                        if self._is_title_match(metadata.title, candidate.metadata.title):
                            # è¿›ä¸€æ­¥æ£€æŸ¥ä½œè€…åŒ¹é…
                            metadata_authors = getattr(metadata, 'authors', None)
                            candidate_authors = getattr(candidate.metadata, 'authors', None)
                            if self._is_author_match(metadata_authors, candidate_authors):
                                return {
                                    'is_duplicate': True,
                                    'existing_lid': candidate.lid,
                                    'reason': f"æ ‡é¢˜+ä½œè€…é‡å¤: {metadata.title[:50]}..."
                                }
            
            logger.info(f"âœ… [æ•°æ®ç®¡é“] å»é‡æ£€æŸ¥å®Œæˆï¼Œæ— é‡å¤")
            return {'is_duplicate': False}
            
        except Exception as e:
            logger.error(f"âŒ [æ•°æ®ç®¡é“] å»é‡æ£€æŸ¥å¼‚å¸¸: {e}")
            # å»é‡å¤±è´¥æ—¶ä¿å®ˆå¤„ç†ï¼Œè¿”å›æ— é‡å¤
            return {'is_duplicate': False}
    
    def _is_title_match(self, title1: str, title2: str, threshold: float = 0.8) -> bool:
        """æ ‡é¢˜åŒ¹é…æ£€æŸ¥"""
        if not title1 or not title2:
            return False
        
        # ç®€åŒ–çš„æ ‡é¢˜åŒ¹é… (å¯ä»¥åç»­ä¼˜åŒ–)
        from difflib import SequenceMatcher
        similarity = SequenceMatcher(None, title1.lower(), title2.lower()).ratio()
        return similarity >= threshold
    
    def _is_author_match(self, authors1, authors2, threshold: float = 0.6) -> bool:
        """ä½œè€…åŒ¹é…æ£€æŸ¥"""
        # ğŸ”§ é˜²æŠ¤æ€§æ£€æŸ¥
        if not authors1 or not authors2:
            return False
        
        # ç¡®ä¿æ˜¯åˆ—è¡¨
        if not isinstance(authors1, list):
            return False
        if not isinstance(authors2, list):
            return False
        
        try:
            # æå–ä½œè€…å§“å
            names1 = set()
            for author in authors1:
                if isinstance(author, dict):
                    name = author.get('name', '')
                elif isinstance(author, str):
                    name = author
                else:
                    # å¦‚æœauthoræœ‰nameå±æ€§
                    name = getattr(author, 'name', '')
                
                if name:
                    names1.add(name.strip())
            
            names2 = set()
            for author in authors2:
                if isinstance(author, dict):
                    name = author.get('name', '')
                elif isinstance(author, str):
                    name = author
                else:
                    # å¦‚æœauthoræœ‰nameå±æ€§
                    name = getattr(author, 'name', '')
                
                if name:
                    names2.add(name.strip())
            
            # è®¡ç®—äº¤é›†æ¯”ä¾‹
            if len(names1) == 0 or len(names2) == 0:
                return False
            
            intersection = len(names1 & names2)
            min_authors = min(len(names1), len(names2))
            similarity = intersection / min_authors
            
            return similarity >= threshold
            
        except Exception as e:
            logger.warning(f"[æ•°æ®ç®¡é“] ä½œè€…åŒ¹é…æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    async def _create_literature(self, literature_data: Dict, task_id: str) -> Dict[str, Any]:
        """åˆ›å»ºæ–°æ–‡çŒ® - ç»Ÿä¸€çš„æ•°æ®åº“å†™å…¥"""
        
        logger.info(f"ğŸ“ [æ•°æ®ç®¡é“] åˆ›å»ºæ–°æ–‡çŒ®ï¼Œä»»åŠ¡: {task_id}")
        
        # ç”ŸæˆLID
        try:
            from ...services.lid_generator import LIDGenerator
            lid_generator = LIDGenerator()
            lid = lid_generator.generate_lid(literature_data['metadata'])
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œç”Ÿæˆä¸€ä¸ªä¸´æ—¶ID
            import uuid
            lid = f"temp-{task_id[:8]}-{str(uuid.uuid4())[:8]}"
            logger.warning(f"[æ•°æ®ç®¡é“] LIDç”Ÿæˆå™¨å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ä¸´æ—¶ID: {lid}")
        except Exception as e:
            # å¦‚æœç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ä¸´æ—¶ID
            import uuid
            lid = f"temp-{task_id[:8]}-{str(uuid.uuid4())[:8]}"
            logger.error(f"[æ•°æ®ç®¡é“] LIDç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ä¸´æ—¶ID: {lid}")
        
        # åˆ›å»ºå ä½ç¬¦
        identifiers_model = self._build_identifiers_model(literature_data['identifiers'])
        literature_id = await self.dao.create_placeholder(task_id, identifiers_model)
        
        # æ„å»ºå®Œæ•´çš„æ–‡çŒ®æ¨¡å‹
        literature_model = self._build_literature_model(literature_data, lid, task_id)
        
        # æœ€ç»ˆåŒ–æ–‡çŒ®
        await self.dao.finalize_literature(literature_id, literature_model)
        
        logger.info(f"âœ… [æ•°æ®ç®¡é“] æ–‡çŒ®åˆ›å»ºå®Œæˆ: {lid}")
        
        return {
            'lid': lid,
            'internal_id': literature_id,
            'metadata': literature_data['metadata'],
            'identifiers': literature_data['identifiers']
        }
    
    def _build_identifiers_model(self, identifiers: Dict):
        """æ„å»ºæ ‡è¯†ç¬¦æ¨¡å‹"""
        try:
            from ...models.literature import IdentifiersModel
            return IdentifiersModel(
                doi=identifiers.get('doi'),
                arxiv_id=identifiers.get('arxiv_id'),
                pmid=identifiers.get('pmid')
            )
        except ImportError as e:
            logger.error(f"[æ•°æ®ç®¡é“] æ— æ³•å¯¼å…¥IdentifiersModel: {e}")
            # è¿”å›ä¸€ä¸ªç®€å•çš„å­—å…¸ä½œä¸ºå¤‡é€‰
            return {
                'doi': identifiers.get('doi'),
                'arxiv_id': identifiers.get('arxiv_id'),
                'pmid': identifiers.get('pmid')
            }
    
    def _build_literature_model(self, literature_data: Dict, lid: str, task_id: str):
        """æ„å»ºæ–‡çŒ®æ¨¡å‹"""
        try:
            from ...models.literature import LiteratureModel, ContentModel, TaskInfoModel
            from datetime import datetime
            
            # ä»»åŠ¡ä¿¡æ¯
            task_info = TaskInfoModel(
                task_id=task_id,
                status="completed",
                created_at=datetime.now(),
                completed_at=datetime.now()
            )
            
            # æ„å»ºæ–‡çŒ®æ¨¡å‹
            return LiteratureModel(
                lid=lid,
                task_info=task_info,
                identifiers=self._build_identifiers_model(literature_data['identifiers']),
                metadata=literature_data['metadata'],
                content=ContentModel(),  # ç©ºå†…å®¹ï¼Œåç»­å¡«å……
                references=[]  # å¼•ç”¨ä¿¡æ¯åç»­å¤„ç†
            )
        except ImportError as e:
            logger.error(f"[æ•°æ®ç®¡é“] æ— æ³•å¯¼å…¥æ–‡çŒ®æ¨¡å‹: {e}")
            # è¿”å›ç®€åŒ–çš„å­—å…¸ç»“æ„
            from datetime import datetime
            return {
                'lid': lid,
                'task_id': task_id,
                'status': 'completed',
                'created_at': datetime.now(),
                'identifiers': literature_data['identifiers'],
                'metadata': literature_data['metadata'],
                'content': {},
                'references': []
            }
    
    async def _trigger_event(self, event: DataEvent, context: Dict[str, Any]):
        """è§¦å‘äº‹ä»¶ - Hookç³»ç»Ÿçš„å…¥å£"""
        logger.info(f"ğŸ¯ [æ•°æ®ç®¡é“] è§¦å‘äº‹ä»¶: {event.value}")
        
        # è¿™é‡Œå¯ä»¥æ‰©å±•Hookç³»ç»Ÿ
        # ç›®å‰ç®€åŒ–å¤„ç†ï¼Œåç»­å¯ä»¥æ·»åŠ å…·ä½“çš„Hookå®ç°
        
        if event == DataEvent.DUPLICATE_FOUND:
            await self._handle_duplicate_found(context)
        elif event == DataEvent.LITERATURE_CREATED:
            await self._handle_literature_created(context)
    
    async def _handle_duplicate_found(self, context: Dict[str, Any]):
        """å¤„ç†é‡å¤å‘ç°äº‹ä»¶"""
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ åˆ«åæ˜ å°„ç­‰é€»è¾‘
        logger.info(f"ğŸ”— [æ•°æ®ç®¡é“] å¤„ç†é‡å¤å‘ç°: {context['existing_lid']}")
    
    async def _handle_literature_created(self, context: Dict[str, Any]):
        """å¤„ç†æ–‡çŒ®åˆ›å»ºäº‹ä»¶"""
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ åˆ«ååˆ›å»ºã€å…³ç³»å»ºç«‹ç­‰é€»è¾‘
        logger.info(f"ğŸ†• [æ•°æ®ç®¡é“] å¤„ç†æ–‡çŒ®åˆ›å»º: {context['literature']['lid']}")
