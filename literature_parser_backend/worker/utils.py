"""
Worker utility functions.
"""

import re
from datetime import datetime
from typing import Any, Dict, Optional, Tuple

from celery import current_task
from loguru import logger

from ..models.literature import (
    AuthorModel,
    IdentifiersModel,
    MetadataModel,
)


def update_task_status(
    stage: str,
    progress: Optional[int] = None,
    details: Optional[str] = None,
) -> None:
    """Update the current task's status with stage information."""
    if current_task:
        meta: Dict[str, Any] = {
            "stage": stage,
            "timestamp": datetime.now().isoformat(),
        }
        if progress is not None:
            meta["progress"] = progress
        if details:
            meta["details"] = details

        current_task.update_state(
            state="PROGRESS",
            meta={"stage": stage, "progress": progress, "details": details},
        )
        logger.info(
            f"Task {current_task.request.id}: {stage} - {details or 'In progress'}",
        )


def extract_authoritative_identifiers(
    source: Dict[str, Any],
) -> Tuple[IdentifiersModel, str, Optional[Dict[str, Any]]]:
    """
    Extract authoritative identifiers from source data.
    Priority: DOI > ArXiv ID > URL mapping > Generated fingerprint

    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. ä¼˜å…ˆä½¿ç”¨ç›´æŽ¥æä¾›çš„DOI/ArXiv IDï¼ˆæœ€å¯é ï¼‰
    2. å¦‚æžœæ²¡æœ‰ç›´æŽ¥æ ‡è¯†ç¬¦ï¼Œå†å°è¯•URLæ˜ å°„
    3. å¯¹äºŽå·²çŸ¥é—®é¢˜åŸŸåï¼ˆå¦‚Semantic Scholarï¼‰ï¼Œè·³è¿‡URLéªŒè¯

    Returns:
        Tuple of (identifiers, primary_type, url_validation_info)
    """
    identifiers = IdentifiersModel(doi=None, arxiv_id=None, fingerprint=None)
    primary_type = None
    url_validation_info = None

    # ðŸ” DEBUG: Log input data structure for troubleshooting
    logger.info(f"ðŸ” [extract_authoritative_identifiers] Input source keys: {list(source.keys())}")
    logger.info(f"ðŸ” [extract_authoritative_identifiers] Input source structure: {source}")
    
    # Check for nested identifiers structure (common issue after get_effective_values)
    if "identifiers" in source:
        logger.info(f"ðŸ” [extract_authoritative_identifiers] Found nested identifiers: {source['identifiers']}")
        if isinstance(source["identifiers"], dict):
            nested_doi = source["identifiers"].get("doi")
            nested_arxiv = source["identifiers"].get("arxiv_id") 
            if nested_doi or nested_arxiv:
                logger.warning(f"âš ï¸ [extract_authoritative_identifiers] DETECTED NESTED IDENTIFIERS! This suggests get_effective_values() didn't flatten properly. DOI: {nested_doi}, ArXiv: {nested_arxiv}")

    # ðŸŽ¯ ä¼˜å…ˆçº§1ï¼šç›´æŽ¥æä¾›çš„DOIï¼ˆæœ€å¯é ï¼‰
    if source.get("doi"):
        identifiers.doi = source["doi"]
        primary_type = "doi"
        logger.info(f"âœ… ä½¿ç”¨ç›´æŽ¥æä¾›çš„DOI: {identifiers.doi}")
        return identifiers, primary_type, url_validation_info

    # ðŸŽ¯ ä¼˜å…ˆçº§2ï¼šç›´æŽ¥æä¾›çš„ArXiv ID
    if source.get("arxiv_id"):
        identifiers.arxiv_id = source["arxiv_id"]
        primary_type = "arxiv"
        logger.info(f"âœ… ä½¿ç”¨ç›´æŽ¥æä¾›çš„ArXiv ID: {identifiers.arxiv_id}")
        return identifiers, primary_type, url_validation_info

    # ðŸŽ¯ ä¼˜å…ˆçº§3ï¼šURLæ˜ å°„æœåŠ¡ï¼ˆå¦‚æžœæ²¡æœ‰ç›´æŽ¥æ ‡è¯†ç¬¦ï¼‰
    if source.get("url"):
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå·²çŸ¥é—®é¢˜åŸŸåï¼Œè·³è¿‡URLéªŒè¯
            problematic_domains = [
                'semanticscholar.org',
                'scholar.google.com',
                'researchgate.net',
                # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šåŸŸå
            ]

            skip_validation = any(domain in source["url"].lower() for domain in problematic_domains)

            if skip_validation:
                logger.info(f"ðŸ”„ è·³è¿‡URLéªŒè¯ï¼ˆå·²çŸ¥é—®é¢˜åŸŸåï¼‰: {source['url']}")
                url_validation_info = {
                    "status": "skipped",
                    "reason": "known_problematic_domain",
                    "original_url": source["url"],
                    "validation_details": {
                        "skip_reason": "domain_has_anti_bot_protection",
                        "validation_time": datetime.now().isoformat(),
                    }
                }
            else:
                # å¯¹å…¶ä»–åŸŸåè¿›è¡ŒURLéªŒè¯
                from ..services.url_mapping.core.service import URLMappingService
                temp_service = URLMappingService(enable_url_validation=True)

                # ç›´æŽ¥ä½¿ç”¨URLéªŒè¯åŠŸèƒ½
                if not temp_service._validate_url(source["url"]):
                    # URLéªŒè¯å¤±è´¥ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯å¹¶æŠ›å‡ºå¼‚å¸¸
                    url_validation_info = {
                        "status": "failed",
                        "error": f"URL {source['url']} æ— æ³•è®¿é—®æˆ–ä¸å­˜åœ¨",
                        "original_url": source["url"],
                        "validation_details": {
                            "error_type": "url_not_accessible",
                            "validation_time": datetime.now().isoformat(),
                        }
                    }
                    logger.warning(f"URLéªŒè¯å¤±è´¥: {source['url']}")
                    # raise ValueError(f"URLéªŒè¯å¤±è´¥: {url_validation_info['error']}")
                    return identifiers, "unknown", url_validation_info
                else:
                    url_validation_info = {
                        "status": "success",
                        "original_url": source["url"],
                        "validation_details": {
                            "validation_time": datetime.now().isoformat(),
                        }
                    }

            # ä½¿ç”¨æ–°ç‰ˆæœ¬çš„URLæ˜ å°„æœåŠ¡ï¼ˆæ”¯æŒPDFé‡å®šå‘ï¼‰
            from ..services.url_mapping import get_url_mapping_service
            url_service = get_url_mapping_service(enable_url_validation=False)  # éªŒè¯å·²å¤„ç†æˆ–è·³è¿‡
            mapping_result = url_service.map_url_sync(source["url"])

            # å¤„ç†æ˜ å°„ç»“æžœ
            if mapping_result.doi:
                identifiers.doi = mapping_result.doi
                primary_type = "doi"
                logger.info(f"âœ… URLæ˜ å°„æå–åˆ°DOI: {identifiers.doi}")
            elif mapping_result.arxiv_id:
                identifiers.arxiv_id = mapping_result.arxiv_id
                primary_type = "arxiv"
                logger.info(f"âœ… URLæ˜ å°„æå–åˆ°ArXiv ID: {identifiers.arxiv_id}")

            # ðŸ†• ä¿å­˜URLæ˜ å°„ç»“æžœä¾›å…ƒæ•°æ®èŽ·å–å™¨ä½¿ç”¨ï¼ˆåŒ…æ‹¬æ ‡é¢˜ç­‰ä¿¡æ¯ï¼‰
            if mapping_result.is_successful():
                # å°†URLæ˜ å°„ç»“æžœæ·»åŠ åˆ°url_validation_infoä¸­ï¼Œä¾›åŽç»­ä½¿ç”¨
                if not url_validation_info:
                    url_validation_info = {}
                
                url_validation_info["url_mapping_result"] = {
                    "title": mapping_result.title,
                    "year": mapping_result.year,
                    "venue": mapping_result.venue,
                    "authors": mapping_result.authors,  # ðŸ†• æ·»åŠ é—æ¼çš„ä½œè€…å­—æ®µï¼
                    "source_page_url": mapping_result.source_page_url,
                    "pdf_url": mapping_result.pdf_url,
                    "source_adapter": mapping_result.source_adapter,
                    "strategy_used": mapping_result.strategy_used,
                    "confidence": mapping_result.confidence
                }
                logger.info(f"âœ… URLæ˜ å°„æå–åˆ°æœ‰ç”¨ä¿¡æ¯: title={bool(mapping_result.title)}, venue={mapping_result.venue}, authors={len(mapping_result.authors) if mapping_result.authors else 0}")

            # å¦‚æžœURLæ˜ å°„æœåŠ¡æ‰¾åˆ°äº†æ ‡è¯†ç¬¦ï¼Œç›´æŽ¥è¿”å›ž
            if identifiers.doi or identifiers.arxiv_id:
                return identifiers, primary_type or "unknown", url_validation_info

        except Exception as e:
            # å¦‚æžœæ˜¯URLéªŒè¯å¤±è´¥ï¼Œç›´æŽ¥æŠ›å‡º
            if "URLéªŒè¯å¤±è´¥" in str(e):
                # This error is now handled by returning the validation info dict
                pass

            # å…¶ä»–å¼‚å¸¸ï¼Œå›žé€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            logger.warning(f"URLæ˜ å°„æœåŠ¡å¤±è´¥ï¼Œå›žé€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {e}")
            # è®°å½•URLæ˜ å°„æœåŠ¡å¤±è´¥ä¿¡æ¯
            if source.get("url"):
                url_validation_info = {
                    "status": "skipped",
                    "error": f"URLæ˜ å°„æœåŠ¡å¼‚å¸¸: {str(e)}",
                    "original_url": source["url"],
                    "validation_details": {
                        "error_type": "service_error",
                        "validation_time": datetime.now().isoformat(),
                    }
                }

    # ðŸŽ¯ ä¼˜å…ˆçº§4ï¼šä¼ ç»Ÿæ–¹æ³•ä½œä¸ºå¤‡ç”¨ï¼ˆä¿æŒå‘åŽå…¼å®¹ï¼‰
    logger.info("ðŸ”„ ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•æå–æ ‡è¯†ç¬¦")

    # ä»ŽURLä¸­æå–DOI
    doi_pattern = r"10\.\d{4,}/[^\s]+"
    if source.get("url") and "doi.org" in source["url"]:
        if match := re.search(doi_pattern, source["url"]):
            identifiers.doi = match.group()
            primary_type = "doi"
            logger.info(f"âœ… ä¼ ç»Ÿæ–¹æ³•ä»ŽURLæå–åˆ°DOI: {identifiers.doi}")

    # ä»ŽURLä¸­æå–ArXiv ID
    if not identifiers.doi and source.get("url") and "arxiv.org" in source["url"]:
        if match := re.search(r"arxiv\.org/(?:abs|pdf)/([^/?]+)", source["url"]):
            identifiers.arxiv_id = match.group(1).replace(".pdf", "")
            primary_type = "arxiv"
            logger.info(f"âœ… ä¼ ç»Ÿæ–¹æ³•ä»ŽURLæå–åˆ°ArXiv ID: {identifiers.arxiv_id}")

    # ðŸ” DEBUG: Log final extraction results
    final_doi = identifiers.doi
    final_arxiv = identifiers.arxiv_id
    final_fingerprint = identifiers.fingerprint
    
    logger.info(f"ðŸ” [extract_authoritative_identifiers] FINAL RESULTS:")
    logger.info(f"  DOI: {final_doi}")
    logger.info(f"  ArXiv ID: {final_arxiv}")
    logger.info(f"  Fingerprint: {final_fingerprint}")
    logger.info(f"  Primary type: {primary_type or 'unknown'}")
    logger.info(f"  URL validation: {url_validation_info.get('status') if url_validation_info else 'not_performed'}")
    
    if not final_doi and not final_arxiv and not final_fingerprint:
        logger.error(f"âŒ [extract_authoritative_identifiers] NO IDENTIFIERS EXTRACTED! This will cause downstream issues.")
        logger.error(f"   Source had keys: {list(source.keys())}")
        logger.error(f"   Source: {source}")

    return identifiers, primary_type or "unknown", url_validation_info


def convert_grobid_to_metadata(grobid_data: Dict[str, Any]) -> MetadataModel:
    """Convert GROBID output to MetadataModel."""
    header = grobid_data.get("TEI", {}).get("teiHeader", {}).get("fileDesc", {})
    title_stmt = header.get("titleStmt", {})

    title = title_stmt.get("title", {}).get("#text")

    authors = []
    author_list = (
        header.get("sourceDesc", {})
        .get("biblStruct", {})
        .get("analytic", {})
        .get("author", [])
    )
    if isinstance(author_list, dict):  # handle case where there is only one author
        author_list = [author_list]

    for author_data in author_list:
        pers_name = author_data.get("persName", {})
        forenames = [
            fn.get("#text")
            for fn in pers_name.get("forename", [])
            if isinstance(fn, dict)
        ]
        surname = pers_name.get("surname", {}).get("#text")
        full_name = " ".join(forenames) + (f" {surname}" if surname else "")
        authors.append(AuthorModel(name=full_name.strip()))

    return MetadataModel(
        title=title or "Unknown Title",
        authors=authors,
        year=None,
        journal=None,
        abstract=header.get("profileDesc", {}).get("abstract", {}).get("#text"),
        source_priority=["grobid"],
    )
