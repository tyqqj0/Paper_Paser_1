#!/usr/bin/env python3
"""
Workerå¹¶è¡Œå¤„ç†èƒ½åŠ›æµ‹è¯•è„šæœ¬

ä¸“é—¨æµ‹è¯•Celery Workerçš„çœŸå®å¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼Œè€Œä¸æ˜¯APIæäº¤èƒ½åŠ›ã€‚
é€šè¿‡ç›‘æ§ä»»åŠ¡çš„å®é™…æ‰§è¡ŒçŠ¶æ€æ¥è¯„ä¼°Workerçš„å¹¶å‘å¤„ç†æ€§èƒ½ã€‚

ä½¿ç”¨æ–¹æ³•:
    python test_worker_concurrency.py --count 16 --timeout 300    # æäº¤16ä¸ªä»»åŠ¡ï¼Œç›‘æ§5åˆ†é’Ÿ
    python test_worker_concurrency.py --count 8 --quick           # å¿«é€Ÿæµ‹è¯•ï¼Œä½¿ç”¨ç®€å•ä»»åŠ¡
"""

import argparse
import asyncio
import json
import sys
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

import aiohttp
from loguru import logger


class WorkerConcurrencyTester:
    """Workerå¹¶å‘æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str = "http://api:8000"):
        self.base_url = base_url
        self.session = None
        self.submitted_tasks = []
        self.monitoring_data = []
    
    async def initialize(self):
        """åˆå§‹åŒ–HTTPä¼šè¯"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=300)
        )
        logger.info("âœ… HTTPä¼šè¯åˆå§‹åŒ–æˆåŠŸ")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.session:
            await self.session.close()
    
    async def submit_task(self, task_data: Dict[str, Any]) -> Optional[str]:
        """æäº¤å•ä¸ªä»»åŠ¡å¹¶è¿”å›task_id"""
        url = f"{self.base_url}/api/literature/"
        
        try:
            async with self.session.post(url, json=task_data) as response:
                if response.status in [200, 201, 202]:
                    response_data = await response.json()
                    task_id = response_data.get("task_id")
                    if task_id:
                        logger.info(f"âœ… ä»»åŠ¡æäº¤æˆåŠŸ: {task_id}")
                        return task_id
                    else:
                        logger.error(f"âŒ å“åº”ä¸­æ²¡æœ‰task_id: {response_data}")
                        return None
                else:
                    error_text = await response.text()
                    logger.error(f"âŒ ä»»åŠ¡æäº¤å¤±è´¥ {response.status}: {error_text}")
                    return None
                    
        except Exception as e:
            logger.error(f"âŒ æäº¤ä»»åŠ¡æ—¶å‡ºé”™: {e}")
            return None
    
    async def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        url = f"{self.base_url}/api/task/{task_id}"
        
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    return {"error": f"HTTP {response.status}"}
        except Exception as e:
            return {"error": str(e)}
    
    async def submit_batch_tasks(self, count: int, quick_mode: bool = False) -> List[str]:
        """æ‰¹é‡æäº¤ä»»åŠ¡"""
        logger.info(f"ğŸš€ å¼€å§‹æäº¤ {count} ä¸ªä»»åŠ¡...")
        
        tasks = []
        task_ids = []
        
        for i in range(count):
            if quick_mode:
                # å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨ä¼šå¿«é€Ÿå¤±è´¥çš„URLï¼Œå‡å°‘å¤„ç†æ—¶é—´
                task_data = {
                    "url": f"https://httpbin.org/status/404?task={i}",  # ä¼šå¿«é€Ÿè¿”å›404
                    "title": f"Quick Test Task {i}",
                    "authors": [f"Test Author {i}"]
                }
            else:
                # æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨çœŸå®ä½†ç®€å•çš„ä»»åŠ¡
                task_data = {
                    "url": f"https://example.com/test_paper_{i}",
                    "title": f"Worker Concurrency Test Paper {i}",
                    "authors": [f"Test Author {i}"]
                }
            
            task = asyncio.create_task(
                self.submit_task(task_data),
                name=f"submit_task_{i}"
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡æäº¤å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            if isinstance(result, str):  # task_id
                task_ids.append(result)
                self.submitted_tasks.append({
                    "task_id": result,
                    "submit_time": datetime.now(),
                    "task_index": i
                })
            elif isinstance(result, Exception):
                logger.error(f"ä»»åŠ¡ {i} æäº¤å¼‚å¸¸: {result}")
            else:
                logger.warning(f"ä»»åŠ¡ {i} æäº¤å¤±è´¥: {result}")
        
        logger.info(f"ğŸ“Š æˆåŠŸæäº¤ {len(task_ids)} / {count} ä¸ªä»»åŠ¡")
        return task_ids
    
    async def monitor_tasks(self, task_ids: List[str], timeout_minutes: int = 10) -> Dict[str, Any]:
        """ç›‘æ§ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€"""
        logger.info(f"ğŸ‘€ å¼€å§‹ç›‘æ§ {len(task_ids)} ä¸ªä»»åŠ¡ï¼Œè¶…æ—¶ {timeout_minutes} åˆ†é’Ÿ")
        
        start_time = datetime.now()
        timeout_time = start_time + timedelta(minutes=timeout_minutes)
        
        task_status_history = {task_id: [] for task_id in task_ids}
        active_tasks = set(task_ids)
        completed_tasks = set()
        failed_tasks = set()
        
        max_concurrent_processing = 0
        processing_count_history = []
        
        while active_tasks and datetime.now() < timeout_time:
            current_time = datetime.now()
            processing_count = 0
            pending_count = 0
            
            # æ£€æŸ¥æ‰€æœ‰æ´»è·ƒä»»åŠ¡çš„çŠ¶æ€
            status_tasks = []
            for task_id in list(active_tasks):
                status_tasks.append(self.get_task_status(task_id))
            
            statuses = await asyncio.gather(*status_tasks, return_exceptions=True)
            
            for task_id, status in zip(list(active_tasks), statuses):
                if isinstance(status, Exception):
                    logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥ {task_id}: {status}")
                    continue
                
                if "error" in status:
                    logger.warning(f"ä»»åŠ¡çŠ¶æ€é”™è¯¯ {task_id}: {status['error']}")
                    continue
                
                # è®°å½•çŠ¶æ€å†å²
                status_info = {
                    "timestamp": current_time.isoformat(),
                    "status": status.get("status", "unknown"),
                    "execution_status": status.get("execution_status", "unknown")
                }
                task_status_history[task_id].append(status_info)
                
                # ç»Ÿè®¡å½“å‰çŠ¶æ€
                execution_status = status.get("execution_status", "unknown")
                task_status = status.get("status", "unknown")
                
                if execution_status in ["processing", "in_progress"] or task_status in ["processing", "in_progress"]:
                    processing_count += 1
                elif execution_status == "pending" or task_status == "pending":
                    pending_count += 1
                elif execution_status in ["completed", "success"] or task_status in ["completed", "success"]:
                    completed_tasks.add(task_id)
                    active_tasks.remove(task_id)
                    logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {task_id}")
                elif execution_status in ["failed", "error"] or task_status in ["failed", "error"]:
                    failed_tasks.add(task_id)
                    active_tasks.remove(task_id)
                    logger.info(f"âŒ ä»»åŠ¡å¤±è´¥: {task_id}")
            
            # æ›´æ–°æœ€å¤§å¹¶å‘å¤„ç†æ•°
            max_concurrent_processing = max(max_concurrent_processing, processing_count)
            
            # è®°å½•å¤„ç†æ•°é‡å†å²
            processing_count_history.append({
                "timestamp": current_time.isoformat(),
                "processing": processing_count,
                "pending": pending_count,
                "completed": len(completed_tasks),
                "failed": len(failed_tasks),
                "active": len(active_tasks)
            })
            
            # æ˜¾ç¤ºå½“å‰çŠ¶æ€
            elapsed = (current_time - start_time).total_seconds()
            logger.info(f"â±ï¸  {elapsed:.0f}s | å¤„ç†ä¸­: {processing_count} | ç­‰å¾…ä¸­: {pending_count} | å·²å®Œæˆ: {len(completed_tasks)} | å·²å¤±è´¥: {len(failed_tasks)} | å‰©ä½™: {len(active_tasks)}")
            
            # å¦‚æœæ²¡æœ‰ä»»åŠ¡åœ¨å¤„ç†ä¸”é˜Ÿåˆ—ä¸ºç©ºï¼Œå¯èƒ½éœ€è¦ç­‰å¾…
            if processing_count == 0 and pending_count == 0 and active_tasks:
                logger.warning("âš ï¸  æ²¡æœ‰ä»»åŠ¡åœ¨å¤„ç†ï¼Œå¯èƒ½æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆæˆ–å¤±è´¥")
                break
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥
            await asyncio.sleep(5)
        
        end_time = datetime.now()
        total_time = (end_time - start_time).total_seconds()
        
        # æ±‡æ€»ç»“æœ
        result = {
            "test_summary": {
                "total_tasks": len(task_ids),
                "completed_tasks": len(completed_tasks),
                "failed_tasks": len(failed_tasks),
                "timeout_tasks": len(active_tasks),
                "success_rate": len(completed_tasks) / len(task_ids) * 100,
                "total_time_seconds": total_time,
                "max_concurrent_processing": max_concurrent_processing
            },
            "task_status_history": task_status_history,
            "processing_count_history": processing_count_history,
            "completed_task_ids": list(completed_tasks),
            "failed_task_ids": list(failed_tasks),
            "timeout_task_ids": list(active_tasks)
        }
        
        return result
    
    def analyze_results(self, results: Dict[str, Any]):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        summary = results["test_summary"]
        
        print("\n" + "="*80)
        print("ğŸ¯ Workerå¹¶å‘å¤„ç†èƒ½åŠ›æµ‹è¯•ç»“æœ")
        print("="*80)
        
        print(f"ğŸ“Š ä»»åŠ¡ç»Ÿè®¡:")
        print(f"   æ€»ä»»åŠ¡æ•°: {summary['total_tasks']}")
        print(f"   å·²å®Œæˆ: {summary['completed_tasks']}")
        print(f"   å·²å¤±è´¥: {summary['failed_tasks']}")
        print(f"   è¶…æ—¶æœªå®Œæˆ: {summary['timeout_tasks']}")
        print(f"   æˆåŠŸç‡: {summary['success_rate']:.2f}%")
        
        print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
        print(f"   æœ€å¤§å¹¶å‘å¤„ç†æ•°: {summary['max_concurrent_processing']}")
        print(f"   æ€»æµ‹è¯•æ—¶é—´: {summary['total_time_seconds']:.0f} ç§’")
        
        if summary['completed_tasks'] > 0:
            avg_time_per_task = summary['total_time_seconds'] / summary['completed_tasks']
            print(f"   å¹³å‡æ¯ä»»åŠ¡æ—¶é—´: {avg_time_per_task:.1f} ç§’")
            
            throughput = summary['completed_tasks'] / (summary['total_time_seconds'] / 60)
            print(f"   å¤„ç†ååé‡: {throughput:.2f} ä»»åŠ¡/åˆ†é’Ÿ")
        
        # åˆ†æå¹¶å‘å¤„ç†å†å²
        processing_history = results["processing_count_history"]
        if processing_history:
            processing_counts = [h["processing"] for h in processing_history]
            avg_concurrent = sum(processing_counts) / len(processing_counts)
            print(f"   å¹³å‡å¹¶å‘å¤„ç†æ•°: {avg_concurrent:.2f}")
        
        print("="*80)
        
        # æ˜¾ç¤ºå¤±è´¥ä»»åŠ¡è¯¦æƒ…
        if summary['failed_tasks'] > 0:
            print(f"\nâŒ å¤±è´¥ä»»åŠ¡è¯¦æƒ…:")
            for task_id in results['failed_task_ids'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   - {task_id}")
        
        # æ˜¾ç¤ºè¶…æ—¶ä»»åŠ¡è¯¦æƒ…
        if summary['timeout_tasks'] > 0:
            print(f"\nâ° è¶…æ—¶ä»»åŠ¡è¯¦æƒ…:")
            for task_id in results['timeout_task_ids'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   - {task_id}")
    
    def export_results(self, results: Dict[str, Any], filename: Optional[str] = None):
        """å¯¼å‡ºæµ‹è¯•ç»“æœ"""
        if filename is None:
            filename = f"worker_concurrency_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“ æµ‹è¯•ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºç»“æœå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Workerå¹¶å‘å¤„ç†èƒ½åŠ›æµ‹è¯•è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
    python test_worker_concurrency.py --count 8 --timeout 5     # 8ä¸ªä»»åŠ¡ï¼Œç›‘æ§5åˆ†é’Ÿ
    python test_worker_concurrency.py --count 16 --quick        # 16ä¸ªä»»åŠ¡ï¼Œå¿«é€Ÿæ¨¡å¼
    python test_worker_concurrency.py --count 12 --timeout 10   # 12ä¸ªä»»åŠ¡ï¼Œç›‘æ§10åˆ†é’Ÿ
        """
    )
    
    parser.add_argument(
        "--count", 
        type=int, 
        default=8,
        help="æäº¤çš„ä»»åŠ¡æ•°é‡ (é»˜è®¤: 8)"
    )
    
    parser.add_argument(
        "--timeout", 
        type=int, 
        default=10,
        help="ç›‘æ§è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰ (é»˜è®¤: 10)"
    )
    
    parser.add_argument(
        "--quick", 
        action="store_true", 
        help="å¿«é€Ÿæ¨¡å¼ï¼Œä½¿ç”¨ä¼šå¿«é€Ÿå¤±è´¥çš„ä»»åŠ¡"
    )
    
    parser.add_argument(
        "--url", 
        default="http://api:8000",
        help="APIåŸºç¡€URL (é»˜è®¤: http://api:8000)"
    )
    
    parser.add_argument(
        "--export", 
        action="store_true", 
        help="å¯¼å‡ºæµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"
    )
    
    args = parser.parse_args()
    
    tester = WorkerConcurrencyTester(args.url)
    
    try:
        await tester.initialize()
        
        logger.info(f"ğŸš€ å¼€å§‹Workerå¹¶å‘æµ‹è¯•")
        logger.info(f"ğŸ“‹ ä»»åŠ¡æ•°é‡: {args.count}")
        logger.info(f"â° ç›‘æ§æ—¶é—´: {args.timeout} åˆ†é’Ÿ")
        logger.info(f"ğŸƒ å¿«é€Ÿæ¨¡å¼: {'æ˜¯' if args.quick else 'å¦'}")
        
        # æäº¤ä»»åŠ¡
        task_ids = await tester.submit_batch_tasks(args.count, args.quick)
        
        if not task_ids:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸæäº¤ä»»ä½•ä»»åŠ¡")
            return
        
        # ç›‘æ§ä»»åŠ¡æ‰§è¡Œ
        results = await tester.monitor_tasks(task_ids, args.timeout)
        
        # åˆ†æç»“æœ
        tester.analyze_results(results)
        
        # å¯¼å‡ºç»“æœ
        if args.export:
            tester.export_results(results)
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
    
    finally:
        await tester.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
