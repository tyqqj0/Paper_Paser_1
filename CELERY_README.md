# Celery å¼‚æ­¥ä»»åŠ¡ç³»ç»Ÿé›†æˆ

æœ¬æ–‡æ¡£è¯´æ˜äº†æ–‡çŒ®è§£æç³»ç»Ÿä¸­ Celery å¼‚æ­¥ä»»åŠ¡å¤„ç†çš„è®¾è®¡å’Œä½¿ç”¨æ–¹æ³•ã€‚

## ğŸ—ï¸ æ¶æ„æ¦‚è¿°

æˆ‘ä»¬çš„ Celery é›†æˆé‡‡ç”¨äº†ç°ä»£ Python å¼‚æ­¥ç¼–ç¨‹æ¨¡å¼ï¼Œå°†è€—æ—¶çš„æ–‡çŒ®å¤„ç†ä»»åŠ¡ä» Web API ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œç¡®ä¿ç³»ç»Ÿçš„å“åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚

### æ ¸å¿ƒç»„ä»¶

```
literature_parser_backend/worker/
â”œâ”€â”€ __init__.py          # Worker åŒ…å¯¼å‡º
â”œâ”€â”€ celery_app.py       # Celery åº”ç”¨é…ç½®
â”œâ”€â”€ tasks.py            # æ ¸å¿ƒæ–‡çŒ®å¤„ç†ä»»åŠ¡
â”œâ”€â”€ worker.py           # Worker å¯åŠ¨è„šæœ¬
â””â”€â”€ test_tasks.py       # ä»»åŠ¡æµ‹è¯•å¥—ä»¶
```

## âš™ï¸ æŠ€æœ¯å®ç°

### 1. Celery åº”ç”¨é…ç½® (`celery_app.py`)

**æ ¸å¿ƒç‰¹æ€§**:
- **Redis ä½œä¸º Broker**: é«˜æ€§èƒ½æ¶ˆæ¯é˜Ÿåˆ—
- **ä»»åŠ¡è·¯ç”±**: ä¸“ç”¨çš„ `literature` é˜Ÿåˆ—
- **ç»“æœæŒä¹…åŒ–**: ä»»åŠ¡ç»“æœä¿å­˜ 1 å°æ—¶
- **è¶…æ—¶æ§åˆ¶**: ç¡¬è¶…æ—¶ 30 åˆ†é’Ÿï¼Œè½¯è¶…æ—¶ 25 åˆ†é’Ÿ

**é…ç½®è¦ç‚¹**:
```python
# ä» settings.py è¯»å–é…ç½®
celery_app = Celery(
    "literature_parser_worker",
    broker=settings.celery_broker_url_computed,
    backend=settings.celery_result_backend_computed,
)

# ä¸“ä¸šåŒ–é…ç½®
task_routes = {"literature_parser_worker.tasks.*": {"queue": "literature"}}
worker_prefetch_multiplier = 1  # ä¸€æ¬¡åªå¤„ç†ä¸€ä¸ªä»»åŠ¡
```

### 2. æ™ºèƒ½æ··åˆæ–‡çŒ®å¤„ç†ä»»åŠ¡ (`tasks.py`)

#### ğŸ§  æ ¸å¿ƒç®—æ³•: `process_literature_task`

è¿™æ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒå¼•æ“ï¼Œå®ç°äº†å®Œæ•´çš„æ™ºèƒ½æ··åˆå·¥ä½œæµï¼š

**è¾“å…¥**: æ–‡çŒ®æºä¿¡æ¯å­—å…¸
```python
{
    "url": "https://doi.org/10.1038/nature12373",
    "title": "Optional fallback title",
    "authors": "Optional author info",
    "doi": "Direct DOI if available"
}
```

**è¾“å‡º**: MongoDB ä¸­æ–°åˆ›å»ºçš„æ–‡çŒ® ID (å­—ç¬¦ä¸²)

#### ğŸ”„ å¤„ç†æµç¨‹è¯¦è§£

##### 1. **æƒå¨æ ‡è¯†ç¬¦æå–** (`extract_authoritative_identifiers`)
- **ä¼˜å…ˆçº§**: DOI > ArXiv ID > å†…å®¹æŒ‡çº¹
- **æ™ºèƒ½è§£æ**: ä» URL ä¸­æå– DOI å’Œ ArXiv ID
- **æŒ‡çº¹ç”Ÿæˆ**: ä½¿ç”¨æ ‡é¢˜+ä½œè€…+å¹´ä»½çš„ MD5 å“ˆå¸Œ

```python
# DOI æå–ç¤ºä¾‹
url = "https://doi.org/10.1038/nature12373"
# æå–ç»“æœ: DOI = "10.1038/nature12373"

# æŒ‡çº¹ç”Ÿæˆç¤ºä¾‹  
title = "Attention Is All You Need"
authors = "Vaswani et al."
year = "2017"
# ç”ŸæˆæŒ‡çº¹: d7729da1a7b25d6f
```

##### 2. **å…ƒæ•°æ®è·å–ç€‘å¸ƒæµ** (`fetch_metadata_waterfall`)
- **ä¸»è·¯å¾„**: CrossRef API (æƒå¨æ€§æœ€é«˜)
- **å¤‡ç”¨è·¯å¾„**: Semantic Scholar API (AI å¢å¼º)
- **æœ€åå¤‡ç”¨**: GROBID PDF è§£æ

##### 3. **å‚è€ƒæ–‡çŒ®è·å–ç€‘å¸ƒæµ** (`fetch_references_waterfall`)
- **ä¸»è·¯å¾„**: Semantic Scholar API (ç»“æ„åŒ–æœ€å¥½)
- **å¤‡ç”¨è·¯å¾„**: GROBID PDF è§£æ

##### 4. **æ•°æ®æ•´åˆä¸æŒä¹…åŒ–**
- **MongoDB å­˜å‚¨**: ä½¿ç”¨ Motor å¼‚æ­¥é©±åŠ¨
- **äº‹åŠ¡å®‰å…¨**: é”™è¯¯æ—¶æä¾›å›é€€æœºåˆ¶
- **ä»»åŠ¡çŠ¶æ€**: å®æ—¶æ›´æ–°å¤„ç†è¿›åº¦

#### ğŸ“Š ä»»åŠ¡çŠ¶æ€ç®¡ç†

ä»»åŠ¡æ”¯æŒç»†ç²’åº¦çš„çŠ¶æ€è·Ÿè¸ªï¼š

```python
meta = {
    "stage": "æ­£åœ¨è·å–å…ƒæ•°æ®",
    "progress": 30,
    "details": "ä½¿ç”¨doiæ ‡è¯†ç¬¦",
    "timestamp": "2024-01-15T10:30:00Z"
}
```

**çŠ¶æ€é˜¶æ®µ**:
- æ­£åœ¨åˆå§‹åŒ–ä»»åŠ¡ (5%)
- æ­£åœ¨æå–æƒå¨æ ‡è¯†ç¬¦ (10%)
- æ­£åœ¨è·å–å…ƒæ•°æ® (30%)
- æ­£åœ¨è§£æPDFå…ƒæ•°æ® (40%) - ä»…åœ¨éœ€è¦æ—¶
- æ­£åœ¨è·å–å‚è€ƒæ–‡çŒ® (60%)
- æ­£åœ¨è§£æPDFå‚è€ƒæ–‡çŒ® (70%) - ä»…åœ¨éœ€è¦æ—¶
- æ­£åœ¨æ•´åˆæ•°æ® (80%)
- æ­£åœ¨ä¿å­˜åˆ°æ•°æ®åº“ (90%)
- ä»»åŠ¡å®Œæˆ (100%)

### 3. å¼‚æ­¥/åŒæ­¥é€‚é…

ç”±äº Celery ä»»åŠ¡å¿…é¡»æ˜¯åŒæ­¥çš„ï¼Œä½†æˆ‘ä»¬çš„å¤–éƒ¨ API å®¢æˆ·ç«¯æ˜¯å¼‚æ­¥çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå·§å¦™çš„é€‚é…æ¨¡å¼ï¼š

```python
# å¼‚æ­¥æ ¸å¿ƒé€»è¾‘
async def _process_literature_async(task_id: str, source: Dict[str, Any]) -> str:
    # æ‰€æœ‰å¼‚æ­¥æ“ä½œåœ¨è¿™é‡Œ

# Celery ä»»åŠ¡åŒ…è£…å™¨
@celery_app.task(bind=True)
def process_literature_task(self, source: Dict[str, Any]) -> str:
    return asyncio.run(_process_literature_async(self.request.id, source))
```

## ğŸ—„ï¸ æ•°æ®åº“é›†æˆ

### MongoDB è¿æ¥ç®¡ç†

**å¼‚æ­¥è¿æ¥**: ä½¿ç”¨ Motor è¿›è¡Œå¼‚æ­¥ MongoDB æ“ä½œ
```python
# æ•°æ®åº“è¿æ¥
await connect_to_mongodb(settings)

# æ•°æ®å­˜å‚¨
literature_dao = LiteratureDAO()
literature_id = await literature_dao.create_literature(literature)
```

**ç´¢å¼•ä¼˜åŒ–**: è‡ªåŠ¨åˆ›å»ºæ€§èƒ½ç´¢å¼•
- DOI å­—æ®µç´¢å¼• (å”¯ä¸€æŸ¥æ‰¾)
- ArXiv ID ç´¢å¼•
- å…¨æ–‡æœç´¢ç´¢å¼• (æ ‡é¢˜+ä½œè€…)
- åˆ›å»ºæ—¶é—´ç´¢å¼• (æ’åº)

### æ•°æ®æ¨¡å‹å…¼å®¹æ€§

ä»»åŠ¡å®Œå…¨å…¼å®¹æˆ‘ä»¬å®šä¹‰çš„ Pydantic æ¨¡å‹ï¼š
- `LiteratureModel`: å®Œæ•´çš„ MongoDB æ–‡æ¡£
- `IdentifiersModel`: æƒå¨æ ‡è¯†ç¬¦
- `MetadataModel`: æ–‡çŒ®å…ƒæ•°æ®
- `ContentModel`: å†…å®¹ä¿¡æ¯
- `ReferenceModel`: å‚è€ƒæ–‡çŒ®

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. ç¯å¢ƒè®¾ç½®

**Redis é…ç½®** (settings.py):
```python
# Redis è®¾ç½®
redis_host: str = "localhost"
redis_port: int = 6379
redis_db: int = 0

# Celery è®¾ç½®  
celery_task_time_limit: int = 30 * 60  # 30 åˆ†é’Ÿ
celery_worker_prefetch_multiplier: int = 1
```

**ç¯å¢ƒå˜é‡æ”¯æŒ**:
```bash
export LITERATURE_PARSER_BACKEND_REDIS_HOST="redis-server"
export LITERATURE_PARSER_BACKEND_REDIS_PORT="6379"
export LITERATURE_PARSER_BACKEND_CELERY_TASK_TIME_LIMIT="1800"
```

### 2. å¯åŠ¨ Worker

#### æ–¹æ³•ä¸€: ä½¿ç”¨ä¾¿æ·è„šæœ¬
```bash
python start_worker.py
```

#### æ–¹æ³•äºŒ: ä½¿ç”¨ Celery å‘½ä»¤
```bash
celery -A literature_parser_backend.worker.celery_app worker \
    --loglevel=info \
    --concurrency=1 \
    --queues=literature \
    --hostname=literature-worker@%h
```

#### æ–¹æ³•ä¸‰: ä½¿ç”¨å†…ç½®è„šæœ¬
```bash
python -m literature_parser_backend.worker.worker
```

### 3. æäº¤ä»»åŠ¡

```python
from literature_parser_backend.worker import process_literature_task

# åˆ›å»ºä»»åŠ¡
source_data = {
    "url": "https://doi.org/10.1038/nature12373",
    "title": "Sample Paper Title"
}

# å¼‚æ­¥æäº¤
task = process_literature_task.delay(source_data)
print(f"Task ID: {task.id}")

# æ£€æŸ¥çŠ¶æ€
result = task.get(timeout=1800)  # 30 åˆ†é’Ÿè¶…æ—¶
print(f"Literature ID: {result}")
```

### 4. ç›‘æ§ä»»åŠ¡çŠ¶æ€

```python
from celery.result import AsyncResult
from literature_parser_backend.worker.celery_app import celery_app

# é€šè¿‡ä»»åŠ¡ ID è·å–çŠ¶æ€
task_result = AsyncResult(task_id, app=celery_app)

if task_result.state == "PROCESSING":
    meta = task_result.info
    print(f"Stage: {meta['stage']}")
    print(f"Progress: {meta['progress']}%")
    print(f"Details: {meta['details']}")
elif task_result.state == "SUCCESS":
    literature_id = task_result.result
    print(f"Completed! Literature ID: {literature_id}")
elif task_result.state == "FAILURE":
    print(f"Failed: {task_result.info}")
```

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæµ‹è¯•å¥—ä»¶

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•
python -m literature_parser_backend.worker.test_tasks

# æµ‹è¯•è¦†ç›–
âœ“ æƒå¨æ ‡è¯†ç¬¦æå– (DOI, ArXiv, æŒ‡çº¹)
âœ“ è¾“å…¥æ•°æ®éªŒè¯ 
âœ“ Celery ä»»åŠ¡æ³¨å†Œ
âœ“ æ–‡çŒ®å¤„ç†æµæ°´çº¿ (æ¨¡æ‹Ÿ)
âœ“ æ•°æ®åº“ä¿å­˜ (æ¨¡æ‹Ÿ)
```

### æµ‹è¯•è¾“å‡ºç¤ºä¾‹

```
============================================================
RUNNING CELERY TASK TESTS
============================================================
Testing identifier extraction...
âœ“ Correct primary type: doi
âœ“ Correct DOI: 10.1038/nature12373
âœ“ Correct ArXiv ID: 1706.03762
âœ“ Generated fingerprint: d7729da1a7b25d6f

Testing task input validation...
âœ“ Valid source passed validation
âœ“ Invalid source correctly failed validation

Testing literature processing pipeline...
âœ“ Literature processing completed successfully
âœ“ Generated literature ID: test_literature_id_123
âœ“ Database save operation was called
============================================================
ALL TESTS COMPLETED
============================================================
```

## ğŸ”§ è°ƒè¯•å’Œæ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. Redis è¿æ¥å¤±è´¥
```bash
# æ£€æŸ¥ Redis æ˜¯å¦è¿è¡Œ
redis-cli ping
# åº”è¯¥è¿”å›: PONG

# å¯åŠ¨ Redis (å¦‚æœæœªè¿è¡Œ)
redis-server
```

#### 2. MongoDB è¿æ¥é—®é¢˜
```python
# åœ¨ä»»åŠ¡ä¸­ä¼šæœ‰è¯¦ç»†é”™è¯¯æ—¥å¿—
logger.error(f"Failed to save literature to database: {e}")
# ä¼šå›é€€åˆ°æ¨¡æ‹Ÿ ID: lit_{task_id}
```

#### 3. ä»»åŠ¡è¶…æ—¶
```python
# è°ƒæ•´è¶…æ—¶è®¾ç½® (settings.py)
celery_task_time_limit: int = 45 * 60  # 45 åˆ†é’Ÿ
celery_task_soft_time_limit: int = 40 * 60  # 40 åˆ†é’Ÿ
```

### æ—¥å¿—é…ç½®

Worker æä¾›è¯¦ç»†çš„æ—¥å¿—è¾“å‡ºï¼š
```
[2024-01-15 10:30:00: INFO/MainProcess] Task literature_parser_worker.tasks.process_literature_task[123] started
[2024-01-15 10:30:01: INFO/MainProcess] Task 123: æ­£åœ¨åˆå§‹åŒ–ä»»åŠ¡ - è§£æè¾“å…¥æ•°æ®
[2024-01-15 10:30:02: INFO/MainProcess] Processing literature from source: https://doi.org/10.1038/nature12373
[2024-01-15 10:30:03: INFO/MainProcess] Extracted identifiers: {'doi': '10.1038/nature12373'}, primary: doi
```

## ğŸ“ˆ æ€§èƒ½å’Œæ‰©å±•

### æ€§èƒ½ç‰¹æ€§

- **å¹¶å‘æ§åˆ¶**: é»˜è®¤å•è¿›ç¨‹ (`concurrency=1`) é¿å…å¤–éƒ¨ API é€Ÿç‡é™åˆ¶
- **å†…å­˜ä¼˜åŒ–**: åªé¢„å–ä¸€ä¸ªä»»åŠ¡ (`prefetch_multiplier=1`)
- **è¶…æ—¶ç®¡ç†**: è½¯ç¡¬è¶…æ—¶é¿å…ä»»åŠ¡å¡æ­»
- **é”™è¯¯é‡è¯•**: å¤±è´¥ä»»åŠ¡å¯é…ç½®é‡è¯•ç­–ç•¥

### æ‰©å±•ç­–ç•¥

#### æ°´å¹³æ‰©å±•
```bash
# å¤šä¸ª worker å®ä¾‹
python start_worker.py &  # Worker 1
python start_worker.py &  # Worker 2
python start_worker.py &  # Worker 3
```

#### å‚ç›´æ‰©å±•
```python
# å¢åŠ å¹¶å‘ (è°¨æ…ä½¿ç”¨ï¼Œæ³¨æ„ API é™åˆ¶)
celery_worker_concurrency = 2
```

#### é˜Ÿåˆ—åˆ†ç¦»
```python
# ä¸åŒç±»å‹ä»»åŠ¡ä½¿ç”¨ä¸åŒé˜Ÿåˆ—
task_routes = {
    "literature_parser_worker.tasks.process_literature_task": {"queue": "literature"},
    "literature_parser_worker.tasks.pdf_processing_task": {"queue": "pdf"},
    "literature_parser_worker.tasks.reference_extraction_task": {"queue": "references"},
}
```

## ğŸ”® æœªæ¥æ”¹è¿›

1. **å®é™…å¼‚æ­¥å¤–éƒ¨ API è°ƒç”¨**: å½“å‰ä¸ºæ¨¡æ‹Ÿï¼Œéœ€è¦é›†æˆçœŸå®çš„ API å®¢æˆ·ç«¯
2. **PDF ä¸‹è½½åŠŸèƒ½**: å®ç°ä» URL ä¸‹è½½ PDF æ–‡ä»¶çš„åŠŸèƒ½
3. **å¢é‡æ›´æ–°**: æ”¯æŒå¯¹å·²å­˜åœ¨æ–‡çŒ®çš„å¢é‡æ›´æ–°
4. **æ‰¹å¤„ç†æ”¯æŒ**: æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªæ–‡çŒ®
5. **æ™ºèƒ½å»é‡**: åœ¨ä»»åŠ¡çº§åˆ«è¿›è¡Œæ–‡çŒ®å»é‡æ£€æŸ¥
6. **ç›‘æ§é›†æˆ**: é›†æˆ Flower æˆ– Prometheus è¿›è¡Œä»»åŠ¡ç›‘æ§

---

**è¿™ä¸ª Celery é›†æˆä¸ºæ–‡çŒ®è§£æç³»ç»Ÿæä¾›äº†å¼ºå¤§ã€å¯é ã€å¯æ‰©å±•çš„å¼‚æ­¥ä»»åŠ¡å¤„ç†èƒ½åŠ›ï¼Œæ˜¯æ•´ä¸ªç³»ç»Ÿæ¶æ„çš„æ ¸å¿ƒå¼•æ“ï¼** ğŸš€ 