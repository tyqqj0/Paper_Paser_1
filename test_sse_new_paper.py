#!/usr/bin/env python3
"""
æµ‹è¯•SSEå¤„ç†å…¨æ–°æ–‡çŒ®çš„è¯¦ç»†çŠ¶æ€å˜åŒ–

ä½¿ç”¨ä¸€ä¸ªå…¨æ–°çš„ã€ç³»ç»Ÿä¸­ä¸å­˜åœ¨çš„æ–‡çŒ®æ¥è§¦å‘å®Œæ•´çš„å¤„ç†æµç¨‹
"""

import requests
import json
import time
import threading
from typing import Dict, Any


class NewPaperSSEAnalyzer:
    """æ–°æ–‡çŒ®SSEåˆ†æå™¨"""
    
    def __init__(self, url: str, data: Dict[str, Any]):
        self.url = url
        self.data = data
        self.events = []
        self.is_connected = False
        self.error = None
        
    def connect_and_analyze(self):
        """è¿æ¥SSEå¹¶åˆ†æ"""
        try:
            response = requests.post(
                self.url,
                json=self.data,
                headers={
                    'Accept': 'text/event-stream',
                    'Cache-Control': 'no-cache'
                },
                stream=True,
                timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            
            if response.status_code != 200:
                self.error = f"HTTP {response.status_code}: {response.text}"
                return
            
            self.is_connected = True
            print(f"âœ… SSEè¿æ¥å»ºç«‹æˆåŠŸ")
            
            for line in response.iter_lines(decode_unicode=True):
                if line:
                    self._process_line(line)
                    
        except Exception as e:
            self.error = str(e)
            print(f"âŒ SSEè¿æ¥é”™è¯¯: {e}")
    
    def _process_line(self, line: str):
        """å¤„ç†SSEè¡Œæ•°æ®"""
        if line.startswith('event:'):
            event_type = line[6:].strip()
            self.current_event_type = event_type
        elif line.startswith('data:'):
            data = line[5:].strip()
            try:
                event_data = json.loads(data)
                event = {
                    'type': getattr(self, 'current_event_type', 'message'),
                    'data': event_data,
                    'timestamp': time.time()
                }
                self.events.append(event)
                self._analyze_event(event)
            except json.JSONDecodeError:
                print(f"âš ï¸ æ— æ³•è§£æJSONæ•°æ®: {data}")
    
    def _analyze_event(self, event: Dict[str, Any]):
        """åˆ†æäº‹ä»¶"""
        event_type = event['type']
        data = event['data']
        timestamp = time.strftime('%H:%M:%S', time.localtime(event['timestamp']))
        
        if event_type == 'status':
            execution_status = data.get('execution_status')
            progress = data.get('overall_progress', 0)
            stage = data.get('current_stage', 'N/A')
            
            print(f"[{timestamp}] ğŸ“Š {execution_status} - {progress}% - {stage}")
            
            # åˆ†ææ–‡çŒ®çŠ¶æ€è¯¦æƒ…
            literature_status = data.get('literature_status')
            if literature_status:
                self._print_literature_details(literature_status)
                
        elif event_type == 'completed':
            literature_id = data.get('literature_id')
            print(f"[{timestamp}] ğŸ‰ ä»»åŠ¡å®Œæˆ! Literature ID: {literature_id}")
            
        elif event_type == 'failed':
            error = data.get('error', 'æœªçŸ¥é”™è¯¯')
            print(f"[{timestamp}] âŒ ä»»åŠ¡å¤±è´¥: {error}")
    
    def _print_literature_details(self, lit_status: Dict[str, Any]):
        """æ‰“å°æ–‡çŒ®çŠ¶æ€è¯¦æƒ…"""
        overall_status = lit_status.get('overall_status')
        overall_progress = lit_status.get('overall_progress', 0)
        
        print(f"    ğŸ“š æ–‡çŒ®æ•´ä½“: {overall_status} ({overall_progress}%)")
        
        # æ‰“å°å„ç»„ä»¶çŠ¶æ€
        component_status = lit_status.get('component_status', {})
        for comp_name, comp_detail in component_status.items():
            if isinstance(comp_detail, dict):
                status = comp_detail.get('status', 'unknown')
                stage = comp_detail.get('stage', 'N/A')
                progress = comp_detail.get('progress', 0)
                source = comp_detail.get('source')
                error_info = comp_detail.get('error_info')
                
                status_icon = {
                    'pending': 'â³',
                    'processing': 'ğŸ”„',
                    'success': 'âœ…',
                    'failed': 'âŒ'
                }.get(status, 'â“')
                
                print(f"      {status_icon} {comp_name}: {status} ({progress}%) - {stage}")
                if source and source != "æœªçŸ¥æ¥æº":
                    print(f"         æ¥æº: {source}")
                if error_info:
                    print(f"         é”™è¯¯: {error_info}")
    
    def generate_summary(self):
        """ç”Ÿæˆæ‘˜è¦"""
        print("\n" + "=" * 70)
        print("ğŸ“‹ å¤„ç†è¿‡ç¨‹æ‘˜è¦")
        print("=" * 70)
        
        print(f"ğŸ“Š æ€»äº‹ä»¶æ•°: {len(self.events)}")
        
        # ç»Ÿè®¡å„é˜¶æ®µ
        stages_seen = set()
        components_seen = set()
        error_count = 0
        
        for event in self.events:
            if event['type'] == 'status':
                data = event['data']
                stage = data.get('current_stage')
                if stage:
                    stages_seen.add(stage)
                
                lit_status = data.get('literature_status')
                if lit_status:
                    comp_status = lit_status.get('component_status', {})
                    for comp_name, comp_detail in comp_status.items():
                        if isinstance(comp_detail, dict):
                            components_seen.add(comp_name)
                            if comp_detail.get('error_info'):
                                error_count += 1
        
        print(f"ğŸ”„ å¤„ç†é˜¶æ®µ: {len(stages_seen)} ä¸ª")
        for stage in sorted(stages_seen):
            print(f"   - {stage}")
        
        print(f"ğŸ”§ æ¶‰åŠç»„ä»¶: {len(components_seen)} ä¸ª")
        for comp in sorted(components_seen):
            print(f"   - {comp}")
        
        print(f"ğŸ’¥ é”™è¯¯æ¬¡æ•°: {error_count}")
        
        return {
            'total_events': len(self.events),
            'stages_count': len(stages_seen),
            'components_count': len(components_seen),
            'error_count': error_count
        }


def test_new_paper_processing():
    """æµ‹è¯•æ–°æ–‡çŒ®çš„å®Œæ•´å¤„ç†æµç¨‹"""
    print("ğŸ§ª æµ‹è¯•æ–°æ–‡çŒ®çš„å®Œæ•´SSEå¤„ç†æµç¨‹")
    print("=" * 70)
    
    # ä½¿ç”¨ä¸€ä¸ªç›¸å¯¹è¾ƒæ–°çš„arXivè®ºæ–‡ï¼Œå¯èƒ½ç³»ç»Ÿä¸­è¿˜æ²¡æœ‰
    test_data = {
        "source": {
            "arxiv_id": "2501.12345",  # ä½¿ç”¨ä¸€ä¸ªä¸å­˜åœ¨çš„arXiv ID
            "title": "Test Paper for SSE Analysis",
            "authors": ["Test Author"]
        }
    }
    
    print(f"ğŸ“ æäº¤æµ‹è¯•æ•°æ®: {json.dumps(test_data, indent=2)}")
    
    analyzer = NewPaperSSEAnalyzer("http://localhost:8000/api/literature/stream", test_data)
    
    # åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œ
    analysis_thread = threading.Thread(target=analyzer.connect_and_analyze)
    analysis_thread.daemon = True
    analysis_thread.start()
    
    print("â³ ç­‰å¾…å¤„ç†å®Œæˆ...")
    analysis_thread.join(timeout=120)
    
    if analyzer.error:
        print(f"âŒ å¤„ç†å¤±è´¥: {analyzer.error}")
        return False
    
    # ç”Ÿæˆæ‘˜è¦
    summary = analyzer.generate_summary()
    return summary


def test_with_real_new_arxiv():
    """ä½¿ç”¨çœŸå®çš„æ–°arXivè®ºæ–‡æµ‹è¯•"""
    print("\nğŸ§ª æµ‹è¯•çœŸå®çš„æ–°arXivè®ºæ–‡")
    print("=" * 70)
    
    # ä½¿ç”¨ä¸€ä¸ªæœ€è¿‘çš„arXivè®ºæ–‡
    test_data = {
        "source": {
            "url": "https://arxiv.org/abs/2501.18585"  # ä¸€ä¸ª2025å¹´1æœˆçš„è®ºæ–‡
        }
    }
    
    print(f"ğŸ“ æäº¤æµ‹è¯•æ•°æ®: {json.dumps(test_data, indent=2)}")
    
    analyzer = NewPaperSSEAnalyzer("http://localhost:8000/api/literature/stream", test_data)
    
    analysis_thread = threading.Thread(target=analyzer.connect_and_analyze)
    analysis_thread.daemon = True
    analysis_thread.start()
    
    print("â³ ç­‰å¾…å¤„ç†å®Œæˆ...")
    analysis_thread.join(timeout=120)
    
    if analyzer.error:
        print(f"âŒ å¤„ç†å¤±è´¥: {analyzer.error}")
        return False
    
    summary = analyzer.generate_summary()
    return summary


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ–°æ–‡çŒ®SSEè¯¦ç»†æµ‹è¯•...")
    
    # æµ‹è¯•1: ä½¿ç”¨ä¸å­˜åœ¨çš„arXiv IDï¼ˆä¼šè§¦å‘é”™è¯¯å¤„ç†ï¼‰
    # summary1 = test_new_paper_processing()
    
    # æµ‹è¯•2: ä½¿ç”¨çœŸå®çš„æ–°arXivè®ºæ–‡
    summary2 = test_with_real_new_arxiv()
    
    print("\n" + "=" * 70)
    print("ğŸ¯ æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    
    if summary2:
        print("âœ… æ–°æ–‡çŒ®SSEå¤„ç†éªŒè¯:")
        print(f"   - äº‹ä»¶æ•°é‡: {summary2['total_events']}")
        print(f"   - å¤„ç†é˜¶æ®µ: {summary2['stages_count']}")
        print(f"   - æ¶‰åŠç»„ä»¶: {summary2['components_count']}")
        print(f"   - é”™è¯¯æ¬¡æ•°: {summary2['error_count']}")
        
        if summary2['stages_count'] > 2:
            print("âœ… åŒ…å«è¯¦ç»†çš„å¤„ç†é˜¶æ®µ")
        else:
            print("âš ï¸ å¤„ç†é˜¶æ®µè¾ƒå°‘ï¼Œå¯èƒ½æ˜¯é‡å¤æ–‡çŒ®")
            
        if summary2['components_count'] >= 3:
            print("âœ… åŒ…å«æ‰€æœ‰ä¸»è¦ç»„ä»¶çŠ¶æ€")
        else:
            print("âš ï¸ ç»„ä»¶çŠ¶æ€ä¿¡æ¯ä¸å®Œæ•´")
    else:
        print("âŒ æ–°æ–‡çŒ®SSEæµ‹è¯•å¤±è´¥")


if __name__ == "__main__":
    main()
