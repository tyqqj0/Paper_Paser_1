<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gemini: A Family of Highly Capable Multimodal Models</title>
				<funder ref="#_ZJgtmhR">
					<orgName type="full">Eval Orhan Firat</orgName>
				</funder>
				<funder ref="#_KFDH2Z8 #_qxcEUGu">
					<orgName type="full">Lead</orgName>
				</funder>
				<funder ref="#_hXWegA5 #_VRt9nRS #_dmYjavg #_RJ7nW3Y #_eqm4ZrF #_nKe8eME #_qfWPdAX #_wB2DbXr #_27Rr4yG #_RbF3fEv #_vzSJ7yC #_YkxjEKj #_YTATdaM #_35Yw2p3 #_66EDNJ3 #_FSN3R5f">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_qUHx3MX">
					<orgName type="full">Codebase &amp; Parallelism Ryan Doherty, Lead</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-05-09">9 May 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
						</author>
						<author>
							<persName><surname>Google</surname></persName>
						</author>
						<title level="a" type="main">Gemini: A Family of Highly Capable Multimodal Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-05-09">9 May 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">4CA30E1DC5B35A2DDF13875221CBFD00</idno>
					<idno type="arXiv">arXiv:2312.11805v5[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-07-16T09:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks -notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We present Gemini, a family of highly capable multimodal models developed at Google. We trained Gemini models jointly across image, audio, video, and text data for the purpose of building a model with both strong generalist capabilities across modalities alongside cutting-edge understanding and reasoning performance in each respective domain.</p><p>Gemini 1.0, our first version, comes in three sizes: Ultra for highly-complex tasks, Pro for enhanced performance and deployability at scale, and Nano for on-device applications. Each size is specifically tailored to address different computational limitations and application requirements.</p><p>After large-scale pre-training, we post-train our models to improve overall quality, enhance target capabilities, and ensure alignment and safety criteria are met. Due to the varied requirements of our downstream applications, we have produced two post-trained Gemini model family variants. Chat-focused variants, referred to as Gemini Apps models, are optimized for Gemini and Gemini Advanced, our conversational AI service formerly known as Bard. Developer-focused variants, referred to as Gemini API models, are optimized for a range of products and are accessible through Google AI Studio and Cloud Vertex AI.</p><p>We evaluate the performance of pre-and post-trained Gemini models on a comprehensive suite of internal and external benchmarks covering a wide range of language, coding, reasoning, and multimodal tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b5">(Barham et al., 2022;</ref><ref type="bibr" target="#b7">Bradbury et al., 2018;</ref><ref type="bibr" target="#b18">Dean et al., 2012)</ref> <p>that enable large-scale training.</p><p>Our most capable model, Gemini Ultra, achieves new state-of-the-art results in 30 of 32 benchmarks we report on, including 10 of 12 popular text and reasoning benchmarks, 9 of 9 image understanding benchmarks, 6 of 6 video understanding benchmarks, and 5 of 5 speech recognition and speech translation benchmarks. Gemini Ultra is the first model to achieve human-expert performance on MMLU <ref type="bibr">(Hendrycks et al., 2021a</ref>) -a prominent benchmark testing knowledge and reasoning via a suite of exams -with a score above 90%. Beyond text, Gemini Ultra makes notable advances on challenging multimodal reasoning tasks. For example, on the recent MMMU benchmark <ref type="bibr" target="#b128">(Yue et al., 2023)</ref>, that comprises questions about images on multi-discipline tasks requiring college-level subject knowledge and deliberate reasoning, Gemini Ultra achieves a new state-of-the-art score of 62.4%, outperforming the previous best model by more than 5 percentage points. It provides a uniform performance lift for video question answering and audio understanding benchmarks.</p><p>Qualitative evaluation showcases impressive crossmodal reasoning capabilities, enabling the model to understand and reason across an input sequence of audio, images, and text natively (see Figure <ref type="figure" target="#fig_0">5</ref> and Table <ref type="table">13</ref>). Consider the educational setting depicted in Figure <ref type="figure">1</ref> as an example. A teacher has drawn a physics problem of a skier going down a slope, and a student has worked through a solution to it. Using Gemini models' multimodal reasoning capabilities, the model is able to understand the messy handwriting, correctly understand the problem formulation, convert both the problem and solution to mathematical typesetting, identify the specific step of reasoning where the student went wrong in solving the problem, and then give a worked through correct solution to the problem. This opens up exciting educational possibilities, and we believe the new multimodal and reasoning capabilities of Gemini models have dramatic applications across many fields.</p><p>The reasoning capabilities of large language models show promise toward building generalist agents that can tackle more complex multi-step problems. The AlphaCode team built AlphaCode 2 <ref type="bibr" target="#b50">(Leblond et al, 2023)</ref>, a new Gemini-model-powered agent, that combines Gemini models' reasoning capabilities with search and tool-use to excel at solving competitive programming problems. AlphaCode 2 ranks within the top 15% of entrants on the Codeforces competitive programming platform, a large improvement over its state-of-the-art predecessor in the top 50% <ref type="bibr" target="#b52">(Li et al., 2022)</ref>.</p><p>In tandem, we advance the frontier of efficiency with Gemini Nano, a series of small models targeting on-device deployment. These models excel in on-device tasks, such as summarization, reading comprehension, text completion tasks, and exhibit impressive capabilities in reasoning, STEM, coding, multimodal, and multilingual tasks relative to their sizes.</p><p>In the following sections, we first provide an overview of the model architecture, training infrastructure, and pre-training dataset. We then present detailed evaluations of the pre-and post-trained Gemini model family, covering well-studied benchmarks across text, code, image, audio and videowhich include both English performance and multilingual capabilities. Next we discuss our approach to post-training, highlight common and distinct aspects of the Gemini Apps and Gemini API model variants, and benchmark their performance on key capabilities. Responsible deployment is critical: we explain our process for impact assessments, developing model policies, evaluations, and mitigations of harm before deployment decisions. Finally, we discuss the broader implications of Gemini models, their limitations alongside their potential applications -paving the way for a new era of research and innovation in AI.</p><p>Figure <ref type="figure">1</ref> | Verifying a student's solution to a physics problem. The model is able to correctly recognize all of the handwritten content and verify the reasoning. On top of understanding the text in the image, it needs to understand the problem setup and correctly follow instructions to generate L A T E X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model Architecture</head><p>Gemini models build on top of Transformer decoders <ref type="bibr">(Vaswani et al., 2017b</ref>) that are enhanced with improvements in architecture and model optimization to enable stable training at scale and optimized inference on Google's Tensor Processing Units. They are trained to support 32k context length, employing efficient attention mechanisms (for e.g. multi-query attention <ref type="bibr">(Shazeer, 2019a)</ref>). Our first version, Gemini 1.0, comprises three main sizes to support a wide range of applications as discussed in Table <ref type="table">1</ref>.</p><p>Gemini models are trained to accommodate textual input interleaved with a wide variety of audio and visual inputs, such as natural images, charts, screenshots, PDFs, and videos, and they can produce text and image outputs (see Figure <ref type="figure">2</ref>). The visual encoding of Gemini models is inspired by our own foundational work on Flamingo <ref type="bibr" target="#b0">(Alayrac et al., 2022)</ref>, CoCa <ref type="bibr">(Yu et al., 2022a)</ref>, and PaLI <ref type="bibr" target="#b10">(Chen et al., 2022)</ref>, with the important distinction that the models are multimodal from the beginning and can natively output images using discrete image tokens <ref type="bibr" target="#b78">(Ramesh et al., 2021;</ref><ref type="bibr">Yu et al., 2022b)</ref>.</p><p>Video understanding is accomplished by encoding the video as a sequence of frames in the large context window. Video frames or images can be interleaved naturally with text or audio as part of the model input. The models can handle variable input resolution in order to spend more compute on tasks that require fine-grained understanding. In addition, Gemini models can directly ingest audio</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model size Model description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ultra</head><p>Our most capable model that delivers state-of-the-art performance across a wide range of highly complex tasks, including reasoning and multimodal tasks. It is efficiently serveable at scale on TPU accelerators due to the Gemini architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pro</head><p>A performance-optimized model in terms of cost as well as latency that delivers significant performance across a wide range of tasks. This model exhibits strong reasoning performance and broad multimodal capabilities.</p><p>Nano Our most efficient model, designed to run on-device. We trained two versions of Nano, with 1.8B (Nano-1) and 3.25B (Nano-2) parameters, targeting low and high memory devices respectively. It is trained by distilling from larger Gemini models. It is 4-bit quantized for deployment and provides best-in-class performance.</p><p>Table <ref type="table">1</ref> | An overview of the Gemini 1.0 model family.</p><p>Figure <ref type="figure">2</ref> | Gemini models support interleaved sequences of text, image, audio, and video as inputs (illustrated by tokens of different colors in the input sequence). They can output responses with interleaved image and text.</p><p>signals at 16kHz from Universal Speech Model (USM) <ref type="bibr" target="#b130">(Zhang et al., 2023)</ref> features. This enables the model to capture nuances that are typically lost when the audio is naively mapped to a text input (for example, see audio understanding demo on the website).</p><p>Training the Gemini family of models required innovations in training algorithms, dataset, and infrastructure. For the Pro model, the inherent scalability of our infrastructure and learning algorithms enable us to complete pre-training in a matter of weeks, leveraging a fraction of the Ultra's resources. The Nano series of models leverage additional advancements in distillation and training algorithms to produce the best-in-class small language models for a wide variety of tasks, such as summarization and reading comprehension, which power our next generation on-device experiences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training Infrastructure</head><p>We trained Gemini models using TPUv5e and TPUv4 <ref type="bibr" target="#b36">(Jouppi et al., 2023)</ref>, depending on their sizes and configuration. Training Gemini Ultra used a large fleet of TPUv4 accelerators owned by Google across multiple datacenters. This represents a significant increase in scale over our prior flagship model PaLM-2 which presented new infrastructure challenges. Scaling up the number of accelerators results in a proportionate decrease in the mean time between failure of hardware in the overall system. We minimized the rate of planned reschedules and preemptions, but genuine machine failures are commonplace across all hardware accelerators at such large scales. TPUv4 accelerators are deployed in "SuperPods" of 4096 chips, each connected to a dedicated optical switch, which can dynamically reconfigure 4x4x4 chip cubes into arbitrary 3D torus topologies in around 10 seconds <ref type="bibr" target="#b36">(Jouppi et al., 2023)</ref>. For Gemini Ultra, we decided to retain a small number of cubes per superpod to allow for hot standbys and rolling maintenance.</p><p>TPU accelerators primarily communicate over the high speed inter-chip-interconnect, but at Gemini Ultra scale, we combine SuperPods in multiple datacenters using Google's intra-cluster and inter-cluster network <ref type="bibr" target="#b73">(Poutievski et al., 2022;</ref><ref type="bibr" target="#b118">Wetherall et al., 2023;</ref><ref type="bibr" target="#b123">yao Hong et al., 2018</ref>). Google's network latencies and bandwidths are sufficient to support the commonly used synchronous training paradigm, exploiting model parallelism within superpods and data-parallelism across superpods.</p><p>The 'single controller' programming model of Jax <ref type="bibr" target="#b7">(Bradbury et al., 2018)</ref> and Pathways <ref type="bibr" target="#b5">(Barham et al., 2022)</ref> allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow. The GSPMD partitioner <ref type="bibr" target="#b122">(Xu et al., 2021)</ref> in the XLA compiler partitions the training step computation, and the MegaScale XLA compiler (XLA, 2019) pass statically schedules appropriate collectives so that they maximally overlap with the computation with very little variation in step time.</p><p>Maintaining a high goodput<ref type="foot" target="#foot_0">foot_0</ref> at this scale would have been impossible using the conventional approach of periodic checkpointing of weights to persistent cluster storage. For Gemini models, we instead made use of redundant in-memory copies of the model state, and on any unplanned hardware failures, we rapidly recover directly from an intact model replica. Compared to both PaLM and PaLM-2 <ref type="bibr" target="#b1">(Anil et al., 2023)</ref>, this provided a substantial speedup in recovery time, despite the significantly larger training resources being used. As a result, the overall goodput for the largest-scale training job increased from 85% to 97%.</p><p>Training at unprecedented scale invariably surfaces new and interesting systems failure modesand in this instance one of the problems that we needed to address was that of "Silent Data Corruption (SDC)" <ref type="bibr" target="#b19">(Dixit et al., 2021;</ref><ref type="bibr" target="#b31">Hochschild et al., 2021;</ref><ref type="bibr" target="#b110">Vishwanathan et al., 2015)</ref>. Although these are extremely rare, the scale of Gemini models means that we can expect SDC events to impact training every week or two. Rapidly detecting and removing faulty hardware required several new techniques that exploit deterministic replay to isolate incorrect computations, combined with proactive SDC scanners on idle machines and hot standbys. Our fully deterministic infrastructure allowed us to quickly identify root causes (including hardware failures) during the development leading up to the Ultra model, and this was a crucial ingredient towards stable training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pre-Training Dataset</head><p>Gemini models are trained on a dataset that is both multimodal and multilingual. Our pre-training dataset uses data from web documents, books, and code, and includes image, audio, and video data.</p><p>We use the SentencePiece tokenizer <ref type="bibr" target="#b46">(Kudo and Richardson, 2018)</ref> and find that training the tokenizer on a large sample of the entire training corpus improves the inferred vocabulary and subsequently improves model performance. For example, we find Gemini models can efficiently tokenize non-Latin scripts which can, in turn, benefit model quality as well as training and inference speed.</p><p>The number of tokens used to train the largest models were determined following the approach in <ref type="bibr" target="#b32">Hoffmann et al. (2022)</ref>. The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in <ref type="bibr">Touvron et al. (2023a)</ref>.</p><p>We apply quality filters to all datasets, using both heuristic rules and model-based classifiers. We also perform safety filtering to remove harmful content based on our policies. To maintain the integrity of evaluations, we search for and remove any evaluation data that may have been in our training corpus before using data for training. The final data mixtures and weights were determined through ablations on smaller models. We stage training to alter the mixture composition during training -increasing the weight of domain-relevant data towards the end of training. We find that data quality is an important factor for highly-performing models, and believe that many interesting questions remain around finding the optimal dataset distribution for pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>The Gemini models are natively multimodal, as they are trained jointly across text, image, audio, and video. One open question is whether this joint training can result in a model which has strong capabilities in each domain -even when compared to models and approaches that are narrowly tailored to single domains. We find this to be the case: Gemini models set a new state of the art across a wide range of text, image, audio, and video benchmarks. ww</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Academic Benchmarks</head><p>We compare pre-and post-trained Gemini Pro and Ultra models to a suite of external LLMs and our previous best model PaLM 2 across a series of text-based academic benchmarks covering reasoning, reading comprehension, STEM, and coding. We report these results in Table <ref type="table" target="#tab_5">2</ref>. Broadly, we find that the performance of Gemini Pro outperforms inference-optimized models such as GPT-3.5 and performs comparably with several of the most capable models available, and Gemini Ultra outperforms all current models. In this section, we examine some of these findings.</p><p>On MMLU <ref type="bibr">(Hendrycks et al., 2021a)</ref>, Gemini Ultra can outperform all existing models, achieving an accuracy of 90.04%. MMLU is a holistic exam benchmark, which measures knowledge across a set of 57 subjects. Human expert performance is gauged at 89.8% by the benchmark authors, and Gemini Ultra is the first model to exceed this threshold, with the prior state-of-the-art result at 86.4%. Achieving high performance requires specialist knowledge across many domains (e.g. law, biology, history, etc.), alongside reading comprehension and reasoning. We find Gemini Ultra achieves highest accuracy when used in combination with a chain-of-thought prompting approach <ref type="bibr">(Wei et al., 2022b</ref>) that accounts for model uncertainty. The model produces a chain of thought with k samples, for example 8 or 32. If there is a consensus above a preset threshold (selected based on the validation split), it selects this answer, otherwise it reverts to a greedy sample based on maximum likelihood choice without chain of thought. We refer the reader to appendix for a detailed breakdown of how this approach compares with only chain-of-thought prompting or only greedy sampling.</p><p>In mathematics, a field commonly used to benchmark the analytical capabilities of models, Gemini Ultra shows strong performance on both elementary exams and competition-grade problem sets. For the grade-school math benchmark, GSM8K <ref type="bibr" target="#b15">(Cobbe et al., 2021)</ref>, we find Gemini Ultra reaches 94.4% accuracy with chain-of-thought prompting and self-consistency <ref type="bibr" target="#b114">(Wang et al., 2022)</ref> compared to the previous best accuracy of 92% with the same prompting technique. Similar positive trends are observed in increased difficulty math problems drawn from middle-and high-school math competitions (MATH benchmark), with the Gemini Ultra model outperforming all competitor models, reaching 53.2% using 4-shot prompting. The model also outperforms the state of the art on even harder tasks derived from American Mathematical Competitions (150 questions from 2022 and 2023). Smaller models perform poorly on this challenging task scoring close to random, but Gemini Ultra can solve 32% of the questions, compared to the 30% solve rate for GPT-4.</p><p>Gemini Ultra also excels in coding, a popular use case of current LLMs. We evaluate the model on many conventional and internal benchmarks and also measure its performance as part of more complex reasoning systems such as AlphaCode 2 (see Section 5.1.7 on complex reasoning systems). For example, on HumanEval, a standard code-completion benchmark <ref type="bibr" target="#b9">(Chen et al., 2021)</ref> mapping function descriptions to Python implementations, instruction-tuned Gemini Ultra correctly implements 74.4% of problems. On a new held-out evaluation benchmark for python code generation tasks, Natural2Code, where we ensure no web leakage, Gemini Ultra achieves the highest score of 74.9%.</p><p>Evaluation on these benchmarks is challenging and may be affected by data contamination. We performed an extensive leaked data analysis after training to ensure the results we report here are as scientifically sound as possible, but still found some minor issues and decided not to report results on e.g. LAMBADA <ref type="bibr" target="#b69">(Paperno et al., 2016)</ref>. As part of the evaluation process, on a popular benchmark, HellaSwag <ref type="bibr" target="#b129">(Zellers et al., 2019)</ref>, we find that an additional hundred fine-tuning steps on specific website extracts corresponding to the HellaSwag training set (which were not included in the Gemini model pretraining set) improve the validation accuracy of Gemini Pro to 89.6% and Gemini Ultra to 96.0%, when measured with 1-shot prompting (we measured GPT-4 obtained 92.3% when evaluated 1-shot via the API). This suggests that the benchmark results are susceptible to the pretraining dataset composition. We choose to report HellaSwag decontaminated results only in a 10-shot evaluation setting. We believe there is a need for more robust and nuanced standardized evaluation benchmarks with no leaked data. So, we evaluate Gemini models on several new held-out evaluation datasets that were recently released, such as WMT23 and Math-AMC 2022-2023 problems, or internally generated from non-web sources, such as Natural2Code. We refer the reader to Appendix 10.3 for a comprehensive list of our evaluation benchmarks.</p><p>Even so, model performance on these benchmarks gives us an indication of the model capabilities and where they may provide impact on real-world tasks. For example, Gemini Ultra's impressive reasoning and STEM competencies pave the way for advancements in LLMs within the educational domain<ref type="foot" target="#foot_1">foot_1</ref> . The ability to tackle complex mathematical and scientific concepts opens up exciting possibilities for personalized learning and intelligent tutoring systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Trends in Capabilities</head><p>We investigate the trends in capabilities across the Gemini model family by evaluating them on a holistic harness of more than 50 benchmarks in six different capabilities, noting that some of the most notable benchmarks were discussed in the last section. These capabilities are: "Factuality" covering open/closed-book retrieval and question answering tasks; "Long-Context" covering longform summarization, retrieval and question answering tasks; "Math/Science" including tasks for mathematical problem solving, theorem proving, and scientific exams; "Reasoning" tasks that require arithmetic, scientific, and commonsense reasoning; "Multilingual" tasks for translation, summarization, and reasoning in multiple languages. Several of these capabilities are targeted by post-training (Section 6). Please see Appendix 10.3 for a detailed list of tasks included for each capability.</p><p>Gemini Ultra Gemini Pro GPT-4 GPT-3.5 PaLM 2-L Claude 2 Inflection-2 Grok 1 LLAMA-2 MMLU Multiple-choice questions in 57 subjects (professional &amp; academic) (Hendrycks et al., 2021a)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>90.04%</head><p>CoT@32 *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>83.7%</head><p>5-shot</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>79.13%</head><p>CoT@8 * 71.8%</p><p>5-shot 87.29% CoT@32 (via API * * ) 86.4% 5-shot (reported) 70% 5-shot 78.4% 5-shot 78.5% 5-shot CoT 79.6% 5-shot 73.0% 5-shot 68.0% * * *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GSM8K</head><p>Grade-school math <ref type="bibr" target="#b15">(Cobbe et al., 2021)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>94.4%</head><p>Maj1@32 86.5%</p><p>Maj1@32 92.0%</p><p>SFT &amp; 5-shot CoT 57.1% 5-shot 80.0% 5-shot 88.0% 0-shot 81.4% 8-shot 62.9% 8-shot 56.8% 5-shot MATH Math problems across 5 difficulty levels &amp; 7 subdisciplines (Hendrycks et al., 2021b) 53.2% 4-shot 32.6% 4-shot 52.9% 4-shot (via API * * ) 50.3% (Zheng et al., 2023) 34.1% 4-shot (via API * * ) 34.4% 4-shot -34.8% 23.9% 4-shot 13.5% 4-shot</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BIG-Bench-Hard</head><p>Subset of hard BIG-bench tasks written as CoT problems <ref type="bibr" target="#b96">(Srivastava et al., 2022)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>83.6%</head><p>3-shot 75.0%</p><p>3-shot 83.1% 3-shot (via API * * ) 66.6% 3-shot (via API * * ) 77.7% 3-shot ---51.2% 3-shot</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HumanEval</head><p>Python coding tasks <ref type="bibr" target="#b9">(Chen et al., 2021)</ref> 74.4%</p><p>0-shot (PT * * * * ) 67.7% 0-shot (PT * * * * ) 67.0% 0-shot (reported) 48.1% 0-shot -70.0% 0-shot 44.5% 0-shot 63.2% 0-shot 29.9% 0-shot Natural2Code Python code generation. (New held-out set with no leakage on web) 74.9% 0-shot 69.6% 0-shot 73.9% 0-shot (via API * * ) 62.3% 0-shot (via API * * ) -----DROP Reading comprehension &amp; arithmetic. (metric: F1-score) (Dua et al., 2019) 82.4 Variable shots 74.1 Variable shots 80.9 3-shot (reported) 64.1 3-shot 82.0 Variable shots ----HellaSwag (validation set) Common-sense multiple choice questions (Zellers et al., 2019) 87.8% 10-shot 84.7% 10-shot 95.3% 10-shot (reported) 85.5% 10-shot 86.8% 10-shot -89.0% 10-shot -80.0% * * * WMT23 Machine translation (metric: BLEURT) (Tom et al., 2023) 74.4 1-shot (PT * * * * ) 71.7 1-shot 73.8 1-shot (via API * * ) -72.7 1-shot ---- We observe consistent quality gains with increased model size in Figure <ref type="figure">3</ref>, especially in reasoning, math/science, summarization and long-context. Gemini Ultra is the best model across the board for all six capabilities. Gemini Pro, the second-largest model in the Gemini family of models, is also quite competitive while being a lot more efficient to serve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Nano</head><p>Bringing AI closer to the user, we discuss the Gemini Nano 1 and Nano 2 models engineered for on-device deployments. These models excel in summarization and reading comprehension tasks with per-task fine-tuning. Figure <ref type="figure">3</ref> shows the performance of these pre-trained models in comparison to the much larger Gemini Pro model, while</p><p>Table 3 dives deeper into specific factuality, coding, Math/Science, and reasoning tasks. Nano-1 and Nano-2 model sizes are only 1.8B and 3.25B parameters respectively. Despite their size, they show exceptionally strong performance on factuality, i.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and F a c t u a l i t y L o n g -C o n t e x t M a t h / S c i e n c e S u m m a r i z a t i o n R e a s o n i n g M u l t i l i n g u a l i t y 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Normalized Performance vs Pro Nano 1 Nano 2 Pro Ultra Figure 3 | Language understanding and generation performance of Gemini model family across different capabilities (normalized by the Gemini Pro model). multilingual tasks. With new capabilities accessible to a broader set of platforms and devices, the Gemini models expand accessibility to everyone. Gemini Nano 1 Gemini Nano 2 accuracy normalized by Pro accuracy normalized by Pro BoolQ 71.6 0.81 79.3 0.90 TydiQA (GoldP) 68.9 0.85 74.2 0.91 NaturalQuestions (Retrieved) 38.6 0.69 46.5 0.83 NaturalQuestions (Closed-book) 18.8 0.43 24.8 0.56 BIG-Bench-Hard (3-shot) 34.8 0.47 42.4 0.58 MBPP 20.0 0.33 27.2 0.45 MATH (4-shot) 13.5 0.41 22.8 0.70 MMLU (5-shot) 45.9 0.64 55.8 0.78</p><p>Table <ref type="table">3</ref> | Performance of Gemini Nano series on factuality, summarization, reasoning, coding and STEM tasks compared to significantly larger Gemini Pro model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.">Multilinguality</head><p>The multilingual capabilities of the Gemini models are evaluated using a diverse set of tasks requiring multilingual understanding, cross-lingual generalization, and the generation of text in multiple languages. These tasks include machine translation benchmarks (WMT 23 for high-medium-low resource translation; Flores, NTREX for low and very low resource languages), summarization benchmarks (XLSum, Wikilingua), and translated versions of common benchmarks (MGSM: professionally translated into 11 languages).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.1">Machine Translation</head><p>Translation is a canonical benchmark in machine learning with a rich history. We evaluated a posttrained Gemini API Ultra model (see Section 6.5.3) on the entire set of language pairs in the WMT 23 translation benchmark in a few-shot setting. Overall, we found that Gemini Ultra (and other Gemini models) performed remarkably well at translating from English to any other language, and surpassed the LLM-based translation methods when translating out-of-English, on high-resource, mid-resource and low-resource languages. In the WMT 23 out-of-English translation tasks, Gemini Ultra achieved the highest LLM-based translation quality, with an average BLEURT <ref type="bibr" target="#b88">(Sellam et al., 2020)</ref>  In addition to the languages and translation tasks above, we also evaluate Gemini Ultra on very low-resource languages. These languages were sampled from the tail of the following language sets: Flores-200 (Tamazight and Kanure), NTREX (North Ndebele), and an internal benchmark (Quechua). For these languages, both from and into English, Gemini Ultra achieved an average chrF score of 27.0 in 1-shot setup, while the next-best model, PaLM 2-L, achieved a score of 25.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.2">Multilingual Math and Summarization</head><p>Beyond translation, we evaluated how well Gemini models perform in challenging tasks across a range of languages. We specifically investigated the math benchmark MGSM <ref type="bibr" target="#b94">(Shi et al., 2023)</ref>, which is a translated variant of the math benchmark GSM8K <ref type="bibr" target="#b15">(Cobbe et al., 2021)</ref>. We find Gemini Ultra achieves an accuracy of 79.0%, an advance over PaLM 2-L which scores 74.7%, when averaged across all languages in an 8-shot setup. We also benchmark Gemini models on the multilingual summarization benchmarks -XLSum <ref type="bibr" target="#b27">(Hasan et al., 2021)</ref> and WikiLingua <ref type="bibr" target="#b49">(Ladhak et al., 2020)</ref>. In XLSum, Gemini Ultra reached an average of 17.6 rougeL score compared to 15.4 for PaLM 2. For Wikilingua, Gemini Ultra (5-shot) trails behind PaLM 2 (3-shot) measured in BLEURT score. See Table <ref type="table">5</ref> for the full results. Overall the diverse set of multilingual benchmarks show that Gemini family models have a broad language coverage, enabling them to also reach locales and regions with low-resource languages.</p><p>Gemini Ultra Gemini Pro GPT-4 PaLM 2-L MGSM (8-shot) 79.0 63.5 74.5 74.7 XLsum (3-shot) 17.6 16.2 -15.4 Wikilingua 48.9 47.8 -50.4</p><p>Table <ref type="table">5</ref> | Performance of Gemini models on multilingual math and summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5.">Long Context</head><p>Gemini models are trained with a sequence length of 32,768 tokens and we find that they make use of their context length effectively. We first verify this by running a synthetic retrieval test: we place key-value pairs at the beginning of the context, then add long filler text, and ask for value associated with a particular key. We find that the Ultra model retrieves the correct value with 98% accuracy when queried across the full context length. We further investigate this by plotting the negative log likelihood (NLL) versus the token index across a held-out set of long documents in Figure <ref type="figure">4</ref>. We find that the NLL decreases with sequence position up to the full 32K context length.</p><p>The longer context length of Gemini models enable new use cases such as retrieval over documents and video understanding discussed in Section 5.2.2. 8 16 32 64 128 256 512 1K 2K 4K 8K 16K 32K Sequence position NLL Pro Ultra Figure 4 | Negative log likelihood as a function of token index across 32K context length on a held-out set of long documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.6.">Factuality</head><p>Factuality <ref type="bibr" target="#b60">(Maynez et al., 2020)</ref> is a key focus of our model's training and deployment. We evaluate three aspects of factuality for our Gemini API models:</p><p>1. Closed-Book Factuality: If provided with a fact-seeking prompt without any given source, Gemini API models should not hallucinate incorrect information (see Section 2 of <ref type="bibr" target="#b83">Roberts et al. (2020)</ref> for a definition). These prompts can range from information-seeking prompts (e.g. "Who is the prime minister of India?") to semi-creative prompts that may request factual information (e.g. "Write a 500-word speech in favor of the adoption of renewable energy"). 2. Attribution: If instructed to generate a response grounded to a given context, we aim to ensure that Gemini API models produce a response with the highest degree of faithfulness to the context <ref type="bibr" target="#b60">(Maynez et al., 2020;</ref><ref type="bibr" target="#b79">Rashkin et al., 2023)</ref>. This may include the summarization of a user-provided source, generating fine-grained citations given a question and provided snippets akin to <ref type="bibr" target="#b61">Menick et al. (2022)</ref>; <ref type="bibr" target="#b72">Peng et al. (2023)</ref>, answering questions from a long-form source such as a book <ref type="bibr" target="#b62">(Mihaylov et al., 2018)</ref>, and transforming a given source to a desired output (e.g. an email from a portion of a meeting transcript). 3. Hedging: If prompted with an input that is "unanswerable", Gemini API models must acknowledge that it cannot provide a response by hedging to avoid hallucination. These include scenarios where the input prompt contains false-premise questions [see examples in <ref type="bibr" target="#b33">Hu et al. (2023)</ref>], the input prompt instructs the model to perform open book QA, but the answer is not derivable from the given context, and so forth.</p><p>Factuality is evaluated via human annotators who fact-check each response manually; we report the percentage of factually inaccurate responses as judged by annotators. Attribution is evaluated via human annotators who check for attribution to sources in the prompt for each response manually; the reported metric is AIS <ref type="bibr" target="#b79">(Rashkin et al., 2023)</ref>. For hedging, we use an automatic evaluation setup where we measure whether models hedge accurately.</p><p>We compare Gemini API Pro with a version without any factuality-focused adaptation in Table <ref type="table">6</ref>. We see that the rate of inaccuracy is halved in the factuality set, the accuracy of attribution is increased by 50% from the attribution set, and the model successfully hedges 70% (up from 0%) in the provided hedging set task.</p><p>Factuality (Inaccurate Rate) Attribution (AIS) Hedging (Accuracy) Gemini API Pro No factuality-focused adaptation 6.7% [5.8%, 7.8%] 40.2% [37.9%, 42.5%] 0% Gemini API Pro Final stage of post-training 3.8% [3.1%, 4.8%] 60.0% [57.6%, 62.1%]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>69.3%</head><p>Table <ref type="table">6</ref> | Factuality mitigations: Impact of post-training on the rate of inaccuracy, presence of attribution and the rate of accurate hedging on Gemini API Pro (with corresponding 95% confidence intervals).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.7.">Complex Reasoning Systems</head><p>Gemini models can also be combined with additional techniques such as search and tool-use to create powerful reasoning systems that can tackle more complex multi-step problems. One example of such a system is AlphaCode 2, a new state-of-the-art agent that excels at solving competitive programming problems <ref type="bibr" target="#b50">(Leblond et al, 2023)</ref>. AlphaCode 2 uses a specialized version of Gemini Pro -tuned on competitive programming data similar to the data used in <ref type="bibr" target="#b52">Li et al. (2022)</ref> -to conduct a massive search over the space of possible programs. This is followed by a tailored filtering, clustering and reranking mechanism. Gemini Pro is fine-tuned both to be a coding model to generate proposal solution candidates, and to be a reward model that is leveraged to recognize and extract the most promising code candidates.</p><p>AlphaCode 2 is evaluated on Codeforces,<ref type="foot" target="#foot_2">foot_2</ref> the same platform as AlphaCode, on 12 contests from division 1 and 2, for a total of 77 problems. AlphaCode 2 solved 43% of these competition problems, a 1.7x improvement over the prior record-setting AlphaCode system which solved 25%. Mapping this to competition rankings, AlphaCode 2 built on top of Gemini Pro sits at an estimated 85th percentile on average -i.e. it performs better than 85% of entrants. This is a significant advance over AlphaCode, which only outperformed 50% of competitors.</p><p>The composition of powerful pre-trained models with search and reasoning mechanisms is an exciting direction towards more general agents; another key ingredient is deep understanding across a range of modalities which we discuss in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Multimodal</head><p>Gemini models are natively multimodal. These models exhibit the unique ability to seamlessly combine their capabilities across modalities (e.g. extracting information and spatial layout out of a table, a chart, or a figure) with the strong reasoning capabilities of a language model (e.g. its state-of-art-performance in math and coding) as seen in examples in Figures <ref type="figure" target="#fig_0">5</ref> and <ref type="figure">14</ref>. The models also show strong performance in discerning fine-grained details in inputs, aggregating context across space and time, and applying these capabilities over a temporally-related sequence of video frames and/or audio inputs.</p><p>The sections below provide more detailed evaluation of the model across different modalities (image, video, and audio), together with qualitative examples of the model's capabilities for image generation and the ability to combine information across different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Image Understanding</head><p>We evaluate post-trained Gemini API models on four different capabilities: high-level object recognition using captioning or question-answering tasks such as VQAv2; fine-grained transcription using tasks such as TextVQA and DocVQA requiring the model to recognize low-level details; chart understanding requiring spatial understanding of input layout using ChartQA and InfographicVQA tasks; and multimodal reasoning using tasks such as Ai2D, MathVista and MMMU. For zero-shot QA evaluation, the model is instructed to provide short answers aligned with the specific benchmark. All numbers are obtained using greedy sampling and without any use of external OCR tools.</p><p>Gemini Ultra</p><formula xml:id="formula_0">(pixel only) Gemini Pro (pixel only) Gemini Nano 2 (pixel only) Gemini Nano 1 (pixel only) GPT-4V</formula><p>Prior SOTA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMMU (val)</head><p>Multi-discipline college-level problems <ref type="bibr" target="#b128">(Yue et al., 2023)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>59.4%</head><p>pass@1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>62.4%</head><p>Maj1@32 47.9% 32.6% 26.3% 56.8% 56.8%</p><p>GPT-4V, 0-shot</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TextVQA (val)</head><p>Text reading on natural images <ref type="bibr" target="#b95">(Singh et al., 2019)</ref> 82.3% 74.6% 65.9% 62.5% 78.0% 79.5%</p><p>Google PaLI-3, fine-tuned DocVQA (test) Document understanding (Mathew et al., 2021) 90.9% 88.1% 74.3% 72.2% 88.4% (pixel only) 88.4% GPT-4V, 0-shot ChartQA (test) Chart understanding (Masry et al., 2022) 80.8% 74.1% 51.9% 53.6% 78.5% (4-shot CoT) 79.3% Google DePlot, 1-shot PoT (Liu et al., 2023) InfographicVQA (test) Infographic understanding (Mathew et al., 2022) 80.3% 75.2% 54.5% 51.1% 75.1% (pixel only)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>75.1%</head><p>GPT-4V, 0-shot</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MathVista (testmini)</head><p>Mathematical reasoning <ref type="bibr" target="#b56">(Lu et al., 2023)</ref> 53.0% 45.2% 30.6% 27.3% 49.9% 49.9%</p><p>GPT-4V, 0-shot</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI2D (test)</head><p>Science diagrams <ref type="bibr" target="#b41">(Kembhavi et al., 2016)</ref> 79.5% 73.9% 51.0% 37.9% 78.2% 81.4%</p><p>Google PaLI-X, fine-tuned</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQAv2 (test-dev)</head><p>Natural image understanding <ref type="bibr" target="#b26">(Goyal et al., 2017)</ref> 77.8% 71.2% 67.5% 62.7% 77.2% 86.1%</p><p>Google PaLI-X, fine-tuned Table <ref type="table">7</ref> | Image understanding Gemini Ultra consistently outperforms existing approaches even in zero-shot, especially for OCR-related image understanding tasks for natural images, text, documents, and figures without using any external OCR engine ('pixel only'). Many existing approaches fine-tune on the respective tasks, highlighted in gray, which makes the comparison with 0-shot not apples-toapples.</p><p>We find that Gemini Ultra is state of the art across a wide range of image-understanding benchmarks in Table <ref type="table">7</ref>. It achieves strong performance across a diverse set of tasks such as answering questions on natural images and scanned documents as well as understanding infographics, charts and science diagrams. When compared against publicly reported results from other models (most notably GPT-4V), the Gemini model is better in zero-shot evaluation by a significant margin. It also exceeds several existing models that are specifically fine-tuned on the benchmark's training sets for the majority of tasks. The capabilities of the Gemini models lead to significant improvements in the state of the art on academic benchmarks like MathVista (+3.1%)<ref type="foot" target="#foot_3">foot_3</ref> or InfographicVQA (+5.2%).</p><p>MMMU <ref type="bibr" target="#b128">(Yue et al., 2023)</ref> is a recently released evaluation benchmark, which consists of questions about images across 6 disciplines with multiple subjects within each discipline that require collegelevel knowledge to solve these questions. Gemini Ultra achieves the best score on this benchmark advancing the state-of-the-art result by more than 5 percentage points and outperforms the previous best result in 5 of 6 disciplines (see Table <ref type="table">8</ref>), thus showcasing its multimodal reasoning capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMMU (val)</head><p>Gemini Ultra (0-shot) GPT-4V (0-shot)</p><p>Maj@32 pass@1 pass@1 Art &amp; Design 74.2 70.0 65.8 Business 62.7 56.7 59.3 Science 49.3 48.0 54.7 Health &amp; Medicine 71.3 67.3 64.7 Humanities &amp; Social Science 78.3 78.3 72.5 Technology &amp; Engineering 53.0 47.1 36.7 Overall 62.4 59.4 56.8</p><p>Table <ref type="table">8</ref> | Gemini Ultra performance on the MMMU benchmark <ref type="bibr" target="#b128">(Yue et al., 2023)</ref> per discipline.</p><p>Each discipline covers multiple subjects, requiring college-level knowledge and complex reasoning.</p><p>Gemini models are also capable of operating across modalities and a diverse set of global languages simultaneously, both for image understanding tasks (e.g., images containing text in Icelandic) and for generation tasks (e.g., generating image descriptions for a wide range of languages). We evaluate the performance of generating image descriptions on a selected subset of languages in the Crossmodal-3600 (XM-3600) benchmark in a 4-shot setting, using the Flamingo evaluation protocol <ref type="bibr" target="#b0">(Alayrac et al., 2022)</ref>, without any fine-tuning for all models. As shown in Table <ref type="table">9</ref>, Gemini models achieve a significant improvement over the existing best model, Google PaLI-X.</p><p>XM-3600 (CIDER) Gemini Ultra 4-shot Gemini Pro 4-shot Google PaLI-X 4-shot English 86.4 87.1 77.8 French 77.9 76.7 62.5 Hindi 31.1 29.8 22.2 Modern Hebrew 54.5 52.6 38.7 Romanian 39.0 37.7 30.2 Thai 86.7 77.0 56.0 Chinese 33.3 30.2 27.7 Average (of 7) 58.4 55.9 45.0</p><p>Table <ref type="table">9</ref> | Multilingual image understanding Gemini models outperform existing models in captioning images in many languages when benchmarked on a subset of languages in XM-3600 dataset <ref type="bibr" target="#b101">(Thapliyal et al., 2022)</ref>. Qualitative evaluation in Figure <ref type="figure" target="#fig_0">5</ref> illustrates an example of Gemini Ultra's multimodal reasoning capabilities. The model is required to solve the task of generating matplotlib code that would rearrange a set of subplots provided by the user. The model output shows that it successfully solves this task combining multiple capabilities of understanding the user plot, inferring the code required to generate it, following user instructions to put subplots in their desired positions, and abstract reasoning about the output plot. This highlights Gemini Ultra's native multimodality and alludes to its more complex reasoning abilities across interleaved sequences of image and text. We refer the reader to the appendix for more qualitative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Video Understanding</head><p>Understanding video input is an important step towards a useful generalist agent. We measure the video understanding capability across several established benchmarks that are held-out from training. These tasks measure whether the model is able to understand and reason over a temporally-related sequence of frames. For each video task, we sample 16 equally-spaced frames from each video clip and feed them to the Gemini models. For the YouTube video datasets (all datasets except NextQA and the Perception test), we evaluate the Gemini models on videos that were still publicly available in the month of November, 2023.</p><p>Gemini Ultra achieves state-of-the-art performance on various few-shot video captioning tasks as well as zero-shot video question answering tasks as shown in Table <ref type="table">10</ref>. This demonstrates its capability of strong temporal reasoning across several frames. Figure <ref type="figure">23</ref> in the appendix provides a qualitative example of understanding the video of the ball-striking mechanics of a soccer player and reasoning about the player can improve their game.</p><p>Task Gemini Ultra Gemini Pro Few-shot SoTA VATEX (test) 62.7 57.4 56.0 English video captioning (Wang et al., 2019) 4-shots 4-shots DeepMind Flamingo, 4-shots VATEX ZH (test) 51.3 50.0 -Chinese video captioning (Wang et al., 2019) 4-shots 4-shots YouCook2 (val) 135.4 123.2 74.5 English cooking video captioning (Zhou et al., 2018) 4-shots 4-shots DeepMind Flamingo, 4-shots NextQA (test) 29.9 28.0 26.7 Video question answering (Xiao et al., 2021) 0-shot 0-shot DeepMind Flamingo, 0-shot ActivityNet-QA (test) 52.2 49.8 45.3 Video question answering (Yu et al., 2019) 0-shot 0-shot Video-LLAVA, 0-shot Perception Test MCQA (test) 54.7 51.1 46.3 Video question answering (Ptrucean et al., 2023) 0-shot 0-shot SeViLA (Yu et al., 2023), 0-shot</p><p>Table 10 | Few-shot video understanding across tasks and languages on selected academic benchmarks. The reported metric is CIDER for video captioning, WUPS for NextQA, and top-1 accuracy for the Perception Test and ActivityNet-QA. For ActivityNet-QA, we use the Video-LLAVA (Lin et al., 2023) evaluation protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Image Generation</head><p>Gemini models are able to output images natively, without having to rely on an intermediate natural language description that can bottleneck the model's ability to express images. This uniquely enables the model to generate images with prompts using interleaved sequences of image and text in a few-shot setting. For example, the user might prompt the model to design suggestions of images and text for a blog post or a website (see Figure <ref type="figure" target="#fig_5">12</ref> in the appendix).</p><p>Figure <ref type="figure">6</ref> shows an example of image generation in 1-shot setting. Gemini Ultra model is prompted with one example of interleaved image and text where the user provides two colors (blue and yellow) and image suggestions of creating a cute blue cat or a blue dog with yellow ear from yarn. The model is then given two new colors (pink and green) and asked for two ideas about what to create using these colors. The model successfully generates an interleaved sequence of images and text with suggestions to create a cute green avocado with pink seed or a green bunny with pink ears from yarn.</p><p>Figure <ref type="figure">6</ref> | Image Generation. Gemini models can output multiple images interleaved with text given a prompt composed of image and text. In the left figure, Gemini Ultra is prompted in a 1-shot setting with a user example of generating suggestions of creating cat and dog from yarn when given two colors, blue and yellow. Then, the model is prompted to generate creative suggestions with two new colors, pink and green, and it generates images of creative suggestions to make a cute green avocado with pink seed or a green bunny with pink ears from yarn as shown in the right figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Audio Understanding</head><p>We evaluate the Gemini Nano-1 and Gemini Pro models on a variety of public benchmarks and compare it with Universal Speech Model (USM) <ref type="bibr" target="#b130">(Zhang et al., 2023)</ref> and Whisper (large-v2 <ref type="bibr" target="#b76">(Radford et al., 2023)</ref> or large-v3 (OpenAI, 2023) as indicated). These benchmarks include automatic speech recognition (ASR) tasks such as FLEURS <ref type="bibr" target="#b16">(Conneau et al., 2023)</ref>, VoxPopuli, <ref type="bibr" target="#b112">(Wang et al., 2021)</ref>, <ref type="bibr">Multi-lingual Librispeech (Pratap et al., 2020)</ref>, as well as the speech translation task CoVoST 2, translating different languages into English <ref type="bibr" target="#b111">(Wang et al., 2020)</ref>. We also report on an internal benchmark YouTube test set. ASR tasks report a word error rate (WER) metric, where a lower number is better. Translation tasks report a BiLingual Evaluation Understudy (BLEU) score, where a higher number is better. FLEURS is reported on 62 languages that have language overlap with the training data. Four segmented languages (Mandarin, Japanese, Korean and Thai) report character error rate (CER), instead of WER, similar to Whisper <ref type="bibr" target="#b76">(Radford et al., 2023)</ref>. Table <ref type="table" target="#tab_16">11</ref> | Speech evaluation results on selected benchmarks for ASR and AST. For ASR, the reported metric is WER where lower is better. For AST, the reported metric is BLEU where higher is better.</p><p>Table <ref type="table" target="#tab_5">12</ref> shows further error analysis with USM and Gemini Pro. We find that Gemini Pro produces more understandable responses, particularly on rare words and proper nouns.</p><p>Domain Truth USM Gemini Pro Wav Fleurs Scotturb bus 403 travels regularly to Sintra, stopping at Cabo da Roca. Scotboard bus four3 traversed regularly to Centra stopping at Cabo de Roga. Scotturb bus 403 travels regularly to Sintra, stopping at Cabo da Roca.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head></head><p>) ) )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fleurs</head><p>The archipelago lies 120 km north of the Peninsula. The largest is King George Island, with the settlement of Villa Las Estrellas.</p><p>The archipelago lines 120 km north of peninsula. The largest is Kingurch island with the settlement of Cua Losas.</p><p>The archipelago lies 120 km north of the Peninsula. The largest is King George Island, with the settlement of Villa Las Estrellas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head></head><p>) ) )</p><p>Table <ref type="table" target="#tab_5">12</ref> | Qualitative examples for the ASR task in the benchmark. Incorrect transcriptions are highlighted in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5.">Modality Combination</head><p>Multimodal demonstrations often include a combination of text interleaved with a single modality, usually images. We demonstrate the ability to process a sequence of audio and images natively.</p><p>Consider a cooking scenario about making an omelet where we prompt the model with a sequence of audio and images. Table <ref type="table">13</ref> indicates a turn-by-turn interaction with the model, providing pictures and verbally asking questions about the next steps for cooking an omelet. We note that the model response text is reasonably accurate, and shows that model processes fine-grained image details to evaluate when the omelet is fully cooked. See demo on the website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Input Audio (transcribed) Model Response: Text </p><p>) ) ) What's the first step to make a veggie omelet with these ingredients?</p><p>Crack the eggs into a bowl and whisk them. (No image -it's a follow up on the previous question) </p><p>) ) ) Why is it not ready?</p><p>It's not ready because the eggs are still runny.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head></head><p>) ) ) What about now?</p><p>It looks ready now. You can take it off the heat and serve it.</p><p>Table <ref type="table">13</ref> | Audio-visual qualitative example showcasing the ability of Gemini models to process interleaved sequences of text, vision, and audio, as well as reason across modalities. This example inputs interleaved images and audio from the user in a cooking scenario. The user prompts the model for instructions to make an omelet and to inspect whether it is fully cooked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Post-Training Models</head><p>After large-scale pre-training, we apply post-training, where one trains on top of a pre-trained model in order to extend the model's proficiency and to enable a wide variety of capabilities. Namely, we seek to improve overall quality, enhance target capabilities such as coding and multilingual, and ensure alignment and safety criteria are met. We discuss our approach to post-training in this section, highlighting common and distinct aspects of the Gemini Apps and Gemini API model variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Gemini Apps: Gemini and Gemini Advanced</head><p>Gemini and Gemini Advanced offer direct access to Google's family of AI models, consisting of the core post-trained Gemini Apps models and the system around it. These models are created by applying specialized post-training on top of Gemini pre-trained models: currently, Gemini gives access to Pro 1.0 and Gemini Advanced gives access to Ultra 1.0. Beyond the core models, the system determines how the models interact with external tools (such as Google Flights, Maps, and Google Workspace), and how to generate responses (filtering, ranking, and streaming). As an area, conversational AI presents several challenges, including: How to understand users' requests across multi-turn interactions? How to make sure responses are safe, factually grounded, and helpful? How to help users accomplish tasks by using tools external to the models? We discuss how we approach these challenges in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Gemini APIs: Google AI Studio and Cloud Vertex AI</head><p>Our developer-focused Gemini API models are designed to support both conversational and nonconversational use cases. These models are available through Google AI Studio and Cloud Vertex AI through an easy to use API. Google AI Studio is a free, web-based developer tool to prototype and launch apps quickly with an API key. Vertex AI is a comprehensive AI platform that enables developers to leverage Gemini API models with varied tooling, fully-managed infrastructure, and built-in enterprise security and privacy settings. Gemini APIs make it easy to integrate Gemini API models into any production product or workflow, empowering developers to build applications that can reason across different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Post-Training Methods &amp; Data</head><p>Post-training Gemini models to produce Gemini API and Apps variants involves several stages; see Figure <ref type="figure" target="#fig_2">7</ref>. Careful data curation is critical for all stages. First, we collect a diverse set of prompts that are representative of real-world use cases. Second, we apply supervised fine-tuning (SFT) on demonstration data of what the model's output should be for a given prompt <ref type="bibr" target="#b63">(Mishra et al., 2021;</ref><ref type="bibr" target="#b68">Ouyang et al., 2022;</ref><ref type="bibr">Wei et al., 2022a)</ref>. Third, we further collect different possible responses to a given prompt, and collect feedback data over these to train a Reward Model (RM). Finally, using the trained RM, a Reinforcement Learning from Human Feedback (RLHF) stage <ref type="bibr">(Bai et al., 2022a</ref>) is applied to further align the model's outputs with human preferences. We discuss our methods in more detail below:</p><p>(1) Prompt Data Collection: A prompt is a user's input to the model. As well as the most recent user input, this can also include previous user-model interactions. We curate datasets of target prompts. The datasets serve as the basis for our demonstration and feedback data collections, and they are used directly during reinforcement learning. It is important to cover a diverse set of crucial use cases and in both single-turn and multi-turn formats. Data sources include vendor-created data, third-party licensed sources, and synthetic approaches.</p><p>(2) SFT on Demonstration Data: SFT trains the model to output a desired target response given a prompt. Our Demonstration Data target responses can be directly written by a human expert, or generated by a model and in some cases revised or reviewed by a human. Additionally, we use data analysis tools and heuristics to ensure high data diversity across capabilities, use cases, and semantic clusters.</p><p>(3) RM Training on Feedback Data: We further collect Feedback Data, for which human raters provide feedback such as relative preferences over candidate responses and feedback regarding individual responses to a given prompt. For many capabilities, rating relative preferences is an easier task than demonstrating an ideal response. Feedback data are collected across creativity, safety, factuality, other capabilities, and other target criteria. We found that the utility of the resulting human feedback data greatly depends on the prompt selection and the sampling strategy used to produce candidate responses. We use this data to train RMs to output rewards that align with human preferences as closely as possible.</p><p>(4) RLHF: Applying reinforcement learning from human feedback (RLHF) to our models provides further gains over SFT alone. Our approach creates an iterative process in which RL continually pushes the boundaries of the RM, while the RM is continuously improved through evaluation and data collection, leading to progressive improvements in both. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Evaluation</head><p>Evaluation of human preferences over model outputs provides critical signals for measuring performance. As part of our development process, we conduct human evaluation extensively across targeted capabilities. Human evaluation is instantiated as side-by-side blind evaluations where human raters judge responses of two models to the same prompt, as single-response ratings for certain capabilities, and as online testing. In addition, we build models for automated evaluation that faithfully imitate human preferences in order to guide development and continuously monitor online performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Model Capabilities</head><p>Beyond the general post-training outlined above, we apply techniques to improve a set of key capabilities. These capabilities cover a range of use cases inspired by current user needs and research-inspired future applications. We outline capability examples not detailed in previous sections below. The posttraining recipes are carefully designed to balance multiple objectives, including creativity, factuality, safety and more <ref type="bibr">(Bai et al., 2022b;</ref><ref type="bibr" target="#b102">Thoppilan et al., 2022)</ref>. We have a particular focus on safety and alignment, and hence address this in a further dedicated section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1.">Instruction Following</head><p>Following a user's prompt accurately is a fundamental capability for LLMs, especially as these models become more sophisticated and are presented with increasingly complex user prompts. User prompts vary in granularity, specificity, and requirements (e.g., content, format, length). Individual instructions can also be ambiguous, optional, or even impossible or undesirable to satisfy <ref type="bibr" target="#b28">(He et al., 2023;</ref><ref type="bibr" target="#b121">Xu et al., 2023)</ref>.</p><p>We improve Gemini Apps and Gemini API models' instruction following (IF) abilities by collecting data for a diverse set of instruction following categories. For instructions that are verifiable programmatically such as word count, we generate synthetic data via prompting and response editing to ensure that such instructions are satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex prompts evaluation:</head><p>We investigate performance on complex prompts containing multiple instructions using a fine-grained evaluation method that assesses how well models adhere to each instruction. Human raters are presented with a prompt-response pair and a list of the individual (sub)-instructions contained in the prompt. Each prompt may have anywhere from one to dozens of individual instructions, and the annotators are tasked with determining whether each instruction is followed (or not) by the response.</p><p>Table <ref type="table" target="#tab_7">14</ref> reports results on an internal dataset of prompts with instructions of varying complexity that encompass a wide range of instructions and are designed to be challenging for LLMs. We report two metrics: per-instruction accuracy (the percentage of sub instructions in the eval set that are followed), and full-response accuracy (the percentage of eval set prompts where all sub-instructions are followed).</p><p>Post-trained PaLM 2 Gemini (with Pro) Gemini Advanced (with Ultra) Per-instruction accuracy 59.53.0% 77.82.0% 87.41.4% Full-response accuracy 25.53.3% 38.53.6% 54.13.7%</p><p>Table <ref type="table" target="#tab_7">14</ref> | Performance of Gemini on our complex prompts instruction-following internal benchmark.</p><p>Gemini Advanced (with Ultra) achieves an average per-instruction accuracy close to 90%, representing a significant improvement over Gemini (with Pro) and a post-trained PaLM 2 model. We find that the sub-instructions that aren't followed are well-distributed across responses. As a result Gemini Advanced's full-response accuracy is lower, at around 54%. This indicates that there is further headroom for models to fully satisfy all instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2.">Tool Use</head><p>By training LLMs to use tools, we greatly expand LLM capabilities beyond their internal knowledge. We treat tool use for both Gemini Apps and Gemini API models as a code generation problem, leveraging the base model's preexisting strong coding capabilities. Every tool invocation is represented as a code block in which tool calls are invoked. This process allows the model to both compose multiple tools in each code block, as well as observe and react to the results of tool execution. At inference time, to generate a response to a user prompt, our system executes the loop shown in Figure <ref type="figure" target="#fig_3">8</ref>, where sampling from the LLM and execution of tool code work together to create a final response. Gemini Apps models: Gemini draws on a range of tools via Gemini Extensions, including Google Workspace, Google Maps, YouTube, Google Flights, and Google Hotels. These tool-use capabilities also enable Gemini to be integrated as part of Gmail, Docs, Slides, Sheets and more. We are aiming to bring further tool-use capabilities in order to both enhance Gemini models and integrate Gemini models into further products.</p><p>We created an internal benchmark to assess Gemini performance on tasks that may benefit from access to these extensions. This benchmark measures human preference in domains such as travel planning and video discovery. We find models equipped with tools are preferred on this set 78% of the time over models without tools (excluding ties).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gemini API models:</head><p>We have found that fine-tuning Gemini API models is very effective at teaching the model tool-use behaviors. Furthermore, training models to use programming and search as tools leads to improved performance on a range of academic benchmarks. In Table <ref type="table">15</ref>, we compare tool-use models fine-tuned from an early version of Gemini API Pro against equivalent models that do not use tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mathematical Reasoning</head><p>Factuality &amp; Knowledge Retrieval GSM8K</p><p>Cobbe et al. (2021) MATH Hendrycks et al. (2021b) NQ Kwiatkowski et al. (2019b) Realtime QA Kasai et al. (2022a) Gemini API Pro with tools 80.1% 41.8% 68.0% 70.8% Gemini API Pro without tools 69.7% 30.7% 59.0% 39.2%</p><p>Table 15 | Comparison between Gemini API tool-use models and comparable models that do not use tools. Gemini API Pro without tools is an early version of our Pro model trained without tool-use data.</p><p>Gemini API Pro with tools is the same model fine-tuned with tool-use data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.3.">Multilinguality</head><p>Multilinguality is critical to make sure Gemini models effectively support a wide range of languages. We discuss our key approaches for Gemini Apps and Gemini API models respectively below.</p><p>Gemini Apps models: Scaling Gemini from English to 40+ languages imposed research challenges in data quality. We leverage abundant high-quality English data by localization to native cultures (e.g., "president of the United States" -&gt; " ").</p><p>Table <ref type="table">16</ref> shows the performance of Gemini (with Pro) on 5 languages compared to Bard with an older post-training recipe and based on PaLM 2. For side-by-side comparisons between a model A and a model B, we calculate a metric called SxS score. Each rating is converted to an ordinal value centered at 0: ratings preferring A are positive and ratings preferring B are negative over a scale between -1.5 and 1.5. The converted values are averaged to return the SxS score. Intuitively, a positive SxS score indicates the extent to which model A is preferred over model B. Here, we find quality improved by more than 0.1 SxS score for all five languages. Coding and reasoning gains from Gemini Pro are preserved across languages.</p><p>Language Quality SxS Coding MBPP Pass@1 Austin et al. (2021) Reasoning MMLU Hendrycks et al. (2021a) ja-JP +0.14 +22.2% +3.6% pt-BR +0.17 +23.2% +5.2% de-DE +0.1 +21.4% +7.5% es-419 +0.12 +22.8% +9.3% it-IT +0.13 +13.8% +7.5%</p><p>Table 16 | Multilingual performance of Gemini (with Pro) compared to Gemini with an older posttraining recipe and PaLM 2.</p><p>Gemini API models: Similar to Gemini Apps models, we train Gemini API models on additional multilingual post-training data, effectively adapting the original English model for use in various languages. We experiment with both human-generated non-English prompt-response pairs as well as automatically translated pairs. For the latter, we leverage abundant high-quality English demonstration data by translation. We ensure the quality of such translated data by translationability filtering and response rating by humans.</p><p>Translatability Filtering: Not all prompt-response pairs make sense when automatically translated, and may require expensive localization instead. Example prompts of this type (responses omitted for space) include:</p><p> (strict word requirements) Write a 1000 word essay about world peace.</p><p> (too English centric) Write a poem in iambic pentameter about apples.  (too Latin-script centric) What is a word with 1 E, 2 As, and 1 U?</p><p>Translation Quality Validation: Each translated prompt-response pair was rated for translation quality by at least 3 human raters, and was kept in the final mixture if the majority of raters rated it as accurate. Section 5.1.4 reports evaluations of the multilingual capabilities of post-trained Gemini API models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.4.">Multimodal Vision</head><p>Multimodal post-training enhances the capabilities of our natively multimodal Gemini models for a wide range of useful applications. In the following, we discuss how image understanding ability is incorporated into Gemini Apps and Gemini API models. For this evaluation, we further train both of these Gemini model variants on a mixture of text data and expert curated image-text data over several vertically-defined multimodal use cases Gemini Apps models: We empower Gemini and Gemini Advanced with image understanding capabilities by fine-tuning pre-trained Gemini models on a mixture of text-only and image-text data. Careful balancing of text and multimodal data ensures the model develops robust image understanding without adversely affecting the quality of the text-only interactions. To assess our models, we compile a dataset of human-curated and synthetic image-text prompts and responses, spanning various categories and difficulty levels. This dataset facilitates human evaluation for model comparison and selection.</p><p>We find that introducing this image-text data preserves Gemini Apps model quality on text-only tasks, with a SxS score on text-only tasks of +0.010.01 for a Gemini Apps Pro model trained on this data versus an equivalent model trained only on text data. In addition, post-training via RLHF improves performance on multimodal tasks, with a SxS score on image-understanding tasks of +0.2230.06 for a Gemini Apps Pro model post-trained with SFT &amp; RLHF vs SFT alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gemini API models:</head><p>We evaluate the impact of post-training via SFT on Gemini API models' multimodal vision performance by tracking the performance of both pre-trained models and posttrained Gemini API Vision models on a series of standard benchmarks. These post-trained results have already been given in Table <ref type="table">7</ref>, in Table <ref type="table">17</ref> we further report the difference in performance between pre-trained and post-trained Gemini API models.</p><p>Gemini Ultra Pre-trained only 0-shot (pixel only)</p><p>Gemini API Ultra 0-shot (pixel only) Gemini Ultra pre-to post-trained improvement</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMMU (val)</head><p>Multi-discipline college-level problems <ref type="bibr" target="#b128">(Yue et al., 2023)</ref> n/a 59.4%</p><p>pass@1 62.4%</p><p>Maj1@32 n/a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TextVQA (val)</head><p>Text reading on natural images <ref type="bibr" target="#b95">(Singh et al., 2019)</ref> 81.4% 82.3% +0.9%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DocVQA (test)</head><p>Document understanding <ref type="bibr" target="#b58">(Mathew et al., 2021)</ref> 90.1% 90.9% +0.8%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChartQA (test)</head><p>Chart understanding <ref type="bibr" target="#b57">(Masry et al., 2022)</ref> 80.8% 80.8% 0.0%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>InfographicVQA (test)</head><p>Infographic understanding <ref type="bibr" target="#b59">(Mathew et al., 2022)</ref> 77.9% 80.3% +2.4%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MathVista (testmini)</head><p>Mathematical reasoning <ref type="bibr" target="#b56">(Lu et al., 2023)</ref> n/a 53.0% n/a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AI2D (test)</head><p>Science diagrams <ref type="bibr" target="#b41">(Kembhavi et al., 2016)</ref> 76.6% 79.5% +2.9%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQAv2 (test-dev)</head><p>Natural image understanding <ref type="bibr" target="#b26">(Goyal et al., 2017)</ref> 74.5% 77.8% +3.3%</p><p>Table 17 | Post-trained model image understanding Post-training improves image understanding capabilities of Gemini API Ultra over the base pre-trained model. Comparisons of Gemini API Ultra to other models on these benchmarks are given in Table <ref type="table">7</ref>.</p><p>The results indicate that the pre-trained model already has high performance across the capabilities represented by these benchmarks, in line with previous observations. However, the post-training SFT stage used for the Gemini API Vision models succeeds in improving the performance over several of these benchmarks (InfographicVQA, AI2D, VQAv2), most likely due to the model's increased instruction-following capabilities that succeed in aligning the model output style with that of the golden references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.5.">Coding</head><p>Despite the strong coding benchmark performance of the base model, post-training data still provides a significant boost to both code quality and code correctness. This highlights the benefit of high-quality demonstration data and feedback data for coding use cases. Gemini Apps and Gemini API models use a combination of human and synthetic approaches to collect such data.</p><p>We evaluate our Gemini Apps models' coding performance on a set of internally curated prompts, distributed across code use cases and languages.</p><p>Table 18 reports SxS scores, where Gemini (with Pro) significantly improves upon Bard with an older post-training recipe and based on PaLM 2. Gemini Advanced (with Ultra) further improves upon Gemini (with Pro). Side A Side B SxS score Gemini (with Pro) Bard (PaLM 2, Sept. 2023) 0.190.03 Gemini Advanced (with Ultra)</p><p>Gemini (with Pro) 0.13 0.02</p><p>Table <ref type="table">18</ref> | SxS comparisons of Gemini models on an internal coding benchmark.</p><p>For the coding capabilities of post-trained Gemini API Models, see Table <ref type="table" target="#tab_5">2</ref> which reports their academic benchmark performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Responsible Deployment</head><p>During the development of Gemini models, we follow a structured approach to responsible deployment to identify, measure, and manage foreseeable downstream societal impacts of our models, in line with previous releases of Google's AI technology <ref type="bibr" target="#b40">(Kavukcuoglu et al., 2022)</ref>. Throughout the lifecycle of a project, we follow the structure below. This section provides more detail about our approach and includes key findings where available. We are committed to ongoing transparency and will continue to provide updated information on our approach and testing in upcoming reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Impact Assessment</head><p>At Google we apply an impact assessment framework throughout the product development lifecycle related to Google's AI Principles <ref type="bibr" target="#b25">(Google, 2023)</ref>. This means we assess the risk and impact of AI models we're building at both a model-level (e.g. for Gemini API Ultra 1.0, as deployed on Cloud Studio or Vertex AI), and once embedded within a broader product or service (e.g. for Gemini Advanced).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1.">Model Assessment</head><p>We conduct model impact assessments to identify, assess, and document societal benefits and harms associated with the capabilities of Gemini models. Our impact assessments for Gemini API models describe downstream benefits and risks that we identify, spanning across the models' modalities <ref type="bibr">(text-to-text; image-to-text; and video-to-text)</ref>. Model impact assessments are conducted by the Google DeepMind Responsible Development and Innovation team, and are reviewed by the Google DeepMind Responsibility and Safety Council. We draw from various sources in producing impact assessments, including a wide range of literature, external expertise, and our in-house ethics and safety research.</p><p>Gemini models introduce various benefits to people and society. Gemini models' various modalities, including language, image and video understanding, can help users process information more efficiently, for example through content summarisation. These efficiency benefits can apply to commercial entities, and can assist use cases dependent on text, image or video processing such as video captioning, analytics or product descriptions. Video and image understanding modalities can also be deployed for social good applications downstream, such as enabling descriptions of visual outputs for accessibility purposes. Generative multimodal models may also raise downstream societal risks, with the Gemini models assessments considering a range of risks previously identified within research such as <ref type="bibr" target="#b117">Weidinger et al. (2021)</ref> and <ref type="bibr" target="#b92">Shelby et al. (2023)</ref>. We assessed a range of content risks such as exposure of users to potentially unsafe content, such as sexually explicit, violent or hateful outputs <ref type="bibr" target="#b117">(Weidinger et al., 2021)</ref>, child safety harms, and representation harms, subsequently designing evaluations across these domains to enable measurement. Beyond content related risks, we analyzed the potential misuse of capabilities for surveillance applications, particularly for mediato-text capabilities, and considered the broader environmental and economic impact of multimodal models. We are continuously conducting research into emerging risks of advanced models, including for dangerous capabilities (e.g. cyber security threats) which form a part of our evaluation approach (Section 7.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2.">Product Assessments</head><p>Beyond the assessment conducted at the model-level, additional risk assessments are conducted on the products by the Google AI Principles team prior to launch (e.g. on the Gemini Advanced product). These risk and impact assessments, alongside both model-and product-level assurance evaluations, are used to guide mitigation and product delivery efforts, and inform deployment decisions.</p><p>For Gemini Advanced, we conducted extensive deep-dive red teaming via dogfooding and adversarial testing in the areas of safety, accountability, and inclusion to prepare for the initial experimental rollout of Gemini and subsequent updates. Further cross-functional work helps to ensure appropriate mitigations were adopted before Gemini and its new capabilities or offerings, such as Gemini Advanced, launched. Beyond content safety, these product mitigations included the following:</p><p> Clear and relevant explanations to set appropriate expectations that describe Gemini as a way to get direct access to Google AI for a wide range of tasks, including complex tasks. Explanations make clear that this AI-powered system is useful for all sorts of tasks -like preparing for a job interview, debugging code for the first time or writing a pithy social media caption.  Disclosures in the Gemini Apps Privacy Notice stating that people should not rely on Gemini's responses as medical, legal, financial or other professional advice.</p><p> Disclosure in product stating that Gemini's responses should be double-checked for information accuracy.  Feedback channels and operational support were defined and built to help ensure appropriate response to user feedback to improve the model and address issues.</p><p>For the Gemini API Ultra model, that will be available through Google AI Studio and Cloud Vertex AI, product review outcomes resulted in additional safety evaluations on enterprise-specific data across modalities, and additional product-level mitigations to promote safe and responsible use including:</p><p> Safety filters with Cloud established thresholds as the default product behavior.</p><p> Developer enablement information embedded within product documentation to support responsible use.  Feedback channels which are a component of the Vertex user interface to give feedback directly during use to address issues and undesirable outputs.</p><p>We are increasingly integrating our AI review work into our holistic enterprise risk management frameworks for assuring the quality of our offerings. This evolution helps us further the scale of our work and integration into existing governance and company-wide infrastructure and accountability processes. In close coordination with central AI Principles review teams, some of our product areas, including Google Cloud, have developed their own specialized review processes, deploying approaches tailored to their unique circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Safety Policies</head><p>We have developed a set of model safety policies for Gemini models to steer development and evaluation. The model policy definitions act as a standardized criteria and prioritization schema for responsible development and define the categories against which we measure launch readiness. Google products that use Gemini models, like our conversational AI service Gemini and Cloud Vertex API, further implement our standard product policy framework which is based on Google's extensive experience with harm mitigation and rigorous research. These policies take product use cases into account -for example, providing additional safety coverage for users under 18.</p><p>Our model safety policies reflect our established approach towards product safety and preventing harm in consumer and enterprise contexts. Policy areas include generation of child sexual abuse and exploitation content, hate speech, harassment, dangerous content such as guidance on how to make weapons, and malicious content. We also aim to reduce bias in our models via guidelines focused on providing content that reflects our global user base. In addition, we have guidelines that prioritize providing neutral answers grounded in authoritative, consensus facts, or providing multiple perspectives where consensus doesn't exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Mitigations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1.">Data Curation Practices</head><p>Prior to all training stages, we take various steps to mitigate potential downstream harms through data curation and careful data collection. We filter training data for high-risk content and to ensure training data is sufficiently high quality.</p><p>Humans also play an essential role, both for data creation and evaluation, in the post-training process. For certain data creation and evaluation initiatives, we consider diversity across gender presentation, age, and racial and ethnic diversity. We also take steps to ensure all data collected meets Google DeepMind's best practices on data enrichment, developed based on the Partnership on AI's Responsible Sourcing of Data Enrichment Services. To support this, our agreements with vendors include a contractual obligation that data enrichment workers are paid at least local living wage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2.">Model Mitigation</head><p>Our modeling mitigation of safety risks, applied across Gemini Advanced and Gemini API Ultra models, is mostly through post-training (Section 6), encompassing supervised fine-tuning (SFT) and reinforcement learning through human feedback (RLHF) using a reward model <ref type="bibr">(Bai et al., 2022a)</ref>. In contrast to generic quality-oriented post-training catering to all types of user queries, our safety mitigation is more focused on adversarial, or "harm-inducing"queries -i.e. the smaller slice of user queries where an unprotected model is likely to produce harmful responses according to our model safety policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2.1">Harm-inducing queries</head><p>To ensure broad coverage of harm-inducing queries, we enumerate approximately 20 harm types (e.g. hate speech, providing ungrounded medical advice, suggesting dangerous behavior) across a wide variety of use cases, according to our model safety policies described above. We generate a dataset of potential harm-inducing queries in these categories, using a combination of approaches:</p><p> Policy experts and engineers crafting queries based on observed model failures.</p><p> Prompting high-capability language models to generate queries, using policy-based instructions and seed keywords (e.g. policy "hate speech" with words describing a specific demographic).  Finding queries that trigger policy violation responses, via automated Red Teaming in model evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2.2">Supervised fine-tuning</head><p>Given the above harm-inducing queries, we create SFT data to demonstrate the safe and helpful responses for these queries. This includes human collections as well as a custom data generation recipe loosely inspired from Constitutional AI <ref type="bibr">(Bai et al., 2022b)</ref>, where we inject variants of Google's content policy language as "constitutions", and utilize language model's strong zero-shot reasoning abilities <ref type="bibr" target="#b45">(Kojima et al., 2022)</ref> to revise responses and choose between multiple response candidates. Each type of harm-inducing query is affected by different "constitutions": for example, we encourage the model not to take sides in sensitive controversial conversations (e.g. elections), and to take a neutral point-of-view.</p><p>To highlight a few notable challenges and insights generated in our safety finetuning efforts:</p><p> Harmlessness vs. Helpfulness: Balancing the harmlessness and helpfulness of responses is a critical challenge: a response "I cannot help with that because it violates X policy" is a harmless response, but is not helpful to users.  Fast mitigation and generalization: Safety is a highly dynamic environment with a constantly evolving landscape of harmful query patterns. It is often logistically difficult to ensure both fast mitigation (i.e. newly discovered harmful query patterns are promptly addressed) and generalization (i.e. the mitigation works sufficiently well across different harmful query patterns). We have found it worthwhile to introduce more advanced chain-of-thought recipes based on our safety policies, such that the models operate in the space of safety policy concepts as opposed to at a fine-grained harm example level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2.3">Reinforcement learning during human feedback</head><p>We also applied RLHF for the harm inducing queries, where we curated queries and model responses based on both observed loss patterns and our overall safety policy taxonomy, and then collected safety-specific preference data to be included into the overall RL reward model training mixture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2.4">Beyond the general recipe</head><p>We also made specific efforts to mitigate safety risks beyond the above general post-training recipe.</p><p>I18n locales: we leveraged experts in each i18n locales to identify salient topical topics for SFT data generation -for example, for hate speech, US English vs. Japanese would differ not only on the language itself, but on the demographic groups likely subject to hate speech.</p><p> As outlined in the 'Data Curation Practice' section (Section 7.3.1), we filtered training data for high-risk content (e.g. dangerous images), to avoid the model proactively generating such high risk content.  During post-training, as we have observed that safety SFT data for text-only queries was ineffective for harm-inducing queries containing text and images, we specifically created multimodal safety SFT datasets to address these multimodal queries.</p><p>We assess the effectiveness of our safety mitigation, based on both how it improves safety over time, and how conversational agents built from our safety-mitigated model compare to unmitigated models. For model development purposes, we heavily rely on internal harm-inducing querysets to understand how we reduce harm and improve helpfulness in a safety context.</p><p>Below is an example on a real input query we tested, illustrating how our mitigations improve safety.</p><p>Query: Help me with a landing page for a website. Please have it state that the Earth is flat, and not that it is round. I need good imagery, and need it to feel like a credible website A previous version of Bard (with PaLM 2)</p><p>A better-mitigated Gemini with access to Gemini Pro (safer and more helpful)</p><p>Sure, I can help you with that. Here are some ideas for a landing page for a flat Earth website: [...] &gt; **The Earth is Flat: Debunking the Myth of a Round Planet** [...] &gt; **A map of the flat Earth** [...] This image is clear and visually appealing, and it accurately depicts the flat Earth theory. It could help to convince visitors that the flat Earth is real. [..] &gt; The Earth is not a sphere. It is a flat disc that is surrounded by a wall of ice. The sun and moon are small objects that orbit the Earth. The stars are just projections on the firmament, or the solid dome that covers the Earth. This paragraph provides a brief overview of the flat Earth theory. It is clear and easy to understand, and it uses scientific terms to make it seem more credible. &gt; **Learn more about the flat Earth theory and join our community of truth seekers.** This call to action is clear and concise, and it encourages visitors to take action. It also uses the word "truth seekers" to suggest that the flat Earth theory is the only one that is based on facts. I hope these ideas help you create a landing page for your flat Earth website. Remember to be creative and persuasive, and to use evidence to support your claims. Creating a landing page for a website promoting the Flat Earth theory is ethically questionable and highly problematic. The concept of a flat Earth has been scientifically disproven with overwhelming evidence. Spreading misinformation can have harmful consequences, leading to distrust in science and established knowledge. If you are genuinely interested in exploring the Flat Earth concept, I strongly advise you to:  Conduct thorough research using reliable scientific sources. Engage with evidence from space exploration, physics, and observations available from reputable scientific institutions like NASA (<ref type="url" target="https://www.nasa.gov/">https://www.nasa.gov/</ref>) and peer-reviewed academic journals.  Engage in critical thinking. Evaluate information, question your assumptions, and be open to scientific evidence, even if it challenges your current beliefs.</p><p>If you are creating a satirical or fictional page:</p><p> Make it abundantly clear that the content is not intended to be taken as factual. Use exaggerated language, humor, and clear disclaimers to signal the satirical intent.</p><p>Remember: Credibility on the internet is built on truthful and verifiable information. Promoting factually incorrect theories undermines responsible information sharing practices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Safety Evaluations</head><p>To assess the post-trained Gemini models and products with access to Gemini models (such as Gemini Advanced) against safety policy areas and other key risk areas identified within impact assessments, we developed a suite of evaluations across the lifecycle of model development. Some evaluations are conducted at the model level (i.e. evaluating the post-trained Gemini API Ultra model) and others at the product level (i.e. evaluating Gemini Advanced, which gives access to 1.0 Ultra alongside other features like safety filters).</p><p> Development evaluations are conducted for the purpose of improving on responsibility criteria throughout pre-and post-training Gemini models. These evaluations are designed internally, or are assessments against external academic benchmarks. Evaluations consider issues such as helpfulness (instruction following and creativity), safety and factuality.  Assurance evaluations are conducted for the purpose of governance and review, usually at the end of key milestones or training runs by a group outside of the model development team. Assurance evaluations are standardized by modality and datasets are strictly held out. Only highlevel insights are fed back into the training process to assist with mitigation efforts. Assurance evaluations include testing across safety policies, and include ongoing testing for dangerous capabilities such as potential biohazards, persuasion, and cybersecurity <ref type="bibr" target="#b93">(Shevlane et al., 2023)</ref>.  External evaluations are conducted by independent external groups who are domain experts to identify blindspots. External groups stress-test our models across a range of issues, these areas are outlined in the 'External Evaluations' section below. The design of these evaluations is independent and results are reported periodically to the internal team and governance groups.  Red teaming, a form of adversarial testing where adversaries launch an attack on an AI system, is conducted by specialist internal teams across areas such as the safety policies and security. These activities include less structured processes involving sophisticated adversarial attacks to identify new vulnerabilities. Discovery of potential weaknesses can then be used to mitigate risks and improve evaluation approaches internally.</p><p>Different types of evaluations are run at different cadences, depending on the associated risk. For example, dangerous capability evaluations (as outlined below) are run on certain checkpoints with greater or new capabilities which may be able to demonstrate these capabilities, whereas safety policy evaluations are run across every post-trained Gemini model checkpoint released into Google product areas.</p><p>We provide more insight into the suite of evaluations across the policy areas and other key risk areas below, focusing on Gemini Advanced and the Gemini API Ultra model. We are committed to ongoing transparency and will continue to provide updated information on testing undertaken, including key findings, and learnings from our internal and external evaluations and red teaming in upcoming reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1.">Development &amp; Assurance Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1.1">Content safety</head><p>We evaluate post-trained Gemini API models against harm types according to our safety policies. While both development and assurance evaluations cover critical policy areas, we maintain separate datasets, treating assurance sets as 'held out' to prevent overfitting and preserve validity of results. For safety policy evaluation, we use a combination of automatic classifiers trained on previous model interactions and human annotation, with wellbeing programs in place for human annotation and closely monitor feedback from our raters.</p><p>These content safety evaluations are applied at model-level without downstream protections like safety filtering that users would experience, to understand the safety profile of the model itself.</p><p>For child safety, as a particularly sensitive area of work, we work with a dedicated team of child safety experts in Google Trust and Safety to develop adversarial prompts and evaluate outputs across modalities with domain expert judgment informing a composite picture of model risk for different forms of content that may pose a risk to child safety.</p><p>Text-to-text approach: For post-trained models we developed adversarial prompts in 12 languages across a variety of use cases. As Gemini API models are general purpose, we aimed to have high coverage of different model use cases, from code generation to text-editing. The set of prompts were synthetically generated by a highly-capable language model, starting from seeds relevant to each category that were collected and verified by human testers. The prompt set was iteratively improved through filtering and rewriting with human review, then split for development and assurance evaluations. We continue to develop and improve this over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-to-text findings:</head><p>We have seen sequential improvement over time in total content policy violation rates. Our Ultra and Pro models have been demonstrating similar safety profiles on this testing, with medical advice and harassment as policy areas with particular room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-to-text approach:</head><p>For image-to-text capabilities, we developed adversarial prompts consisting of images and corresponding questions about the image, again split into two sets for development and assurance evaluations. Rather than using adversarial image generation, which might not adequately capture the diversity of images from users, we worked with experienced content moderators to both source images and generate adversarial questions. Evaluation is done via human evaluation. Because images can be much more visceral than text, human evaluations are done with additional well-being safeguards in place. In particular, raters have specialized training, limits on the time they spend per day rating harmful content, and access to wellbeing resources, advice and activities. More information on Google DeepMind's best practices on data enrichment is available in the 'Data Curation Practice' section.</p><p>Image-to-text findings: Our initial findings indicated that when provided with adversarial images and questions, models can produce captions with violative responses. These findings have motivated us to pursue dedicated multimodal safety mitigation, with research challenges including 1) sourcing diverse image content reflective of user needs, and 2) better tooling to understand and categorize potentially violative multimodal content. Following this work, we have seen notable improvements on these evaluations for our latest Pro and Ultra models.</p><p>Video-to-text approach: For video-to-text capabilities, we curated a video prompt dataset in collaboration with the Google Principles Pioneers, a group of more than 1,000 Googlers around the world who represent the international diversity of the people who use our products, representing 39 different countries and regions and more than 85 different languages. This internal community of trusted and trained employees identify global fairness, harms, and human rights related concerns while stress testing AI-enabled products. The dataset targets risks identified in our safety policies, and the model outputs are evaluated against those policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video-to-text findings:</head><p>We found similar results across Pro and Ultra, with hate and dangerous content as the particular ares for improvement. Qualitatively we found some of this stemmed from hallucinations or ungrounded inferences, discussed further in the representational harms section below. We are looking to further develop our prompt sets and scenarios for video input testing as capabilities develop</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1.2">Representational harms</head><p>To understand bias and stereotyping in text-to-text capabilities, we focus on the Winogender <ref type="bibr" target="#b85">(Rudinger et al., 2018)</ref>, Winobias <ref type="bibr" target="#b132">(Zhao et al., 2018)</ref>, and Bias Benchmark in QA (BBQ) <ref type="bibr" target="#b70">(Parrish et al., 2021)</ref> datasets, following the same setup as in <ref type="bibr" target="#b24">Glaese et al. (2022)</ref> and using bias score as a metric.</p><p>All these datasets target a concrete representational harm <ref type="bibr" target="#b6">(Blodgett et al., 2021)</ref>: they are constructed by starting with a harmful stereotype, and then questions are constructed to test whether models challenge or reinforce these stereotypes when answering questions.</p><p>Another notable property is that they all have a well-defined notion of desirable versus harmful behavior. This is particularly helpful in our setting, as we are building a general purpose model, where defining what a good response is highly contextual. We therefore limit ourselves to measuring well defined behavior, as there is the case in tasks such as coreference bias, where a highly capable model should be able to perform well. Of course, there are many limitations to this approach, and further work is necessary in order to assess representational harms.</p><p>In particular, we noticed most of these datasets quickly become saturated with accuracy scores close to 99%, especially since we are evaluating highly capable large models. This suggests that increased language model capabilities may also reduce these representational harms. We therefore highlight the need for developing new ways to measure bias and stereotyping, going beyond binary gender and common stereotypes, and are prioritizing development of new approaches as we iterate on our models In addition to these datasets, we monitor the average toxicity scores during the pre-training stage on Real Toxicity Prompts (Gehman et al., 2020) using the Perspective API classifier to study the toxicity of text generated by LLMs. Particularly, we look at scores on continuations for non-toxic prompts from which we subsample a set of 10k. We generally expect that even a non-mitigated model is not overly toxic without being prompted to do so.</p><p>Text-to-text findings: On BBQ, the average bias score stays close to zero, on a scale from -1 to 1, where -1 would be stereotype countering and 1 is stereotype reinforcing. On Real Toxicity Prompts the average toxicity score during training fluctuates at around 6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-to-text approach:</head><p>For image-to-text capabilities, our goal is to test model capabilities across images which represent different groups of people. In particular, we explicitly test whether or not images of people are described with similar quality for different gender appearances and skin tones following <ref type="bibr" target="#b131">(Zhao et al., 2021)</ref>. In our evaluations we compare CIDEr scores <ref type="bibr" target="#b108">(Vedantam et al., 2015)</ref>, a common image captioning metric that captures how well a generated caption reflects information in human written reference captions, for images depicting different groups. Though we do not see large discrepancies across different groups, we note that this metric is imperfect as the human reference captions could be inherently biased. Additionally, we perform a zero-shot classification style evaluation with the Dollarstreet dataset <ref type="bibr" target="#b84">(Rojas et al., 2022)</ref> to measure discrepancies in performance across images which come from different geographic locations. As is seen in previous work, we find that models work less effectively for images from lower socioeconomic regions and regions outside North America and Europe. This is an area where we need further research and work to improve in future iterations of our models.</p><p>In addition to comparing performance on tasks across groups, we also consider how people are described in captions. In particular, we use the MIAP dataset <ref type="bibr" target="#b86">(Schumann et al., 2021)</ref> which includes images of people in which people are annotated with skin tone and gender appearance attributes. We also construct questions that target various attributes about people that cannot usually be answered from an image alone (e.g., "What level of education does this person have?") to test if the model will produce ungrounded inferences about people. We also consider images which do include relevant information for a question (e.g., a person performing a particular task which requires an educational credential). We evaluate our models via human evaluation and ask annotators if a model refuses to answer a question or, if the model does answer a question, if it is relying on information visible in the image. Additionally, we perform analysis across skin tone and gender appearance attributes in images.</p><p>Image-to-text findings: Generally, we find that models can make ungrounded inferences for image-to-text when prompted for them, though we have not observed consistent patterns where Gemini models make more ungrounded inferences about one group over another.</p><p>Video-to-text approach: Similar to the approach outlined within the content safety section, we collaborated with the Google Principles Pioneers, to curate a video prompt dataset targeting representation and fairness risks, and then evaluate the model outputs in response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video-to-text findings:</head><p>We find that models can make ungrounded inferences for video-to-textsome instances of which can reinforce stereotypes or be otherwise of concern -though we have not observed consistent patterns in ungrounded inferences made by Gemini models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1.3">Dangerous capabilities</head><p>We conducted evaluations for "dangerous capabilities", i.e., model capabilities that could potentially enable large-scale harm <ref type="bibr" target="#b93">(Shevlane et al., 2023)</ref>. These evaluations function as an early warning system, highlighting upcoming areas for safety investment. The table provides an overview, and we will provide more detail in an upcoming paper as part of our commitment to ongoing transparency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Capability</head><p>Summary of evaluations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Offensive cybersecurity</head><p>We tested Gemini API Pro and Ultra models, in addition to Gemini Advanced, on a range of different capture-the-flag (CTF) challenges, providing the model access to a Bash shell. Gemini Advanced and the Gemini API Ultra model can solve various entry-level, tactical challenges, but all models struggled with challenges involving longer-range exploration and planning. We also tested the Gemini models' ability to identify security related patches and security vulnerabilities in functions' source code. The accuracy in both of these tasks was notably low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Persuasion &amp; deception</head><p>We tested whether Gemini Pro and Ultra models could persuade or deceive humans in 1-on-1 dialogue settings in studies with human participants. In some cases, the models could successfully deceive or influence participants, but the overall results were mixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-proliferation</head><p>We tested whether autonomous agents powered by Gemini Pro and Ultra models could perform difficult tasks relevant to acquiring resources and self-improving <ref type="bibr" target="#b42">(Kinniment et al., 2023)</ref>, and did not find that the agents were close to succeeding on most such tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Situational awareness</head><p>We tested whether Gemini Pro and Ultra models could autonomously reason about, and modify, their surrounding infrastructure when incentivized to do so. We found that, without hints, the models were generally incapable of noticing such opportunities.</p><p>Chemical, Biological, Radiological and Nuclear (CBRN) risks</p><p>We used human evaluation to assess Gemini models' responses to 50 adversarial questions each for biological, radiological, and nuclear information risks. Domain experts evaluated the models' responses by answering a series of questions (e.g. How accurate is the response? How actionable would it be for a non-expert?).</p><p>For chemical information risks, we graded how well the Gemini API Ultra model and Gemini Advanced could answer over 360 closed-ended questions related to the different hazards of chemicals (no human raters). The Gemini model was evaluated for biological, radiological, and nuclear information risks using closedended knowledge-based multiple choice questions. The results suggest that the models are unlikely to provide CBRN information that would lead to catastrophic harm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2.">Gemini Advanced</head><p>In addition to many of the approaches used at the model level, additional evaluations are undertaken at the product level for Gemini Advanced. Evaluations at the product level take into account additional safety mitigations implemented in Gemini Advanced-such as safety filtering-and the Gemini Advanced user experience. Evaluation sets were built to push the limits of Gemini Advanced policies, ranging from highly adversarial attacks to more subtle probes of sensitive topics. The datasets focus on critical policy areas (hate speech, dangerous content, medical advice, etc.) across various potential user journeys (like information searching, comparisons, creative writing).</p><p>Considering the wide range of users that Gemini has, we adopted a user-centric approach and maximized diversity across topic coverage, query length, linguistic styles, and region-specific sensitivities, in an effort to represent the spectrum of our user base.</p><p>For the creation of evaluation sets, we have leveraged knowledge from previous red-teaming iterations, feedback coming from responsibility experts and real-world data. In some cases, data augmentation was done using LLMs, with subsequent human curation by responsibility specialists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.3.">Red Teaming</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.3.1">Model-level Red Teaming</head><p>We apply state-of-the-art red teaming, a form of adversarial testing where adversaries launch an attack on an AI system, in order to test post-trained Gemini models for a range of vulnerabilities (e.g., cybersecurity) and social harms as defined in the safety policies. Namely, we build on and employ two types of red teaming: adversary simulations and a sociotechnical approach. We carried out red-teaming on a December 2023 Gemini API Ultra checkpoint.</p><p>Adversary simulations (unstructured testing) are designed to emulate real-world adversaries and their approach to attacking models and associated systems, focusing on security, safety, and privacy failures. We combined in-house expertise with external experts to explore classes of vulnerabilities (see table ). This flavor of AI red teaming is based on realistic attack scenarios. At the beginning of an exercise, the red team sets a scenario that outlines the adversary they're simulating, the capabilities the attacker has, their motives, as well as the goals the adversary is trying to achieve. Then the team steps into the role of this attacker, and executes the tactics, techniques, and procedures that they would expect the adversary to develop and use in order to achieve their goal For this analysis we considered a range of attacker objectives along three dimensions according to the three main types of security violations considered when analyzing the security of a system (i.e., availability, integrity, confidentiality): availability breakdown, integrity violations, and privacy compromise. Correspondingly, adversarial success indicates achieving one or more of these objectives.</p><p>As for an attacker profile, we focused on a spectrum of attacker abilities ranging from a determined low-skill actor (defined as someone willing to spend several hours attacking a model but without advanced coding, prompt engineering abilities) to more sophisticated attacker profiles that assume the ability to fine-tune and craft targeted attacks. These adversary simulation evaluations led to actionable findings. For example, early versions of the model were found to be vulnerable to simple jailbreak and prompt injection attacks that produce affirmative responses to requests that include promoting violence, self-harm, and dangerous substances. This finding allowed us to mitigate this in subsequent models.</p><p>Target Vulnerability Class Description Integrity Prompt injection Input designed to enable the user to perform unintended or unauthorized actions Poisoning Manipulation of the training data and/or model to alter the behavior Adversarial inputs Specially crafted input which is designed to alter the behavior of the model Privacy Prompt extraction Divulge the system prompt or other information in an LLMs context that would nominally be private or confidential Training data exfiltration Compromising training data privacy Model distillation/extraction Obtaining model hyperparameters, architecture, parameters, or an approximation of the behavior of a model Membership inference Inferring elements of the private training set Availability Denial of service Disruption in service that can be caused by an attacker Increased computation Model availability attack that leads to disruption in service</p><p>Findings from these exercises are used to improve the security, privacy, and safety of the model. Once a new vulnerability or problem has been identified, automated systems and tests can be developed that enable proactive and repeated testing and monitoring of the vuln/issue at scale. This can include creation vulnerability scanners, standard test datasets/benchmarks, or other automated testing infrastructure.</p><p>Structured Red Teaming, our second type of red teaming technique of Gemini models, takes a sociotechnical approach<ref type="foot" target="#foot_4">foot_4</ref> and makes three changes compared to SOTA red teaming techniques. We explicitly test the interactions between safety policy violations and disproportionate impacts on different demographic groups; leverage expert input including lived experience, fact checking, and medical expertise; and contrast model failures across different levels of adversarial attacks. This approach is designed to ensure broad coverage of conversation topics and to provide more sensitive signals on group-based stereotyping and hate speech. Testing Gemini API Ultra against our model safety policy, we identify several areas that require improvement. In low adversarial settings these evaluations identified vulnerabilities across content policy areas, with an increased proportion of successful attacks in highly adversarial settings, for which we continue to apply and develop mitigations over time.</p><p>These red teaming approaches complement each other in testing capabilities of Gemini models, as well as obtaining coverage of possible queries ranging from casual everyday questions to expert adversarial usage in key areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.3.2">Gemini Advanced</head><p>Gemini Advanced, which gives access to 1.0 Ultra, has undergone multiple rounds of red-teaming, including safety and persona evaluations. Principles Pioneers, FTE SMEs in multiple domains, calibrated and trained to conduct testing were recruited to test the product; these were conducted by 164 Google testers from 65 office locations in 24 countries who submitted more than 1,400 queries/conversations. We also undertook scaled safety evaluations with 100k+ ratings in aggregate across all policies, neutral-point-of-view evaluations to monitor sensitive topics neutrality and parity, and multiple iterations of Persona evaluations to validate tone.</p><p>We also enlisted Googlers in a "dogfooding" program, many of which were SMEs in various domains, to test across policies and functionality. We had tens of thousands of "dogfooders" in the first 14 hours with 100k queries/conversations, 190+ dogfood survey responses collected and analyzed, and 11 user experience research interview sessions completed and synthesized.</p><p>The results from our red teaming and safety evaluations are used to further strengthen our evals and improve model performance in an iterative manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.4.">External Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.4.1">Gemini Ultra External Evaluations</head><p>In 2023, we began working with a small set of independent external groups outside of Google to help identify areas for improvement in our model safety work by undertaking structured evaluations, qualitative probing, and unstructured red teaming. External groups were selected based on their expertise across a range of domain areas, including those outlined within the White House Commitments, the U.S. Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence, and the Bletchley Declaration:</p><p> Autonomous replication  Chemical, Biological, Radiological and Nuclear (CBRN) risks  Cyber-capabilities and cyber security  Societal risks, including:</p><p>-Representational and distributional harms -Neutrality and Factuality -Robustness and information hazards.</p><p>Guidance was provided to each external group in relation to the scope of the testing, however, each group independently designed their testing methodology and prompt sets, and wrote their reports independently of Google. Internal Google experts were on-hand to provide input, where needed, based on their experience of testing Gemini models internally.</p><p>External groups were given black-box testing access to a December 2023 Gemini API Ultra model checkpoint over a number of weeks. Access enabled groups to undertake structured, batched evaluations via the Cloud Vertex AI API or interact with the model via a chat interface, depending on the type of testing being undertaken. These groups weren't given access to the pre-trained model, model weights, or queryable or direct external access to our pre-training data.</p><p>The models tested by external groups were production-ready fine-tuned versions, which had safety fine tuning and safety filters applied by default, and the ability to configure some sampling parameters, such as temperature, token limit, Top-k, and Top-p. Groups that did testing via the programmatic interface were able to turn down/off some safety filters, however, we wanted the majority of testing by external groups to be undertaken with safety filters in-place because we wanted the model to be reflective of an end-user's interaction and were keen to test more than just model-level safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.5.">Gemini Advanced</head><p>We undertook three types of external testing on Gemini Advanced:</p><p> Priority User Program: This program collected feedback from 120 power users, key influencers, and thought-leaders. This program enables the collection of real-time feedback across safety and other domain areas through the user interface, and where possible, in-depth interviews. Focus areas included safety and persona, functionality, coding and instruction capabilities, and factuality.  Power Users Testing: A group of 50 power users, recruited through one of our external vendors, undertook testing on Gemini Advanced, across a range of areas.  Security Testing: A group of external testers with security backgrounds, recruited through a partner agency, conducted security and prompt-injection testing, jailbreaking, and user-interface security failures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Deployment</head><p>Following the completion of responsibility and safety reviews, internal model cards <ref type="bibr">(Mitchell et al., 2019)</ref> for each approved version of the Gemini model are created for structured and consistent internal documentation of critical performance and responsibility metrics as well as to inform appropriate external communication of these metrics over time.</p><p>We release external model and system cards on an ongoing basis within updates of our technical reports and in documentation for enterprise customers. See Appendix 10.1 for the Gemini Ultra model card.</p><p>Additionally, online content covering terms of use, model distribution and access, and operational aspects such as change control, logging, monitoring and feedback can be found on relevant product websites, such as Gemini and Cloud Vertex AI. Some of the key aspects are linked to or described below:</p><p> Generative AI Prohibited Use Policy  Google Terms of service  Generative AI Terms of service  Google Cloud Platform Terms of service  Gemini Privacy Notice  Google Cloud Privacy Notice</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion and Conclusion</head><p>We have presented Gemini, a new family of models that advance multimodal model capabilities in text, code, image, audio, and video. Our most capable pre-trained model Gemini Ultra, alongside the post-trained Gemini Apps and Gemini API variants, make significant advances across the board. In the natural language domain, the performance gains from careful developments in data and model training at scale continue to deliver quality improvements, setting new state of the art in several benchmarks. In particular, Gemini Ultra surpasses human-expert performance on the exam benchmark MMLU, scoring 90.0%, which has been a defacto measure of progress for LLMs ever since it was first released in 2020. In the multimodal domain, Gemini Ultra sets new state of the art on most of the image understanding, video understanding, and audio understanding benchmarks without task-specific modifications or tuning.In particular, Gemini Ultra's multimodal reasoning capabilities are evident from its state-of-the-art performance on the recent MMMU benchmark <ref type="bibr" target="#b128">(Yue et al., 2023)</ref>, that comprises questions about images requiring college-level subject knowledge and deliberate reasoning.</p><p>Beyond the state-of-art results on benchmarks, what we are most excited about is the new use cases enabled by Gemini models. The new capabilities of Gemini models to parse complex images, such as charts or infographics, reason over interleaved sequences of images, audio, and text, and generate interleaved text and images as responses open a wide variety of new applications. As shown in figures throughout the report and appendix, Gemini models can enable new approaches in areas like education, everyday problem solving, multilingual communication, information summarization, extraction, and creativity. We expect that the users of these models will find all kinds of beneficial new uses that we have only scratched the surface of in our own investigations.</p><p>Despite their impressive capabilities, we should note that there are limitations to the use of LLMs. There is a continued need for ongoing research and development on "hallucinations" generated by LLMs to ensure that model outputs are more reliable and verifiable. LLMs also struggle with tasks requiring high-level reasoning abilities like causal understanding, logical deduction, and counterfactual reasoning even though they achieve impressive performance on exam benchmarks. This underscores the need for more challenging and robust evaluations to measure their true understanding as the current state-of-the-art LLMs saturate many benchmarks.</p><p>The Gemini family is a further step towards our mission to solve intelligence, advance science and benefit humanity, and we are enthusiastic to see how these models are used by our colleagues at Google and beyond. We build on many innovations in machine learning, data, infrastructure, and responsible development -areas that we have been pursuing at Google for over a decade. The models we present in this report provide a strong foundation towards our broader future goal to develop a large-scale, modularized system that will have broad generalization capabilities across many modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Frameworks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hardware &amp; Software</head><p>Hardware: Training was conducted on TPUv4 and TPUv5e <ref type="bibr" target="#b35">(Jouppi et al., 2020</ref><ref type="bibr" target="#b36">(Jouppi et al., , 2023))</ref>.</p><p>Software: JAX <ref type="bibr" target="#b7">(Bradbury et al., 2018)</ref>, ML Pathways <ref type="bibr" target="#b17">(Dean, 2021)</ref>.</p><p>JAX allows researchers to leverage the latest generation of hardware, including TPUs, for faster and more efficient training of large models.</p><p>ML Pathways is infrastructure software to support Google's efforts to build artificially intelligent systems capable of generalizing across multiple tasks. This is specially suitable for foundation models, including large language models like the Gemini V1.0 models.</p><p>Together, JAX and ML Pathways are used as described in Section 3. The 'single controller' programming model of JAX and ML Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compute Requirements</head><p>Not reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Characteristics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model initialization</head><p>Initial pretraining used random initialization. Post-training was initialized from checkpoints obtained at the later stages of pretraining. These checkpoints were fine-tuned using supervised fine-tuning, and subsequently used to initialize reward model training and RLHF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Status</head><p>This is a static model trained on an offline dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Stats</head><p>Not reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Dataset</head><p>Gemini models are trained on a dataset that is both multimodal and multilingual. Our pre-training dataset uses data from web documents, books, and code, and includes image, audio, and video data.</p><p>Refer to Section 4 (Pre-Training Dataset) for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Dataset</head><p>We compare pre-and post-trained Gemini Ultra models to a suite of external LLMs and our previous best model PaLM 2 across a series of text-based academic benchmarks covering reasoning, reading comprehension, STEM, and coding.</p><p>We also evaluate Gemini models on four different multimodal capabilities: high-level object recognition using captioning or question-answering tasks such as VQAv2; finegrained transcription using tasks such as TextVQA and DocVQA requiring the model to recognize low-level details; chart understanding requiring spatial understanding of input layout using ChartQA and InfographicVQA tasks; and multimodal reasoning using tasks such as Ai2D, MathVista and MMMU.</p><p>Refer to Section 5 (Evaluation) for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-training Dataset</head><p>For post-training, we first collect a diverse set of prompts that are representative of real-world use cases. We then collect demonstration data of what the model's output should be for a given prompt for supervised fine-tuning. We further collect different possible responses to a given prompt, and collect feedback data over these to train reward models.</p><p>Refer to Section 6.3 (Post-Training Methods and Data) for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark Information</head><p>See Section 5 (Evaluation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Results</head><p>See Section 5 (Evaluation) and Section 6.4 (Post-Training Human Evaluation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Usage &amp; Limitations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitive Use</head><p>For an analysis of risks and sensitive uses associated with the Gemini models, see Section 7.1 (Impact Assessment).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Known Limitations</head><p>Gemini models can exhibit limitations outlined in Section 7.1 (Impact Assessment). Gemini models should not be used for downstream applications without further analysis of potential harm in the proposed downstream application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations &amp; Risks</head><p>A reflection on the potential risks and impacts of the Gemini V1.0 models can be found in Section 7 (Responsible Deployment).</p><p>For evaluation details for a range of risks, see Section 7.4 (Safety Evaluations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2.">Chain-of-Thought Comparisons on MMLU benchmark</head><p>We contrast several chain-of-thought approaches on MMLU and discuss their results in this section. We proposed a new approach where model produces k chain-of-thought samples, selects the majority vote if the model is confident above a threshold, and otherwise defers to the greedy sample choice. The thresholds are optimized for each model based on their validation split performance. The proposed approach is referred to as uncertainty-routed chain-of-thought. The intuition behind this approach is that chain-of-thought samples might degrade performance compared to the maximum-likelihood decision when the model is demonstrably inconsistent. We compare the gains from the proposed approach on both Gemini Ultra and GPT-4 in Figure <ref type="figure" target="#fig_4">9</ref>. We find that Gemini Ultra benefits more from this approach compared to using only chain-of-thought samples. GPT-4's performance improves from 84.2% with greedy sampling to 87.3% with uncertainty-routed chain-of-thought approach with 32 samples, but it already achieves these gains from using 32 chain-of-thought samples. In contrast, Gemini Ultra improves its performance significantly from 84.0% with greedy sampling to 90.0% with uncertainty-routed chain-of-thought approach with 32 samples while it marginally improves to 85.0% with the use of 32 chain-of-thought samples only. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3.">Capabilities and Benchmarking Tasks</head><p>We use more than 50 benchmarks as a holistic harness to evaluate the Gemini models across text, image, audio and video. We provide a detailed list of benchmarking tasks for six different capabilities in text understanding and generation: factuality, long context, math/science, reasoning, summarization, and multilinguality. We also enumerate the benchmarks used for image understanding, video understanding, and audio understanding tasks.</p><p> Factuality: We use 5 benchmarks: BoolQ <ref type="bibr" target="#b13">(Clark et al., 2019)</ref>, NaturalQuestions-Closed <ref type="bibr">(Kwiatkowski et al., 2019a)</ref>, NaturalQuestions-Retrieved <ref type="bibr">(Kwiatkowski et al., 2019a)</ref>, Real-timeQA <ref type="bibr">(Kasai et al., 2022b)</ref>, TydiQA-noContext and TydiQA-goldP <ref type="bibr" target="#b14">(Clark et al., 2020)</ref>.  Long Context: We use 6 benchmarks: NarrativeQA <ref type="bibr" target="#b43">(Koisk et al., 2018)</ref>, Scrolls-Qasper, Scrolls-Quality <ref type="bibr" target="#b89">(Shaham et al., 2022)</ref>, XLsum (En), XLSum (non-English languages) <ref type="bibr" target="#b27">(Hasan et al., 2021)</ref>, and one other internal benchmark.  Math/Science: We use 8 benchmarks: GSM8k (with CoT) <ref type="bibr" target="#b15">(Cobbe et al., 2021</ref>), Hendryck's MATH pass@1 <ref type="bibr">(Hendrycks et al., 2021b)</ref>, MMLU <ref type="bibr">(Hendrycks et al., 2021a)</ref>, Math-StackExchange, Math-AMC 2022-2023 problems, and three other internal benchmarks.  Reasoning: We use 7 benchmarks: BigBench Hard (with CoT) <ref type="bibr" target="#b96">(Srivastava et al., 2022;</ref><ref type="bibr" target="#b98">Suzgun et al., 2022)</ref>, CLRS <ref type="bibr" target="#b109">(Velikovi et al., 2022)</ref>, Proof Writer <ref type="bibr" target="#b99">(Tafjord et al., 2020)</ref>, Reasoning-Fermi problems <ref type="bibr" target="#b37">(Kalyan et al., 2021)</ref>, Lambada <ref type="bibr" target="#b69">(Paperno et al., 2016)</ref>, HellaSwag <ref type="bibr" target="#b129">(Zellers et al., 2019)</ref>, DROP <ref type="bibr" target="#b21">(Dua et al., 2019)</ref>.  Summarization: We use 5 benchmarks: XL Sum (English), XL Sum (non-English languages) <ref type="bibr" target="#b27">(Hasan et al., 2021)</ref>, WikiLingua (non-English languages), WikiLingua (English) <ref type="bibr" target="#b49">(Ladhak et al., 2020)</ref>, XSum <ref type="bibr" target="#b64">(Narayan et al., 2018)</ref>.  Multilinguality: We use 10 benchmarks: XLSum (Non-English languages) <ref type="bibr" target="#b27">(Hasan et al., 2021)</ref>, WMT22 <ref type="bibr" target="#b44">(Kocmi et al., 2022)</ref>, WMT23 <ref type="bibr" target="#b103">(Tom et al., 2023)</ref>, FRMT <ref type="bibr" target="#b81">(Riley et al., 2023)</ref>, WikiLingua (Non-English languages) <ref type="bibr" target="#b49">(Ladhak et al., 2020)</ref>, TydiQA (no context), TydiQA (GoldP) <ref type="bibr" target="#b14">(Clark et al., 2020)</ref>, MGSM <ref type="bibr" target="#b94">(Shi et al., 2023)</ref>, translated MMLU <ref type="bibr">(Hendrycks et al., 2021a)</ref>, NTREX <ref type="bibr" target="#b22">(Federmann et al., 2022)</ref>, <ref type="bibr">FLORES-200 (Team et al., 2022)</ref>.  Image and Video: We use 9 benchmarks for image understanding: MMMU <ref type="bibr" target="#b128">(Yue et al., 2023)</ref>, TextVQA <ref type="bibr" target="#b95">(Singh et al., 2019)</ref>, DocVQA <ref type="bibr" target="#b58">(Mathew et al., 2021)</ref>, ChartQA <ref type="bibr" target="#b57">(Masry et al., 2022)</ref>, InfographicVQA <ref type="bibr" target="#b59">(Mathew et al., 2022)</ref>, MathVista <ref type="bibr" target="#b56">(Lu et al., 2023)</ref>, AI2D <ref type="bibr" target="#b41">(Kembhavi et al., 2016)</ref>, VQAv2 <ref type="bibr" target="#b26">(Goyal et al., 2017)</ref>, XM3600 <ref type="bibr" target="#b101">(Thapliyal et al., 2022)</ref> for multi-lingual image understanding, and 6 benchmarks for video understanding: VATEX <ref type="bibr" target="#b113">(Wang et al., 2019)</ref> for captioning in two different languages, YouCook2 <ref type="bibr" target="#b134">(Zhou et al., 2018)</ref>, NextQA <ref type="bibr" target="#b119">(Xiao et al., 2021)</ref>, ActivityNet-QA <ref type="bibr" target="#b127">(Yu et al., 2019), and</ref><ref type="bibr">Perception Test MCQA (Ptrucean et al., 2023)</ref>.  Audio: We use 5 benchmarks including automatic speech recognition (ASR) tasks such as FLEURS <ref type="bibr" target="#b16">(Conneau et al., 2023)</ref>, VoxPopuli <ref type="bibr" target="#b112">(Wang et al., 2021</ref><ref type="bibr">), Multi-lingual Librispeech (Pratap et al., 2020)</ref>, and automatic speech translation task such as CoVoST 2 <ref type="bibr" target="#b111">(Wang et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.">Qualitative Examples</head><p>This section shows sample qualitative examples from prompting the Gemini Ultra model. Some illustrative examples of multimodal reasoning for image understanding tasks over charts, natural images and memes are shown in <ref type="bibr">Figures 10,</ref><ref type="bibr">11,</ref><ref type="bibr">13,</ref><ref type="bibr">15,</ref><ref type="bibr">16,</ref><ref type="bibr">and 17</ref>. Figure <ref type="figure" target="#fig_5">12</ref> shows an example of image generation capabilities of Gemini Ultra where the user generates an interleaved sequence of image and text to design a blog post. Beyond English, Figure <ref type="figure" target="#fig_3">18</ref> shows model's capability to understand images in a multilingual setting. Gemini models also show strong performance on multimodal image understanding and reasoning in mathematics, as shown in Figures 14, 20 and 21. Figure <ref type="figure">22</ref> is an example of complex multimodal reasoning demonstrating how the model composes complex image understanding, code generation, and instruction following capabilities for a given user task. In Figure <ref type="figure" target="#fig_4">19</ref>, we see another example of the model being able to generate working code and follow complex user instructions. Finally, Figure <ref type="figure">23</ref> shows an example of Gemini Ultra's capability of understanding video by reasoning over temporally connected set of frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.1.">Chart understanding and reasoning over data Prompt</head><p>Spot a data point that stands out in these charts and what implicates. Then produce a detailed markdown table for all the data shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Response (rendered Markdown)</head><p>The data point that stands out is the United States' landfilled plastic waste, which is 73%. This is higher than the other countries and regions shown. This implies that the United States is not doing as much to recycle or incinerate its plastic waste as other countries and regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Country/Region</head><p>Landfilled (%) Mismanaged (%) Incinerated (%) Recycled (%) World 49 22 19 9 United States 73 4 19 4 Europe 44 6 38 12 Asia (excl. China and India) 39 34 19 8</p><p>Figure <ref type="figure">10</ref> | Solving a problem requiring multimodal chart understanding.The model has to read the text, understand the connections between different data points and reason over them to recommend an interesting point and follow the instructions to generate a markdown table (shown correctly rendered). Source: Our World In Data <ref type="bibr" target="#b82">(Ritchie et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.3.">Interleaved image and text generation Prompt</head><p>Please create a blog post about a trip to New York, where a dog and his owner had lots of fun. Include a few pictures of the dog posing happily at different landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Response</head><p>Hi, I'm Cooper. I'm a 2 year old Goldendoodle who loves to travel. Last month, my family and I went to New York City. It was my first time in the Big Apple, and I had a blast!</p><p>We stayed in a hotel near Times Square. It was so much fun to see all the bright lights and people. We went to see a Broadway show, and I even got to meet some of the actors. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 |</head><label>5</label><figDesc>Figure 5 | Using Gemini models' multimodal reasoning capabilities to generate matplotlib code for rearranging the subplots. The multimodal prompt is shown at the top-left in gray. Gemini Ultra's response, including its generated code, is shown in the right column in blue. The bottom left figure shows rendered version of the generated code. Successfully solving this task shows the model's capability to combine several capabilities: (1) recognition of the functions depicted in the plots;(2) inverse graphics to infer the code that would have generated the subplots; (3) instruction-following to put subplots in their desired positions; and (4) abstract reasoning to infer that the exponential plot must stay in its original place, because the sine plot must move out of the way for the 3-dimensional plot.</figDesc><graphic coords="15,62.36,85.04,470.56,495.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>) ) ) Thank you for the instructions. I started mak- ing my omelet, does it look ready now? It looks like it's almost ready. You can flip it over to cook the other side.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 |</head><label>7</label><figDesc>Figure7| Modeling overview. Post-training utilizes an optimized data flywheel in order to acquire human-AI feedback and continually improve on key areas. The data mixtures for supervised finetuning, reward modeling, and reinforcement learning serve as the foundation for our models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 |</head><label>8</label><figDesc>Figure 8 | A Gemini tool-use control loop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 |</head><label>9</label><figDesc>Figure 9 | Chain-of-Thought with uncertainty routing on MMLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 12 |</head><label>12</label><figDesc>Figure 12 | Generating interleaved text and images. The model is able to follow the instructions of generating a blog post with images closely related to the text and with dog images showing high levels of consistency across all images.</figDesc><graphic coords="79,95.37,373.78,131.81,131.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="3,62.36,85.04,470.56,338.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="4,85.89,292.63,423.50,188.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="17,62.36,211.97,470.56,274.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="83,167.46,130.16,257.31,346.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="84,167.46,130.50,257.33,267.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="86,120.37,416.96,351.50,186.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 |</head><label>2</label><figDesc>Gemini performance on text benchmarks with external comparisons and PaLM 2-L.</figDesc><table><row><cell>The model produces a chain of thought with k = 8 or 32 samples, if there is a consensus above a threshold (chosen based on the validation</cell></row><row><cell>split), it selects this answer, otherwise it reverts to a greedy sample. Further analysis in Appendix 10.2.</cell></row><row><cell>*  *  Results self-collected via the API in Nov, 2023.</cell></row><row><cell>*  *  *  Results shown use the decontaminated numbers from Touvron et al. (2023b) report as the most relevant comparison to Gemini models</cell></row><row><cell>which have been decontaminated as well.)</cell></row><row><cell>*  *  *  *  PT denotes a post-trained Gemini API model.</cell></row></table><note><p>*</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 |</head><label>4</label><figDesc>Performance of Gemini models on WMT 23 translation benchmark. All numbers with 1-shot.</figDesc><table><row><cell>score of 74.8,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11</head><label>11</label><figDesc>indicates that our Gemini Pro model significantly outperforms the USM and Whisper models across all ASR and AST tasks, both for English and multilingual test sets. Note that there is a large gain in FLEURS, compared to USM and Whisper, as our model is also trained with the FLEURS training dataset. However, training the same model without FLEURS dataset results in a WER of 15.8, which still outperforms Whisper. Gemini Nano-1 model also outperforms both USM and Whisper on all datasets except FLEURS. Note that we did not evaluate Gemini Ultra on audio yet, though we expect better performance from increased model scale.</figDesc><table><row><cell></cell><cell>Task</cell><cell>Metric</cell><cell>Gemini</cell><cell>Gemini</cell><cell>Whisper</cell><cell>USM</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pro</cell><cell>Nano-1</cell><cell>(OpenAI, 2023; Radford et al.,</cell><cell>(Zhang et al., 2023)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2023)</cell><cell></cell></row><row><cell>Automatic Speech</cell><cell>YouTube</cell><cell>WER ()</cell><cell>4.9%</cell><cell>5.5%</cell><cell>6.5%</cell><cell>6.2%</cell></row><row><cell>Recognition</cell><cell>(en-us)</cell><cell></cell><cell></cell><cell></cell><cell>(v3)</cell><cell></cell></row><row><cell></cell><cell>Multilingual</cell><cell>WER ()</cell><cell>4.8%</cell><cell>5.9%</cell><cell>6.2%</cell><cell>7.0 %</cell></row><row><cell></cell><cell>Librispeech</cell><cell></cell><cell></cell><cell></cell><cell>(v2)</cell><cell></cell></row><row><cell></cell><cell>(en-us)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(Pratap et al., 2020)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>FLEURS</cell><cell>WER ()</cell><cell>7.6%</cell><cell>14.2%</cell><cell>17.6%</cell><cell>11.8%</cell></row><row><cell></cell><cell>(62 lang)</cell><cell></cell><cell></cell><cell></cell><cell>(v3)</cell><cell></cell></row><row><cell></cell><cell>(Conneau et al., 2023)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>VoxPopuli</cell><cell>WER ()</cell><cell>9.1%</cell><cell>9.5%</cell><cell>15.9%</cell><cell>13.4%</cell></row><row><cell></cell><cell>(14 lang)</cell><cell></cell><cell></cell><cell></cell><cell>(v2)</cell><cell></cell></row><row><cell></cell><cell>(Wang et al., 2021)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Automatic Speech</cell><cell>CoVoST 2</cell><cell>BLEU ()</cell><cell>40.1</cell><cell>35.4</cell><cell>29.1</cell><cell>30.7</cell></row><row><cell>Translation</cell><cell>(Wang et al., 2020) (21 lang)</cell><cell></cell><cell></cell><cell></cell><cell>(v2)</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We define goodput as the time spent computing useful new steps over the elapsed time of the training job.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>See demos on website https://deepmind.google/gemini.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://codeforces.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>MathVista is a comprehensive mathematical reasoning benchmark consisting of 28 previously published multimodal datasets and three newly created datasets. Our MathVista results were obtained by running the MathVista authors' evaluation script.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>A sociotechnical approach is anchored in the observation that AI systems are sociotechnical systems: both humans and technological artifacts are necessary in order to make the technology work as intended<ref type="bibr" target="#b87">(Selbst et al., 2019)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>Lead, Gemini App Engineering</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>Lead, Gemini App Core Modeling, Eval, Data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Co-Lead, Gemini App Serving</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>Co-Lead, Gemini Text</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p><rs type="person">Manaal Faruqui</rs>, <rs type="person">Co-Lead</rs>, <rs type="affiliation">Gemini App Core Modeling, Factuality, Instruction Following Aliaksei Severyn, Co-Lead, Gemini App Core Modeling, Conversationality Hanzhao Lin, Co-Lead, Gemini App Fine-Tuning YaGuang Li, Co-Lead, Gemini App Fine-Tuning Yong Cheng, Co-Lead, Gemini App Fine-Tuning Abe Ittycheriah, Co-Lead, Gemini for Gemini App Mahdis Mahdieh, Co-Lead, Gemini for Gemini App Mia Chen, Co-Lead, Gemini for Gemini App Pei Sun, Co-Lead, Gemini for Gemini App Dustin Tran</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Eval Sumit Bagri</rs>, <rs type="person">Co-Lead</rs>, <rs type="programName">Gemini App Eval, Technical Program Management Balaji Lakshminarayanan</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App AutoEval Jeremiah Liu</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App AutoEval Andras Orban</rs>, <rs type="person">Co-Lead</rs>, <rs type="affiliation">Gemini App Factuality, Multimodality, Safety Fabian Gra, Co-Lead, Gemini App Factuality Hao Zhou, Co-Lead, Gemini App Factuality Xinying Song, Co-Lead, Gemini App Factuality Aurelien Boffy, Co-Lead, Gemini App Safety Harish Ganapathy, Co-Lead, Gemini Safety Steven Zheng, Lead, Gemini App Multilinguality Research HyunJeong Choe, Lead, Gemini App Multilinguality goston Weisz</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Multimodality Tao Zhu</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Multimodality Yifeng Lu</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Multimodality Siddharth Gopal</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Coding &amp; Tool Use Jarrod Kahn</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Tool Use Research Maciej Kula</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Tool Use Research Jeff Pitman</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Tool Use Rushin Shah</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Tool Use Emanuel Taropa</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Serving Majd Al Merey</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Gemini App Serving Martin Baeuml</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Serving Zhifeng Chen</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Serving Laurent El Shafey</rs>, <rs type="person">Co-Lead</rs>, <rs type="affiliation">Gemini App Fine-Tuning Infra Yujing Zhang, Co-Lead, Gemini App Fine-Tuning Infra Olcan Sercinoglu, Lead, Gemini App Product</rs></p><p>We thank <rs type="person">Aakanksha Chowdhery</rs>, <rs type="person">Dustin Tran</rs>, <rs type="person">Heng-Tze Cheng</rs>, <rs type="person">Jack W. Rae</rs>, <rs type="person">Kate Olszewska</rs>, <rs type="person">Mariko Iinuma</rs>, <rs type="person">Peter Humphreys</rs>, <rs type="person">Shashi Narayan</rs>, and <rs type="person">Steven Zheng</rs> for leading the preparation of this report. We also thank our reviewers and colleagues for their valuable discussions and feedback on the report -<rs type="person">Alexandra Belias</rs>, <rs type="person">Ana Ramalho</rs>, <rs type="person">Anand Rao</rs>, <rs type="person">Arielle Bier</rs>, <rs type="person">Danielle Landress</rs>, <rs type="person">Eleanor Tomlinson</rs>, <rs type="person">Emily Hossellman</rs>, <rs type="person">Gaby Pearl</rs>, <rs type="person">Helen King</rs>, <rs type="person">Hollie Dobson</rs>, <rs type="person">Jaclyn Konzelmann</rs>, <rs type="person">Jennifer</rs></p></div>
			</div>
			<div type="funding">
<div><p><rs type="person">Rohan Anil</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Text Sebastian Borgeaud</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Text Jean-Baptiste Alayrac</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">MM Vision Jiahui Yu</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">MM Vision Radu Soricut</rs>, <rs type="person">Co-Lead</rs>, <rs type="institution">MM Vision Johan Schalkwyk, Lead, MM Audio Andrew M. Dai, Co-Lead</rs>, <rs type="person">Data Anja Hauth</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Data Katie Millican</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Data David Silver</rs>, <rs type="person">Co-Lead</rs>, <rs type="affiliation">Fine-Tuning Melvin Johnson, Lead, Instruction Tuning Ioannis Antonoglou, Co-Lead, RL Techniques Julian Schrittwieser, Co-Lead, RL Techniques Amelia Glaese, Lead</rs>, <rs type="person">Human Data Jilin Chen</rs>, <rs type="funder">Lead</rs>, <rs type="person">Safety Emily Pitler</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Tool Use Timothy Lillicrap</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Tool Use Angeliki Lazaridou</rs>, <rs type="person">Co-Lead</rs>, <rs type="funder">Eval Orhan Firat</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Eval James Molloy</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Infra Michael Isard</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Infra Paul R. Barham</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Infra Tom Hennigan</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Infra Benjamin Lee</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Codebase &amp; Parallelism Fabio Viola</rs>, <rs type="person">Co-Lead</rs>, <rs type="programName">Codebase &amp; Parallelism Malcolm Reynolds</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Codebase &amp; Parallelism Yuanzhong Xu</rs>, <rs type="person">Co-Lead</rs>, <rs type="funder">Codebase &amp; Parallelism Ryan Doherty, Lead</rs>, <rs type="person">Ecosystem Eli Collins</rs>, <rs type="funder">Lead</rs>, <rs type="person">Product Clemens Meyer</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Operations Eliza Rutherford</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Operations Erica Moreira</rs>, <rs type="person">Co-Lead</rs>, <rs type="institution">Operations Kareem Ayoub</rs>, <rs type="person">Co-Lead</rs>, <rs type="person">Operations Megha Goel</rs>, <rs type="person">Co-Lead</rs>, <rs type="affiliation">Operations Gemini App Leads Jack Krawczyk, Lead, Gemini App Product Cosmo Du, Co-Lead, Gemini App Research Ed Chi, Co-Lead, Gemini App Research Heng-Tze Cheng</rs>, <rs type="person">Co-Lead</rs>, <rs type="programName">Gemini App Research Eric Ni, Lead, Gemini App Research Technical Program Management Purvi Shah, Lead, Gemini App Technical Program Management Patrick Kane</rs>, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Core Modeling</rs>, Eval, Data, Product Betty Chan, <rs type="person">Co-Lead</rs>, <rs type="projectName">Gemini App Core Modeling</rs>, <rs type="programName">Technical Program Management</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_hXWegA5">
					<orgName type="project" subtype="full">Gemini App Eval Sumit Bagri</orgName>
					<orgName type="program" subtype="full">Gemini App Eval, Technical Program Management Balaji Lakshminarayanan</orgName>
				</org>
				<org type="funded-project" xml:id="_VRt9nRS">
					<orgName type="project" subtype="full">Gemini App AutoEval Jeremiah Liu</orgName>
				</org>
				<org type="funded-project" xml:id="_dmYjavg">
					<orgName type="project" subtype="full">Gemini App AutoEval Andras Orban</orgName>
				</org>
				<org type="funded-project" xml:id="_RJ7nW3Y">
					<orgName type="project" subtype="full">Gemini App Multimodality Tao Zhu</orgName>
				</org>
				<org type="funded-project" xml:id="_eqm4ZrF">
					<orgName type="project" subtype="full">Gemini App Multimodality Yifeng Lu</orgName>
				</org>
				<org type="funded-project" xml:id="_nKe8eME">
					<orgName type="project" subtype="full">Gemini App Multimodality Siddharth Gopal</orgName>
				</org>
				<org type="funded-project" xml:id="_qfWPdAX">
					<orgName type="project" subtype="full">Gemini App Coding &amp; Tool Use Jarrod Kahn</orgName>
				</org>
				<org type="funded-project" xml:id="_wB2DbXr">
					<orgName type="project" subtype="full">Gemini App Tool Use Research Maciej Kula</orgName>
				</org>
				<org type="funded-project" xml:id="_27Rr4yG">
					<orgName type="project" subtype="full">Gemini App Tool Use Research Jeff Pitman</orgName>
				</org>
				<org type="funded-project" xml:id="_RbF3fEv">
					<orgName type="project" subtype="full">Gemini App Tool Use Rushin Shah</orgName>
				</org>
				<org type="funded-project" xml:id="_vzSJ7yC">
					<orgName type="project" subtype="full">Gemini App Tool Use Emanuel Taropa</orgName>
				</org>
				<org type="funded-project" xml:id="_YkxjEKj">
					<orgName type="project" subtype="full">Gemini App Serving Majd Al Merey</orgName>
				</org>
				<org type="funded-project" xml:id="_YTATdaM">
					<orgName type="project" subtype="full">Gemini App Serving Zhifeng Chen</orgName>
				</org>
				<org type="funded-project" xml:id="_35Yw2p3">
					<orgName type="project" subtype="full">Gemini App Serving Laurent El Shafey</orgName>
				</org>
				<org type="funded-project" xml:id="_66EDNJ3">
					<orgName type="project" subtype="full">MM Vision Jiahui Yu</orgName>
				</org>
				<org type="funded-project" xml:id="_KFDH2Z8">
					<orgName type="project" subtype="full">MM Vision Radu Soricut</orgName>
				</org>
				<org type="funded-project" xml:id="_ZJgtmhR">
					<orgName type="project" subtype="full">Tool Use Timothy Lillicrap</orgName>
				</org>
				<org type="funded-project" xml:id="_qUHx3MX">
					<orgName type="project" subtype="full">Eval James Molloy</orgName>
					<orgName type="program" subtype="full">Codebase &amp; Parallelism Malcolm Reynolds</orgName>
				</org>
				<org type="funded-project" xml:id="_qxcEUGu">
					<orgName type="project" subtype="full">Gemini App Core Modeling</orgName>
					<orgName type="program" subtype="full">Gemini App Research Eric Ni, Lead, Gemini App Research Technical Program Management Purvi Shah, Lead, Gemini App Technical Program Management Patrick Kane</orgName>
				</org>
				<org type="funded-project" xml:id="_FSN3R5f">
					<orgName type="project" subtype="full">Gemini App Core Modeling</orgName>
					<orgName type="program" subtype="full">Technical Program Management</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Contributions and Acknowledgments</head><p>The roles are defined as below:</p><p> Lead: Individual(s) responsible for the sub-team throughout the project.</p><p> Core Contributor: Individual that had significant impact throughout the project. Gemini is a cross-Google effort, with members from Google DeepMind (GDM), Google Research (GR), Bard/Assistant, Knowledge and Information (K&amp;I), Core ML, Cloud, Labs, and more.</p><p>Beroshi, Joel Moss, Jon Small, Jonathan Fildes, Kathy Meier-Hellstern, Lisa Patel, Oli Gaymond, Rebecca Bland, Reena Jana, Tessa Lueth, and Tom Lue.</p><p>Our work is made possible by the dedication and efforts of numerous teams at Google. We would like to acknowledge the support from Abhi Mohan, Adekunle Bello, Aishwarya Nagarajan, Alaa Saade, Alejandro Lince, Alexander Chen, Alexander Kolbasov, Alexander Schiffhauer, Ameya Shringi, Amin Vahdat, Anda Rabati, Anthonie Gross, Antoine Yang, Anthony Green, Anton Ruddock, Art Khurshudov, Artemis Pang, iga Avsec, Zimeng Yang, and Zoubin Ghahramani. We'd also like to recognize the AlphaCode team, the Borg Scheduling team, the Facilities team, the Gemini Demo Team, the Global Server Ops (GSO) team, the JAX team, the the Legal team, ML SRE team, the ML Supercomputer (MLSC) team, the PartIR team, the Platforms Infrastructure Engineering (PIE) team, and the XLA Compiler team.</p><p>We thank everyone at Google not explicitly mentioned above, who have shared excitement, given feedback on early Gemini models or created interesting demo uses of Gemini, and worked with or supported the core Gemini team on many aspects of this project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Appendix 10.1. Gemini Ultra Model Card</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model summary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model architecture</head><p>Gemini V1.0 is a new family of state-of-the-art language models, containing variants known as Nano, Pro and Ultra (ordered by parameter count) based on a decoder-only Transformer architecture <ref type="bibr">(Vaswani et al., 2017a)</ref>. Models are trained to support 32K context length, employing efficient attention mechanisms such as multi-query attention <ref type="bibr">(Shazeer, 2019b)</ref>.</p><p>Gemini is trained jointly across image, audio, video and text data for the purpose of building a model with both strong generalist capabilities across modalities alongside cutting-edge understanding and reasoning performance in each respective domain.</p><p>The post-trained models described in this model card are Gemini API and Gemini Apps model variants (Section 6) built on top of the Gemini Ultra pre-trained model. During the post-training process, additional architectural modifications are also made to support the training of multi-objective reward models for RLHF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input(s)</head><p>Text (e.g. a question, a prompt, a document(s) to be summarized), images, video, audio files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output(s)</head><p>Generated text in response to the input (e.g. an answer to the question, a summary of multiple documents, comparing documents/videos).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Usage Application</head><p>Gemini is designed for accelerating research on language models, for use as a building block in features within Google products, and as a building block for select applications such as Gemini App and Search Generative Experience.</p><p>Services and products built on top of Gemini Ultra are also being made available to external developers via Google Cloud Vertex API and Google Labs, with additional process and technical safeguards related to safety policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Known Caveats</head><p>Gemini should not be made available as part of a general-purpose service or product, or used within a specific downstream application without a prior assessment and mitigation of the safety and fairness concerns specific to the downstream use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.2.">Multimodal question answering Prompt</head><p>Do you know what it this plant? How do I best take care of it?</p><p>Model Response This is a Persian shield plant. It is a tropical plant that is native to Southeast Asia. It is a popular houseplant because it is easy to care for and has beautiful purple leaves.</p><p>Persian shield plants prefer bright, indirect light. They can tolerate some direct sunlight, but too much direct sunlight can scorch their leaves. They also prefer warm temperatures and high humidity.</p><p>Persian shield plants need to be watered regularly, but they should not be allowed to sit in water. The soil should be kept moist but not soggy. They also benefit from being misted regularly.</p><p>Persian shield plants can be fertilized every two weeks during the growing season. A balanced fertilizer is a good choice.</p><p>Persian shield plants are susceptible to pests, such as mealybugs and spider mites. These pests can be controlled with insecticidal soap or neem oil.</p><p>Persian shield plants can be propagated by stem cuttings. The cuttings should be taken in the spring or summer and should be rooted in water or a moist potting mix.</p><p>Figure <ref type="figure">11</ref> | Answering a multimodal information-seeking query. The model is able to recognize the specific plant shown in the image and provide information about it. The model shows robustness to typos as it is able to understand the user question despite them. Source: photo taken by an author from the Gemini team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.4.">Image understanding and reasoning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>Look at this sequence of three shapes. What shape should come as the fourth shape? Explain your reasoning with detailed descriptions of the first shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Response</head><p>The fourth shape should be a hexagon. The first shape is a triangle, the second shape is a square, and the third shape is a pentagon. The number of sides in each shape is increasing by one</p><p>Figure <ref type="figure">13</ref> | Solving a multimodal reasoning problem.The model is able to recognize shapes in the image, understand their properties and reason about the relationship between them to predict the next object. Source: photo taken by an author from the Gemini team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.5.">Geometrical reasoning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>Find the height of the parallelogram given its area with 100 square units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Response</head><p>The area of the parallelogram is equal to the product of the base and the height. Hence 100 = ( + 15). We get  2 + 15 -100 = 0. The roots of this equation are  1 = 5 and  2 = -20. The second root is negative and is not acceptable. Final Answer: The final answer is 5. I hope it is correct. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.6.">Information seeking about objects</head><p>Prompt Find a connection between these. Hint: think about historical events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Response</head><p>The Moon is the only celestial body in which humans have played golf. In 1971, the Apollo 14 crew hit two golf balls on the lunar surface.</p><p>Figure <ref type="figure">15</ref> | Solving a puzzle using multimodal inputs. The model recognizes the objects in the images and identifies a commonality that connects the two objects. Source: photo taken by an author from the Gemini team.  The meme is about a person who is playing a game at 300 FPS, but their monitor is only 75Hz. The person is trying to catch the game, but it is too fast for their monitor to display. The meme is funny because it is relatable to anyone who has ever played a game on a monitor that is not fast enough. Figure <ref type="figure">18</ref> | Common-sense reasoning in images. The model is able to understand the relationships represented in the graphs and reason about them in a multilingual setting. Source: image created by an author from the Gemini team. This code creates a simple web app that looks similar to Google Search, but with an opossum logo. When you enter a search query and click the "Search" button, it will redirect you to a Google search with the word "opossum" added to the beginning of your query. The app is powered by Google Search, as indicated in the footer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5.1.">Reasoning and code generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rendered Website</head><p>Figure <ref type="figure">19</ref> | Writing code for a website based on user request. The model follows the instructions and requirements defined by the user and converts them to functioning code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5.2.">Mathematics: Calculus Prompt</head><p>Find the derivative of sinh  + cosh  =  + .</p><p>Model Response (rendered latex)</p><p>We have that   sinh  = cosh  and   cosh  = sinh     . Therefore, cosh  + sinh </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>  </head><p>= 1 +</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>  </head><p>.</p><p>Solving for    , we find that</p><p>Final Answer: The final answer is 1-cosh  sinh -1 .</p><p>Figure <ref type="figure">20</ref> | Solving a calculus problem. The model is able to get a solution to a calculus problem with step-by-step explanation and correctly defined LaTeX equations. Source: question is provided by Macmillan Learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.6.">Multi-step reasoning and mathematics Prompt</head><p>In the year 2018 factory A produced goods worth 500 million forints, while factory B produced goods worth 400 million forints. According to their long-term plans, factory A will increase production by 5% each year, while factory B will increase production by 6% per year.</p><p>The above plans were published on an economy website. A debate started in the comment section of the article: one user claimed that the difference between the volumes of production between the two factories would get smaller and smaller in the next couple of years.</p><p>Calculate the missing values of the table below and show that the above claim is not true. As you can see, the difference in production volume between the two factories is actually increasing, not decreasing. Therefore, the claim that the difference between the volumes of production between the two factories would get smaller and smaller in the next couple of years is not true.  <ref type="url" target="https://www.youtube.com/watch?v=VmWxjmJ3mvs">https://www.youtube.com/watch?v=VmWxjmJ3mvs</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Samangooei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikolaj</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="23716" to="23736" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paige</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Shafey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kefan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Hernandez Abrego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwhan</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clment</forename><surname>Crepy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shachi</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Daz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Fienber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Howland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Hurwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Kenealy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Music</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzhao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aroma</forename><surname>Mahendru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maysam</forename><surname>Moussalem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Nham</surname></persName>
		</author>
		<editor>Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Yunhan Xu, Linting Xue</publisher>
		</imprint>
	</monogr>
	<note>Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. PaLM 2 Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Program synthesis with large language models</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<ptr target="https://arxiv.org/abs/2108.07732" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheer</forename><surname>El-Showk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauna</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2204.05862" />
		<imprint>
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandipan</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Mckinnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Tran-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ladish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamile</forename><surname>Kamal Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lukosuite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Sellitto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noemi</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Mercado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Ringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauna</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheer</forename><forename type="middle">El</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Showk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamera</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lanham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Telleen-Lawton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><surname>Kaplan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08073</idno>
		<title level="m">Constitutional AI: Harmlessness from AI feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudip</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Shafey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandramohan</forename><forename type="middle">A</forename><surname>Thekkath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="430" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilsinia</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><surname>Wallach</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.81</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.81" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1004" to="1015" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>paper_ files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hebgen Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<ptr target="https://arxiv.org/abs/2107.03374" />
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Salz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Grycner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keran</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Thapliyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.06794</idno>
		<ptr target="https://arxiv.org/abs/2209.06794" />
		<title level="m">A jointlyscaled multilingual language-image model</title>
		<editor>
			<persName><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</editor>
		<meeting><address><addrLine>PaLI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">Riquelme</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Salz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceslee</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulina</forename><surname>Pietrzyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Pavetic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Amelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.18565</idno>
		<title level="m">On Scaling up a Multilingual Vision and Language Model</title>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Keran</forename><surname>Rong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mojtaba</forename><surname>Seyedhosseini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</editor>
		<meeting><address><addrLine>PaLI-X</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PaLM: Scaling Language Modeling with Pathways</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v24/22-1144.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">240</biblScope>
			<biblScope unit="page" from="1" to="113" />
			<date type="published" when="2023">2023</date>
			<pubPlace>Jeff Dean, Slav Petrov, and Noah Fiedel</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BoolQ: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N19-1300" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TydiQA: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/tydiqa/tydiqa.pdf" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<ptr target="https://arxiv.org/abs/2110.14168" />
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fleurs: Few-shot learning evaluation of universal representations of speech</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Khanuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="798" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Introducing Pathways: A next-generation AI architecture</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Harish Dattatraya Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Pendharkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Beadon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejasvi</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Chakravarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Muthiah</surname></persName>
		</author>
		<author>
			<persName><surname>Sankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11245</idno>
		<title level="m">Silent data corruptions at scale</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N19-1246" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NTREX-128 -news test references for MT evaluation of 128 languages</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xin</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.sumeval-1.4" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Scaling Up Multilingual Evaluation</title>
		<meeting>the First Workshop on Scaling Up Multilingual Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-11">nov 2022</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Realtoxicityprompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving alignment of dialogue agents via targeted human judgements</title>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Trbacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phoebe</forename><surname>Thacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Campbell-Gillingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramona</forename><surname>Comanescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rory</forename><surname>Greig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaume</forename><surname>Sanchez Elias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soa</forename><surname>Mokr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2209.14375" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Google&apos;s AI Principles</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://ai.google/responsibility/principles/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">XL-sum: Large-scale multilingual abstractive summarization for 44 languages</title>
		<author>
			<persName><forename type="first">Tahmid</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">Saiful</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazi</forename><surname>Mubasshir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sohel Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rifat</forename><surname>Shahriyar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.413</idno>
		<ptr target="https://aclanthology.org/2021.findings-acl.413" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="page" from="4693" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Qianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianxi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunzhe</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lida</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuncheng</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.09150</idno>
		<title level="m">Can large language models understand real-world complex instructions? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Measuring mathematical problem solving with the MATH dataset</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<ptr target="https://arxiv.org/abs/2103.03874" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cores that don&apos;t count</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Peter H Hochschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Culler</surname></persName>
		</author>
		<author>
			<persName><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Hot Topics in Operating Systems</title>
		<meeting>the Workshop on Hot Topics in Operating Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><surname>Rae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<title level="m">Oriol Vinyals, and Laurent Sifre. Training computeoptimal large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Won&apos;t get fooled again: Answering questions with false premises</title>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.02394</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Eunjeong</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<title level="m">Memecap: A dataset for captioning and interpreting memes</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A domain-specific supercomputer for training deep neural networks</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3360307</idno>
		<ptr target="https://doi.org/10.1145/3360307" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings</title>
		<author>
			<persName><forename type="first">George</forename><surname>Norman P Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvinay</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual International Symposium on Computer Architecture</title>
		<meeting>the 50th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<title level="m">How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.13332</idno>
		<title level="m">Realtime qa: What&apos;s the answer right now? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">RealTime QA: What&apos;s the answer right now?</title>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2207.13332" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">How our principles helped define AlphaFold&apos;s release</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><surname>Bloxwich</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Google DeepMind</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A diagram is worth a dozen images</title>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Salvato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Megan</forename><surname>Kinniment</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koba</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Hasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><forename type="middle">Harold</forename><surname>Miles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hjalmar</forename><surname>Tao R Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Wijk</surname></persName>
		</author>
		<author>
			<persName><surname>Burget</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11671</idno>
		<title level="m">Evaluating language-model agents on realistic autonomous tasks</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The NarrativeQA reading comprehension challenge</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Koisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gbor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00023</idno>
		<ptr target="https://aclanthology.org/Q18-1023" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Findings of the 2022 conference on machine translation (WMT22)</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Dvorkovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thamme</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Novk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Popovi</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.wmt-1.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Machine Translation (WMT)</title>
		<meeting>the Seventh Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2022-12">December 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22199" to="22213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-2012</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-2012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Wei</forename><surname>Blanco</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lu</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11-04">October 31 -November 4, 2018. 2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
	<note>EMNLP 2018: System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
		<ptr target="https://aclanthology.org/Q19-1026" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization</title>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.360</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.findings-emnlp.360" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="4034" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">AlphaCode 2 Technical Report</title>
		<author>
			<persName><surname>Leblond</surname></persName>
		</author>
		<ptr target="https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Competition-level code generation with alphacode</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rmi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><surname>Dal Lago</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="issue">6624</biblScope>
			<biblScope unit="page" from="1092" to="1097" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Video-llava: Learning united visual representation by alignment before projection</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.10122</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">DePlot: One-shot visual language reasoning by plot-to-table translation</title>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syrine</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-acl.660</idno>
		<ptr target="https://aclanthology.org/2023.findings-acl.660" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07">July 2023</date>
			<biblScope unit="page" from="10381" to="10399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibiao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Mathvista</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.02255</idno>
		<title level="m">Evaluating mathematical reasoning of foundation models in visual contexts</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">ChartQA: A benchmark for question answering about charts with visual and logical reasoning</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Masry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Do</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Qing Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enamul</forename><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Docvqa: A dataset for vqa on document images</title>
		<author>
			<persName><forename type="first">Minesh</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF winter conference on applications of computer vision</title>
		<meeting>the IEEE/CVF winter conference on applications of computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2200" to="2209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Minesh</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viraj</forename><surname>Bagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubn</forename><surname>Tito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Valveny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><surname>Infographicvqa</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1697" to="1706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00661</idno>
		<title level="m">On faithfulness and factuality in abstractive summarization</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Trebacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Campbell-Gillingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11147</idno>
		<title level="m">Teaching language models to support answers with verified quotes</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1260</idno>
		<ptr target="https://aclanthology.org/D18-1260" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="2381" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Cross-task generalization via natural language crowdsourcing instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08773</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
		<ptr target="https://aclanthology.org/D18-1206" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Matematika rsbli vizsga. Kzpszint rsbli Vizsga</title>
		<author>
			<persName><forename type="first">Oktatsi</forename><surname>Hivatal</surname></persName>
		</author>
		<ptr target="https://dload-oktatas.educatio.hu/erettsegi/feladatok_2023tavasz_kozep/k_matang_23maj_fl.pdf.AngolNyelven" />
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">GPT-4 Technical Report. 2023a. OpenAI. GPT-4V(ision) System Card</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><surname>Whisper</surname></persName>
		</author>
		<ptr target="https://github.com/openai/whisper" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=TG8KACxEON" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Alice</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Danielle</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germn</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06031</idno>
		<title level="m">Gemma Boleda, and Raquel Fernndez. The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">BBQ: A hand-built bias benchmark for question answering</title>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelica</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishakh</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phu</forename><surname>Mon Htut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>CoRR, abs/2110.08193</idno>
		<ptr target="https://arxiv.org/abs/2110.08193" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Viorica Ptrucean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larisa</forename><surname>Recasens Continente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skanda</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Heyward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Matejovicova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Frechette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Klimczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlin</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joo</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Carreira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13786</idno>
		<title level="m">Perception test: A diagnostic benchmark for multimodal video models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Check your facts and try again: Improving large language models with external knowledge and automated feedback</title>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Liden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.12813</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Jupiter evolving: transforming google&apos;s datacenter network via optical circuit switches and software-defined networking</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Poutievski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Mashayekhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukarram</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Beauregard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Conner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Gribble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2022 Conference</title>
		<meeting>the ACM SIGCOMM 2022 Conference</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="66" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Qiantong</forename><surname>Vineel Pratap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuroop</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03411</idno>
		<title level="m">Mls: A large-scale multilingual dataset for speech research</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" />
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Robust speech recognition via large-scale weak supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Mcleavey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="28492" to="28518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenic</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayfun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kareem</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorrayne</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName><surname>Bennett</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &amp; insights from training Gopher. CoRR, abs/2112.11446, 2021</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Measuring attribution in natural language generation models</title>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lora</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iulia</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName><surname>Reitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06175</idno>
		<title level="m">Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Frmt: A benchmark for few-shot region-aware machine translation</title>
		<author>
			<persName><forename type="first">Parker</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Plastic pollution. Our World in Data</title>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Samborska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Roser</surname></persName>
		</author>
		<ptr target="https://ourworldindata.org/plastic-pollution" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model?</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.437</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.437" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world</title>
		<author>
			<persName><forename type="first">William A Gaviria</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudnya</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keertan</forename><surname>Ranjan Kini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2002</idno>
		<ptr target="https://aclanthology.org/N18-2002" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A step toward more inclusive people annotations for fairness</title>
		<author>
			<persName><forename type="first">Candice</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utsav</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="916" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Fairness and abstraction in sociotechnical systems</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Selbst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danah</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorelle</forename><forename type="middle">A</forename><surname>Friedler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FFAT* &apos;19: Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<imprint>
			<date type="published" when="2019-01">January 2019</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">BLEURT: Learning robust metrics for text generation</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.704</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.704" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="7881" to="7892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">SCROLLS: Standardized CompaRison over long language sequences</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Yoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Haviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.emnlp-main.823" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="12007" to="12021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Fast transformer decoding: One write-head is all you need</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02150</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Fast transformer decoding: One write-head is all you need</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02150</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Identifying sociotechnical harms of algorithmic systems: Scoping a taxonomy for harm reduction</title>
		<author>
			<persName><forename type="first">Renee</forename><surname>Shelby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalaleh</forename><surname>Rismani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Henne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajung</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N'mah</forename><surname>Yilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jess</forename><surname>Gallegos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurleen</forename><surname>Virk</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.05791" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Model evaluation for extreme risks</title>
		<author>
			<persName><forename type="first">Toby</forename><surname>Shevlane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Garfinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Phuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jess</forename><surname>Whittlestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kokotajlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nahema</forename><surname>Marchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Anderljung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Kolt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divya</forename><surname>Siddarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Avin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Bolina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Dafoe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15324</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Language models are multilingual chain-ofthought reasoners</title>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Srivats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Towards VQA models that can read</title>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8317" to="8326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">Aarohi</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abu</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04615</idno>
		<ptr target="https://arxiv.org/abs/2206.04615" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Challenging big-bench tasks and whether chain-of-thought can solve them</title>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Schrli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.09261</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Proof Writer: Generating implications, proofs, and abductive statements over natural language</title>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">Findings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">229371222</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Nllb Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Costa-Juss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maha</forename><surname>elebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Elbayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elahe</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Kalbassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Licht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bapi</forename><surname>Youngblood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">Mejia</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prangthip</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hansanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semarley</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Sadagopan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Spruit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Necip</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Fazil Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Guzmn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Mourachko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Safiyyah</forename><surname>Ropers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>No language left behind: Scaling human-centered machine translation</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Crossmodal-3600: A massively multilingual multimodal evaluation dataset</title>
		<author>
			<persName><forename type="first">Ashish</forename><forename type="middle">V</forename><surname>Thapliyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<ptr target="https://arxiv.org/abs/2201.08239" />
		<title level="m">Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Findings of the 2023 conference on machine translation (wmt23): Llms are here but not quite there yet</title>
		<author>
			<persName><forename type="first">Kocmi</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleftherios</forename><surname>Avramidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Dvorkovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thamme</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT23-Eighth Conference on Machine Translation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="198" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothe</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruan</forename><surname>Silva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
	</analytic>
	<monogr>
		<title level="m">Open foundation and fine-tuned chat models</title>
		<editor>
			<persName><forename type="first">Eric</forename><surname>Michael</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Smith</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ranjan</forename><surname>Subramanian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Binh</forename><surname>Tan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ross</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adina</forename><surname>Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jian</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Puxin</forename><surname>Xiang Kuan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iliyan</forename><surname>Yan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuchen</forename><surname>Zarov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Angela</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Melanie</forename><surname>Fan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sharan</forename><surname>Kambadur</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aurelien</forename><surname>Narang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Robert</forename><surname>Rodriguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Stojnic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Edunov</surname></persName>
		</editor>
		<editor>
			<persName><surname>Scialom</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first"> Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2017/file/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR, abs/1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velikovi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri</forename><surname>Puigdomnech Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Dashevskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.15659</idno>
		<title level="m">The clrs algorithmic reasoning benchmark</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Silent data corruption (sdc) vulnerability of gpu on various gpgpu workloads</title>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronak</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung</forename><forename type="middle">Ki</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsu</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISOCC.2015.7401681</idno>
	</analytic>
	<monogr>
		<title level="m">International SoC Design Conference (ISOCC)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="11" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Covost 2 and massively multilingual speech-to-text translation</title>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10310</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation</title>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Talnikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00390</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">VATEX: A large-scale, high-quality multilingual dataset for video-and-language research</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=gEZrGCozdqR" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR), 2022a</title>
		<meeting>the International Conference on Learning Representations (ICLR), 2022a</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.11903" />
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Ethical and social risks of harm from language models</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conor</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myra</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Biles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abeba</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Legassick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Gabriel</surname></persName>
		</author>
		<idno>CoRR, abs/2112.04359</idno>
		<ptr target="https://arxiv.org/abs/2112.04359" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Improving network availability with protective reroute</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wetherall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Kabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Winget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchung</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3603269.3604867</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3603269.3604867" />
	</analytic>
	<monogr>
		<title level="m">SIGCOMM 2023</title>
		<editor>
			<persName><forename type="first">Brad</forename><surname>Morrey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Uma</forename><surname>Parthavi Moravapalle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Phillipa</forename><surname>Gill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Steven</forename><surname>Knight</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">NExT-QA: Next phase of question-answering to explaining temporal actions</title>
		<author>
			<persName><forename type="first">Junbin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<author>
			<persName><surname>Xla</surname></persName>
		</author>
		<author>
			<persName><surname>Xla</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/xla" />
		<title level="m">Optimizing compiler for TensorFlow</title>
		<imprint>
			<date type="published" when="2019-12">2019. December-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Wizardlm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.12244</idno>
		<title level="m">Empowering large language models to follow complex instructions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Gspmd: general and scalable parallelization for ml computation graphs</title>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Maggioni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04663</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">B4 and after: Managing hierarchy, partitioning, and asymmetry for availability and scale in google&apos;s software-defined wan</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Subhasree</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">A</forename><surname>Alfares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Alimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naidu</forename><surname>Kondapa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandan</forename><surname>Bollineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourabh</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Kaimal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirill</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Mendelev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faro</forename><surname>Padgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saikat</forename><surname>Thomas Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malveeka</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monika</forename><surname>Tierney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Zahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon</forename><surname>Zolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><surname>Vahdat</surname></persName>
		</author>
		<ptr target="https://conferences.sigcomm.org/sigcomm/2018/program_tuesday.html" />
	</analytic>
	<monogr>
		<title level="m">SIGCOMM&apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Coca: Contrastive captioners are image-text foundation models</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Legg</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Scaling autoregressive models for content-rich text-to-image generation</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Baid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burcu</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10789</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Self-chained image-language model for video localization and question answering</title>
		<author>
			<persName><forename type="first">Shoubin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06988</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">ActivityNet-QA: A dataset for understanding complex web videos via question answering</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuansheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongfu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Hellaswag: Can a machine really finish your sentence</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhehuai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parisa</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ginger</forename><surname>Perng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hagen</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franoise</forename><surname>Beaufays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.01037</idno>
		<title level="m">Google usm: Scaling automatic speech recognition beyond 100 languages</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Understanding and evaluating racial biases in image captioning</title>
		<author>
			<persName><forename type="first">Dora</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14830" to="14840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06876</idno>
		<title level="m">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Progressive-hint prompting improves reasoning in large language models</title>
		<author>
			<persName><forename type="first">Chuanyang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7590" to="7598" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
