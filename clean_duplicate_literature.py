#!/usr/bin/env python3
"""
æ¸…ç†æ•°æ®åº“ä¸­é‡å¤çš„æ–‡çŒ®è®°å½•
ä½¿ç”¨ä¿®å¤åçš„å»é‡é€»è¾‘æ¥è¯†åˆ«å’Œåˆå¹¶é‡å¤æ–‡çŒ®
"""

import asyncio
import sys
import os
import json
from typing import List, Dict, Set, Tuple
from collections import defaultdict

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from literature_parser_backend.worker.execution.data_pipeline import DataPipeline
from literature_parser_backend.db.dao import LiteratureDAO
from literature_parser_backend.db.neo4j import connect_to_mongodb, get_database

async def find_duplicate_groups() -> List[List[Dict]]:
    """æ‰¾åˆ°æ‰€æœ‰é‡å¤æ–‡çŒ®ç»„"""
    print("ğŸ” æŸ¥æ‰¾é‡å¤æ–‡çŒ®ç»„...")
    
    dao = LiteratureDAO()
    pipeline = DataPipeline(dao)
    
    # è·å–æ‰€æœ‰æ–‡çŒ®
    all_literature = await dao.find_all()
    print(f"ğŸ“š æ€»å…±æ‰¾åˆ° {len(all_literature)} ç¯‡æ–‡çŒ®")
    
    if len(all_literature) < 2:
        print("âš ï¸ æ–‡çŒ®æ•°é‡ä¸è¶³ï¼Œæ— éœ€å»é‡")
        return []
    
    # ç”¨äºå­˜å‚¨é‡å¤ç»„
    duplicate_groups = []
    processed_lids = set()
    
    for i, lit1 in enumerate(all_literature):
        if lit1.lid in processed_lids:
            continue
            
        if not lit1.metadata or not lit1.metadata.title:
            print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ–‡çŒ®: {lit1.lid}")
            continue
        
        # å½“å‰æ–‡çŒ®çš„é‡å¤ç»„
        current_group = [lit1]
        processed_lids.add(lit1.lid)
        
        # ä¸åç»­æ–‡çŒ®æ¯”è¾ƒ
        for j, lit2 in enumerate(all_literature[i+1:], i+1):
            if lit2.lid in processed_lids:
                continue
                
            if not lit2.metadata or not lit2.metadata.title:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤
            try:
                title_match = pipeline._is_title_match(lit1.metadata.title, lit2.metadata.title)
                author_match = pipeline._is_author_match(
                    getattr(lit1.metadata, 'authors', []), 
                    getattr(lit2.metadata, 'authors', [])
                )
                
                if title_match and author_match:
                    current_group.append(lit2)
                    processed_lids.add(lit2.lid)
                    print(f"ğŸ”— å‘ç°é‡å¤: {lit1.lid} <-> {lit2.lid}")
                    print(f"   æ ‡é¢˜1: {lit1.metadata.title[:50]}...")
                    print(f"   æ ‡é¢˜2: {lit2.metadata.title[:50]}...")
                    
            except Exception as e:
                print(f"âŒ æ¯”è¾ƒå¼‚å¸¸ {lit1.lid} vs {lit2.lid}: {e}")
                continue
        
        # å¦‚æœæ‰¾åˆ°é‡å¤ï¼Œæ·»åŠ åˆ°é‡å¤ç»„
        if len(current_group) > 1:
            duplicate_groups.append(current_group)
            print(f"ğŸ“‹ é‡å¤ç»„ {len(duplicate_groups)}: {len(current_group)} ç¯‡æ–‡çŒ®")
        
        # æ˜¾ç¤ºè¿›åº¦
        if (i + 1) % 10 == 0:
            print(f"ğŸ“Š å·²å¤„ç†: {i + 1}/{len(all_literature)} ({(i + 1)/len(all_literature)*100:.1f}%)")
    
    print(f"\nğŸ¯ æ€»ç»“: æ‰¾åˆ° {len(duplicate_groups)} ä¸ªé‡å¤ç»„")
    return duplicate_groups

def choose_primary_literature(group: List) -> Tuple[dict, List]:
    """é€‰æ‹©ä¸»è¦æ–‡çŒ®ï¼Œå…¶ä»–ä½œä¸ºé‡å¤"""
    if not group:
        return None, []
    
    if len(group) == 1:
        return group[0], []
    
    # é€‰æ‹©ç­–ç•¥ï¼š
    # 1. ä¼˜å…ˆé€‰æ‹©æœ‰DOIçš„æ–‡çŒ®
    # 2. ä¼˜å…ˆé€‰æ‹©å…ƒæ•°æ®æ›´å®Œæ•´çš„æ–‡çŒ®
    # 3. ä¼˜å…ˆé€‰æ‹©è¾ƒæ—©åˆ›å»ºçš„æ–‡çŒ®ï¼ˆè¾ƒçŸ­çš„LIDé€šå¸¸è¡¨ç¤ºè¾ƒæ—©ï¼‰
    
    scored_literature = []
    
    for lit in group:
        score = 0
        
        # æœ‰DOIåŠ åˆ†
        if lit.identifiers and getattr(lit.identifiers, 'doi', None):
            score += 10
        
        # æœ‰ArXiv IDåŠ åˆ†
        if lit.identifiers and getattr(lit.identifiers, 'arxiv_id', None):
            score += 5
        
        # å…ƒæ•°æ®å®Œæ•´æ€§åŠ åˆ†
        if lit.metadata:
            if getattr(lit.metadata, 'abstract', None):
                score += 3
            if getattr(lit.metadata, 'authors', None):
                score += len(lit.metadata.authors)
            if getattr(lit.metadata, 'year', None):
                score += 2
            if getattr(lit.metadata, 'journal', None):
                score += 2
        
        # LIDé•¿åº¦ï¼ˆè¾ƒçŸ­çš„LIDä¼˜å…ˆï¼‰
        score += max(0, 50 - len(lit.lid))
        
        scored_literature.append((lit, score))
    
    # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€é«˜åˆ†çš„ä½œä¸ºä¸»è¦æ–‡çŒ®
    scored_literature.sort(key=lambda x: x[1], reverse=True)
    
    primary = scored_literature[0][0]
    duplicates = [item[0] for item in scored_literature[1:]]
    
    return primary, duplicates

def print_duplicate_summary(duplicate_groups: List[List[Dict]]):
    """æ‰“å°é‡å¤æ–‡çŒ®æ‘˜è¦"""
    print("\n" + "=" * 80)
    print("é‡å¤æ–‡çŒ®è¯¦ç»†æ‘˜è¦")
    print("=" * 80)
    
    total_duplicates = 0
    
    for i, group in enumerate(duplicate_groups, 1):
        print(f"\nğŸ“‹ é‡å¤ç»„ {i}: {len(group)} ç¯‡æ–‡çŒ®")
        print("-" * 60)
        
        primary, duplicates = choose_primary_literature(group)
        total_duplicates += len(duplicates)
        
        print(f"ğŸ† ä¸»è¦æ–‡çŒ®: {primary.lid}")
        print(f"   æ ‡é¢˜: {primary.metadata.title if primary.metadata else 'N/A'}")
        if primary.identifiers:
            if getattr(primary.identifiers, 'doi', None):
                print(f"   DOI: {primary.identifiers.doi}")
            if getattr(primary.identifiers, 'arxiv_id', None):
                print(f"   ArXiv: {primary.identifiers.arxiv_id}")
        
        print(f"\nğŸ”„ é‡å¤æ–‡çŒ® ({len(duplicates)} ç¯‡):")
        for dup in duplicates:
            print(f"   - {dup.lid}")
            print(f"     æ ‡é¢˜: {dup.metadata.title if dup.metadata else 'N/A'}")
            if dup.identifiers:
                if getattr(dup.identifiers, 'doi', None):
                    print(f"     DOI: {dup.identifiers.doi}")
                if getattr(dup.identifiers, 'arxiv_id', None):
                    print(f"     ArXiv: {dup.identifiers.arxiv_id}")
    
    print(f"\nğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
    print(f"   é‡å¤ç»„æ•°é‡: {len(duplicate_groups)}")
    print(f"   å¯åˆ é™¤çš„é‡å¤æ–‡çŒ®: {total_duplicates}")
    print(f"   èŠ‚çœçš„å­˜å‚¨ç©ºé—´: {total_duplicates} ç¯‡æ–‡çŒ®")

async def merge_duplicate_literature(duplicate_groups: List[List[Dict]], dry_run: bool = True):
    """åˆå¹¶é‡å¤æ–‡çŒ®ï¼ˆé»˜è®¤ä¸ºè¯•è¿è¡Œæ¨¡å¼ï¼‰"""
    if dry_run:
        print("\nğŸ” è¯•è¿è¡Œæ¨¡å¼ - ä¸ä¼šå®é™…åˆ é™¤æ•°æ®")
    else:
        print("\nâš ï¸ å®é™…æ‰§è¡Œæ¨¡å¼ - å°†åˆ é™¤é‡å¤æ•°æ®")
    
    dao = LiteratureDAO()
    
    deleted_count = 0
    
    for i, group in enumerate(duplicate_groups, 1):
        print(f"\nå¤„ç†é‡å¤ç»„ {i}/{len(duplicate_groups)}")
        
        primary, duplicates = choose_primary_literature(group)
        
        print(f"ä¿ç•™ä¸»è¦æ–‡çŒ®: {primary.lid}")
        
        for dup in duplicates:
            print(f"  {'[è¯•è¿è¡Œ] ' if dry_run else ''}åˆ é™¤é‡å¤æ–‡çŒ®: {dup.lid}")
            
            if not dry_run:
                try:
                    await dao.delete_by_lid(dup.lid)
                    deleted_count += 1
                    print(f"    âœ… å·²åˆ é™¤")
                except Exception as e:
                    print(f"    âŒ åˆ é™¤å¤±è´¥: {e}")
            else:
                deleted_count += 1
    
    print(f"\nğŸ¯ {'æ¨¡æ‹Ÿ' if dry_run else 'å®é™…'}åˆ é™¤äº† {deleted_count} ç¯‡é‡å¤æ–‡çŒ®")
    
    if dry_run:
        print("\nâš ï¸ è¿™åªæ˜¯è¯•è¿è¡Œï¼Œæ²¡æœ‰å®é™…åˆ é™¤ä»»ä½•æ•°æ®")
        print("å¦‚è¦å®é™…æ‰§è¡Œåˆ é™¤ï¼Œè¯·è¿è¡Œ: python3 clean_duplicate_literature.py --execute")

async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("æ–‡çŒ®å»é‡å·¥å…·")
    print("=" * 80)
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    dry_run = "--execute" not in sys.argv
    
    if dry_run:
        print("ğŸ” è¯•è¿è¡Œæ¨¡å¼ï¼ˆåªæ˜¾ç¤ºç»“æœï¼Œä¸å®é™…åˆ é™¤ï¼‰")
    else:
        print("âš ï¸ æ‰§è¡Œæ¨¡å¼ï¼ˆå°†å®é™…åˆ é™¤é‡å¤æ–‡çŒ®ï¼‰")
        print("è¯·ç¡®è®¤æ‚¨å·²å¤‡ä»½æ•°æ®åº“ï¼")
        
        confirm = input("ç»§ç»­æ‰§è¡Œå—ï¼Ÿ(yes/no): ")
        if confirm.lower() != 'yes':
            print("æ“ä½œå·²å–æ¶ˆ")
            return
    
    try:
        # è¿æ¥æ•°æ®åº“
        await connect_to_mongodb()
        
        # æŸ¥æ‰¾é‡å¤æ–‡çŒ®
        duplicate_groups = await find_duplicate_groups()
        
        if not duplicate_groups:
            print("ğŸ‰ æ²¡æœ‰å‘ç°é‡å¤æ–‡çŒ®ï¼")
            return
        
        # æ˜¾ç¤ºæ‘˜è¦
        print_duplicate_summary(duplicate_groups)
        
        # æ‰§è¡Œåˆå¹¶
        await merge_duplicate_literature(duplicate_groups, dry_run)
        
        print("\nâœ… å»é‡ä»»åŠ¡å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
