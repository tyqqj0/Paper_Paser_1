#!/usr/bin/env python3
"""
å…¨é¢æµ‹è¯•è„šæœ¬ - æµ‹è¯•æ–°ç€‘å¸ƒæµæ¶æ„å¯¹å„ç§å­¦æœ¯ç½‘ç«™é“¾æ¥çš„å¤„ç†èƒ½åŠ›

æµ‹è¯•è¦†ç›–ï¼š
- ArXiv: æœ‰IDå’Œæ— ID
- NeurIPS: ä¸åŒå¹´ä»½
- ACM Digital Library
- IEEE Xplore  
- SpringerLink
- å„ç§edge cases

Author: Paper Parser Team
Date: 2025-08-14
"""

import asyncio
import json
import time
from datetime import datetime
from typing import Dict, List, Any
import aiohttp
import sys

# æµ‹è¯•ç”¨ä¾‹é…ç½®
TEST_CASES = [
    {
        "name": "ArXivç»å…¸è®ºæ–‡ - Transformer",
        "url": "https://arxiv.org/abs/1706.03762",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["doi", "arxiv_id", "high_quality"],
        "description": "æœ‰ArXiv IDçš„ç»å…¸è®ºæ–‡ï¼Œåº”è¯¥ä¼˜å…ˆä½¿ç”¨Semantic Scholar"
    },
    # {
    #     "name": "ArXivæœ€æ–°è®ºæ–‡ - Vision Transformer", 
    #     "url": "https://arxiv.org/abs/2010.11929",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["arxiv_id"],
    #     "description": "è¾ƒæ–°çš„ArXivè®ºæ–‡æµ‹è¯•"
    # },
    # {
    #     "name": "NeurIPS 2012 - AlexNet",
    #     "url": "https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
    #     "expected_processor": "CrossRef",
    #     "expected_features": ["title_match", "no_doi"],
    #     "description": "ç»å…¸çš„NeurIPSè®ºæ–‡ï¼Œæ— DOIï¼Œéœ€è¦æ ‡é¢˜åŒ¹é…"
    # },
    # {
    #     "name": "Acceleration of stochastic approximation by averaging",
    #     "url": "https://doi.org/10.1137/0330046",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi"],
    #     "description": "ç»å…¸çš„NeurIPSè®ºæ–‡ï¼Œæœ‰DOI"
    # },
    {
        "name": "NeurIPS 2017 - Attentionè®ºæ–‡",
        "url": "https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html",
        "expected_processor": "CrossRef",
        "expected_features": ["title_match"],
        "description": "å¦ä¸€ç¯‡é‡è¦çš„NeurIPSè®ºæ–‡"
    },
    # {
    #     "name": "ACM Digital Library - æœ‰DOI",
    #     "url": "https://dl.acm.org/doi/10.1145/3292500.3330958",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi"],
    #     "description": "ACMè®ºæ–‡ï¼Œæœ‰æ˜ç¡®DOI"
    # },
    # {
    #     "name": "IEEE Xploreè®ºæ–‡",
    #     "url": "https://ieeexplore.ieee.org/document/8578335",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["url_parsing"],
    #     "description": "IEEEè®ºæ–‡æµ‹è¯•"
    # },

    # {
    #     "name": "ResNetåŸè®ºæ–‡ - åº”è¯¥æœ‰DOI",
    #     "url": "https://arxiv.org/abs/1512.03385",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi", "arxiv_id"],
    #     "description": "ç»å…¸ResNetè®ºæ–‡ï¼Œå‘è¡¨åœ¨CVPRï¼Œåº”è¯¥æœ‰DOI"
    # },
    # {
    #     "name": "ArXivè®ºæ–‡ - æœ‰DOIå’ŒArXiv ID",
    #     "url": "https://arxiv.org/abs/1412.6980",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi", "arxiv_id"],
    #     "description": "æœ‰DOIå’ŒArXiv IDçš„è®ºæ–‡"
    # },
    
    # # ğŸ†• æ‰©å±•é‡è¦è®ºæ–‡æµ‹è¯•é›†
    # {
    #     "name": "BERT - è‡ªç„¶è¯­è¨€å¤„ç†é‡Œç¨‹ç¢‘",
    #     "url": "https://arxiv.org/abs/1810.04805",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi", "arxiv_id"],
    #     "description": "BERTè®ºæ–‡ï¼ŒNLPé¢†åŸŸé‡è¦çªç ´"
    # },

    # {
    #     "name": "AlphaGoè®ºæ–‡ - Nature",
    #     "url": "https://www.nature.com/articles/nature16961",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi"],
    #     "description": "AlphaGoçªç ´æ€§è®ºæ–‡ï¼Œå‘è¡¨åœ¨Nature"
    # },
    # {
    #     "name": "GANè®ºæ–‡ - Ian Goodfellow",
    #     "url": "https://arxiv.org/abs/1406.2661",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi", "arxiv_id"],
    #     "description": "ç”Ÿæˆå¯¹æŠ—ç½‘ç»œåŸå§‹è®ºæ–‡"
    # },
    # {
    #     "name": "U-Net - åŒ»å­¦å›¾åƒåˆ†å‰²",
    #     "url": "https://arxiv.org/abs/1505.04597",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi", "arxiv_id"],
    #     "description": "U-Netæ¶æ„ï¼ŒåŒ»å­¦å›¾åƒåˆ†å‰²ç»å…¸"
    # },
    # {
    #     "name": "Batch Normalization",
    #     "url": "https://arxiv.org/abs/1502.03167",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi", "arxiv_id"],
    #     "description": "æ‰¹æ ‡å‡†åŒ–è®ºæ–‡ï¼Œæ·±åº¦å­¦ä¹ é‡è¦æŠ€æœ¯"
    # },
    # {
    #     "name": "Word2Vec - è¯å‘é‡",
    #     "url": "https://arxiv.org/abs/1301.3781",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["arxiv_id"],
    #     "description": "Word2Vecè¯å‘é‡è¡¨ç¤ºå­¦ä¹ "
    # },
    # {
    #     "name": "Seq2Seq - åºåˆ—åˆ°åºåˆ—",
    #     "url": "https://arxiv.org/abs/1409.3215",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi", "arxiv_id"],
    #     "description": "åºåˆ—åˆ°åºåˆ—å­¦ä¹ è®ºæ–‡"
    # },
    # {
    #     "name": "Adamä¼˜åŒ–å™¨",
    #     "url": "https://arxiv.org/abs/1412.6980",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["arxiv_id"],
    #     "description": "Adamä¼˜åŒ–ç®—æ³•è®ºæ–‡"
    # },
    # {
    #     "name": "æ·±åº¦å­¦ä¹ Natureç»¼è¿° - LeCun",
    #     "url": "https://www.nature.com/articles/nature14539",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi"],
    #     "description": "æ·±åº¦å­¦ä¹ Natureç»¼è¿°ï¼ŒLeCunç­‰äºº"
    # },

    # {
    #     "name": "YOLOç›®æ ‡æ£€æµ‹",
    #     "url": "https://arxiv.org/abs/1506.02640",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi", "arxiv_id"],
    #     "description": "YOLOå®æ—¶ç›®æ ‡æ£€æµ‹ç®—æ³•"
    # },
    # {
    #     "name": "LSTM - é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ",
    #     "url": "https://ieeexplore.ieee.org/abstract/document/6795963",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["pdf_url"],
    #     "description": "LSTMåŸå§‹è®ºæ–‡ï¼Œ1997å¹´ç»å…¸"
    # },
    # {
    #     "name": "imagenet",
    #     "url": "https://doi.org/10.1145/3065386",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi"],
    #     "description": "ReLUæ¿€æ´»å‡½æ•°çš„æ·±å…¥ç ”ç©¶"
    # },
    # {
    #     "name": "ReLUæ¿€æ´»å‡½æ•°ç ”ç©¶",
    #     "url": "https://proceedings.mlr.press/v15/glorot11a.html",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["title_match"],
    #     "description": "ReLUæ¿€æ´»å‡½æ•°çš„æ·±å…¥ç ”ç©¶"
    # },
    ############################## æš‚æ—¶æœ‰é—®é¢˜çš„æµ‹è¯•ç”¨ä¾‹ ##############################
    #     {
    #     "name": "LSTM - é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ",
    #     "url": "https://www.bioinf.jku.at/publications/older/2604.pdf",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["pdf_url"],
    #     "description": "LSTMåŸå§‹è®ºæ–‡ï¼Œ1997å¹´ç»å…¸"
    # },
    #     {
    #     "name": "ImageNetå¤§è§„æ¨¡è§†è§‰è¯†åˆ«",
    #     "url": "https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Russakovsky_ImageNet_Large_Scale_2015_CVPR_paper.html",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["title_match"],
    #     "description": "ImageNetæ•°æ®é›†å’Œç«èµ›çš„é‡è¦è®ºæ–‡"
    # },
    #     {
    #     "name": "Dropoutè®ºæ–‡",
    #     "url": "https://jmlr.org/papers/v15/srivastava14a.html",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["title_match"],
    #     "description": "Dropoutæ­£åˆ™åŒ–æŠ€æœ¯è®ºæ–‡"
    # },
    #     {
    #     "name": "GPT-1 åŸè®ºæ–‡",
    #     "url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["pdf_url"],
    #     "description": "GPT-1åŸè®ºæ–‡ï¼Œç›´æ¥PDFé“¾æ¥"
    # },
        # {
    #     "name": "ArXiv PDFç›´é“¾",
    #     "url": "https://arxiv.org/pdf/1706.03762.pdf",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["arxiv_id"],
    #     "description": "ArXiv PDFé“¾æ¥åº”è¯¥èƒ½æå–ArXiv ID"
    # }
]

class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    def __init__(self, test_case: Dict[str, Any]):
        self.test_case = test_case
        self.success = False
        self.literature_id = None
        self.processor_used = None
        self.metadata_quality = None
        self.processing_time = None
        self.error_message = None
        self.raw_response = None
        self.analysis = {}

class ComprehensiveTester:
    """å…¨é¢æµ‹è¯•å™¨ç±»"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results: List[TestResult] = []
        self.session = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def test_single_url(self, test_case: Dict[str, Any]) -> TestResult:
        """æµ‹è¯•å•ä¸ªURL - ä½¿ç”¨SSEæµå¼ä¼ è¾“"""
        result = TestResult(test_case)
        url = test_case["url"]
        
        print(f"\nğŸ§ª æµ‹è¯•: {test_case['name']}")
        print(f"   URL: {url}")
        # print(f"   é¢„æœŸå¤„ç†å™¨: {test_case['expected_processor']}")
        
        start_time = time.time()
        
        try:
            # é¦–å…ˆæäº¤è§£æè¯·æ±‚è·å–task_id
            async with self.session.post(
                f"{self.base_url}/api/resolve",
                json={"url": url},
                timeout=30
            ) as response:
                response_data = await response.json()
                
                if response.status == 202:
                    # è·å–ä»»åŠ¡ID
                    task_id = response_data.get("task_id")
                    if not task_id:
                        result.error_message = "No task_id in 202 response"
                        print(f"   âŒ å¤±è´¥: {result.error_message}")
                        return result
                    
                    print(f"   â³ ä»»åŠ¡å·²åˆ›å»º: {task_id}, å¼€å§‹SSEæµå¼ç›‘å¬...")
                    
                    # ä½¿ç”¨SSEç›‘å¬ä»»åŠ¡çŠ¶æ€
                    result = await self._stream_task_completion(result, task_id)
                    
                elif response.status == 200:
                    # åŒæ­¥å“åº”ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    result.success = True
                    result.raw_response = response_data
                    
                    if isinstance(response_data, dict):
                        result.literature_id = response_data.get("lid")
                        if result.literature_id:
                            result = await self._get_literature_details(result)
                    
                    print(f"   âœ… æˆåŠŸ: LID={result.literature_id}")
                else:
                    result.error_message = f"HTTP {response.status}: {response_data}"
                    print(f"   âŒ å¤±è´¥: {result.error_message}")
                    
        except Exception as e:
            result.error_message = str(e)
            print(f"   âŒ å¼‚å¸¸: {result.error_message}")
        
        result.processing_time = time.time() - start_time
        return result
    
    async def _stream_task_completion(self, result: TestResult, task_id: str, max_wait: int = 60) -> TestResult:
        """ä½¿ç”¨SSEæµå¼ç›‘å¬ä»»åŠ¡çŠ¶æ€ç›´åˆ°å®Œæˆ"""
        stream_start = time.time()
        
        try:
            # å»ºç«‹SSEè¿æ¥
            async with self.session.get(
                f"{self.base_url}/api/tasks/{task_id}/stream",
                headers={
                    "Accept": "text/event-stream",
                    "Cache-Control": "no-cache",
                },
                timeout=aiohttp.ClientTimeout(total=max_wait+10)
            ) as response:
                
                if response.status != 200:
                    result.error_message = f"SSEè¿æ¥å¤±è´¥: HTTP {response.status}"
                    print(f"   âŒ SSEè¿æ¥å¤±è´¥: {result.error_message}")
                    return result
                
                print(f"   ğŸ“¡ SSEè¿æ¥å·²å»ºç«‹ï¼Œå¼€å§‹æ¥æ”¶å®æ—¶çŠ¶æ€...")
                
                # è¯»å–SSEæµ
                current_event_type = None
                async for line in response.content:
                    # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                    if time.time() - stream_start > max_wait:
                        result.error_message = f"Task timeout after {max_wait}s"
                        print(f"   âŒ è¶…æ—¶: ä»»åŠ¡åœ¨{max_wait}ç§’å†…æœªå®Œæˆ")
                        return result
                    
                    line_str = line.decode('utf-8').strip()
                    if not line_str:
                        continue
                    
                    # print(f"   ğŸ” SSEåŸå§‹æ•°æ®: {repr(line_str)}")
                    
                    # è§£æSSEäº‹ä»¶
                    if line_str.startswith('event:'):
                        current_event_type = line_str[6:].strip()
                        print(f"   ğŸ“ SSEäº‹ä»¶ç±»å‹: {current_event_type}")
                        continue
                    elif line_str.startswith('data:'):
                        data_str = line_str[5:].strip()
                        # print(f"   ğŸ“ SSEæ•°æ®: {data_str}")
                        
                        try:
                            data = json.loads(data_str)
                            # print(f"   ğŸ“ SSEè§£æåæ•°æ®: {data}")
                        except json.JSONDecodeError as e:
                            print(f"   âš ï¸  SSE JSONè§£æå¤±è´¥: {e}")
                            continue
                        
                        # å¤„ç†ä¸åŒç±»å‹çš„äº‹ä»¶ - ä½¿ç”¨ä»eventè¡Œè§£æçš„ç±»å‹
                        if current_event_type:
                            
                            if current_event_type == 'completed':
                                # ä»»åŠ¡å®Œæˆ
                                result.literature_id = data.get('literature_id')
                                result.raw_response = data
                                
                                if result.literature_id:
                                    result.success = True
                                    print(f"   âœ… æˆåŠŸå®Œæˆ: LID={result.literature_id}")
                                    result = await self._get_literature_details(result)
                                else:
                                    result.success = False
                                    result.error_message = "ä»»åŠ¡å®Œæˆä½†æœªç”Ÿæˆæœ‰æ•ˆçš„æ–‡çŒ®ID"
                                    print(f"   âŒ å¤±è´¥: æœªç”Ÿæˆæœ‰æ•ˆLID")
                                
                                return result
                                
                            elif current_event_type in ['url_validation_failed', 'component_failed', 'task_failed', 'failed']:
                                # ä»»åŠ¡å¤±è´¥
                                error_msg = data.get('error', data.get('error_message', 'Unknown error'))
                                error_type = data.get('error_type', 'Unknown')
                                result.error_message = f"Task failed: {error_msg}"
                                result.raw_response = data
                                
                                print(f"   âŒ ä»»åŠ¡å¤±è´¥: {error_msg}")
                                print(f"   ğŸ” é”™è¯¯ç±»å‹: {error_type}")
                                
                                # æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´å¤šä¿¡æ¯
                                self._analyze_error_type(error_type, error_msg)
                                
                                return result
                                
                            elif current_event_type == 'progress':
                                # è¿›åº¦æ›´æ–°äº‹ä»¶
                                progress = data.get('progress', 0)
                                stage = data.get('stage', '')
                                print(f"   ğŸ”„ {stage} ({progress}%)")
                                # é‡ç½®äº‹ä»¶ç±»å‹ï¼Œç»§ç»­ç­‰å¾…ä¸‹ä¸€ä¸ªäº‹ä»¶
                                current_event_type = None
                                continue
                                
                        # å¤„ç†çŠ¶æ€æ›´æ–°äº‹ä»¶ï¼ˆå¸¦è¿›åº¦ä¿¡æ¯ï¼‰- å…¼å®¹æ—§æ ¼å¼
                        elif 'task_id' in data and 'execution_status' in data:
                            execution_status = data.get('execution_status', '').lower()
                            overall_progress = data.get('overall_progress', 0)
                            current_stage = data.get('current_stage', '')
                            
                            print(f"   ğŸ”„ {current_stage} ({overall_progress}%)")
                            
                            if execution_status == 'completed':
                                # ä»å®Œæ•´çŠ¶æ€ä¿¡æ¯ä¸­æå–ç»“æœ
                                literature_status = data.get('literature_status', {})
                                if 'literature_id' in literature_status:
                                    result.literature_id = literature_status['literature_id']
                                    result.success = True
                                    result.raw_response = data
                                    print(f"   âœ… æˆåŠŸå®Œæˆ: LID={result.literature_id}")
                                    result = await self._get_literature_details(result)
                                    return result
                            
                            elif execution_status == 'failed':
                                error_info = data.get('error_info', {})
                                error_msg = error_info.get('error_message', 'Unknown error')
                                error_type = error_info.get('error_type', 'Unknown')
                                result.error_message = f"Task failed: {error_msg}"
                                result.raw_response = data
                                
                                print(f"   âŒ ä»»åŠ¡å¤±è´¥: {error_msg}")
                                print(f"   ğŸ” é”™è¯¯ç±»å‹: {error_type}")
                                self._analyze_error_type(error_type, error_msg)
                                
                                return result
                        
        except asyncio.TimeoutError:
            result.error_message = f"SSE stream timeout after {max_wait}s"
            print(f"   âŒ SSEæµè¶…æ—¶: {result.error_message}")
            return result
        except Exception as e:
            result.error_message = f"SSE stream error: {e}"
            print(f"   âŒ SSEæµå¼‚å¸¸: {result.error_message}")
            return result
        
        # å¦‚æœæµç»“æŸä½†æ²¡æœ‰æ˜ç¡®çš„å®Œæˆæˆ–å¤±è´¥äº‹ä»¶
        result.error_message = "SSE stream ended without completion"
        print(f"   âŒ SSEæµå¼‚å¸¸ç»“æŸ")
        return result
    
    async def _get_literature_details(self, result: TestResult) -> TestResult:
        """è·å–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯å¹¶åˆ†æ"""
        if not result.literature_id:
            return result
            
        try:
            async with self.session.get(
                f"{self.base_url}/api/literatures/{result.literature_id}"
            ) as response:
                if response.status == 200:
                    details = await response.json()
                    result = self._analyze_literature_details(result, details)
                else:
                    result.analysis["detail_fetch_error"] = f"HTTP {response.status}"
                    
        except Exception as e:
            result.analysis["detail_fetch_error"] = str(e)
            
        return result
    
    def _analyze_literature_details(self, result: TestResult, details: Dict[str, Any]) -> TestResult:
        """åˆ†ææ–‡çŒ®è¯¦ç»†ä¿¡æ¯"""
        metadata = details.get("metadata", {})
        task_info = details.get("task_info", {})
        component_status = task_info.get("component_status", {})
        metadata_status = component_status.get("metadata", {})
        
        # æå–å¤„ç†å™¨ä¿¡æ¯
        result.processor_used = metadata_status.get("source", "Unknown")
        
        # è®¡ç®—å…ƒæ•°æ®è´¨é‡åˆ†æ•°
        result.metadata_quality = self._calculate_quality_score(metadata)
        
        # éªŒè¯é¢„æœŸç‰¹æ€§
        result.analysis = self._verify_expected_features(result.test_case, details)
        
        # æ‰“å°åˆ†æç»“æœ
        # print(f"   ğŸ“Š å¤„ç†å™¨: {result.processor_used}")
        print(f"   ğŸ“Š è´¨é‡åˆ†æ•°: {result.metadata_quality}/100")
        print(f"   ğŸ“Š å¤„ç†æ—¶é—´: {result.processing_time:.2f}s")
        
        # éªŒè¯é¢„æœŸ
        expected_processor = result.test_case["expected_processor"]
        if result.processor_used == expected_processor:
            print(f"   âœ… å¤„ç†å™¨åŒ¹é…é¢„æœŸ: {expected_processor}")
        else:
            print(f"   âš ï¸  å¤„ç†å™¨ä¸åŒ¹é…: é¢„æœŸ{expected_processor}, å®é™…{result.processor_used}")
        
        return result
    
    def _analyze_error_type(self, error_type: str, error_msg: str):
        """æ ¹æ®é”™è¯¯ç±»å‹æä¾›è¯¦ç»†åˆ†æå’Œå»ºè®®"""
        if error_type == "HTTPError":
            print(f"   ğŸ’¡ HTTPé”™è¯¯åˆ†æ: å¯èƒ½æ˜¯URLæ— æ•ˆã€æœåŠ¡å™¨ä¸å¯è¾¾æˆ–æƒé™é—®é¢˜")
            if "404" in error_msg:
                print(f"   ğŸ“ å»ºè®®: æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®ï¼Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨")
            elif "403" in error_msg:
                print(f"   ğŸ“ å»ºè®®: å¯èƒ½éœ€è¦è®¿é—®æƒé™æˆ–åçˆ¬è™«é™åˆ¶")
            elif "timeout" in error_msg.lower():
                print(f"   ğŸ“ å»ºè®®: ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œå¯ä»¥ç¨åé‡è¯•")
        
        elif error_type == "GROBIDConnectionError":
            print(f"   ğŸ’¡ GROBIDæœåŠ¡é”™è¯¯: PDFè§£ææœåŠ¡ä¸å¯ç”¨")
            print(f"   ğŸ“ å»ºè®®: æ£€æŸ¥GROBIDæœåŠ¡çŠ¶æ€ï¼Œå¯èƒ½éœ€è¦é‡å¯æœåŠ¡")
        
        elif error_type == "URLValidationError":
            print(f"   ğŸ’¡ URLæ ¼å¼é”™è¯¯: è¾“å…¥çš„é“¾æ¥æ ¼å¼ä¸æ­£ç¡®")
            print(f"   ğŸ“ å»ºè®®: ç¡®ä¿URLä»¥http://æˆ–https://å¼€å¤´")
        
        elif error_type == "ParseError":
            print(f"   ğŸ’¡ è§£æé”™è¯¯: PDFå†…å®¹æ— æ³•æ­£ç¡®è§£æ")
            print(f"   ğŸ“ å»ºè®®: å¯èƒ½æ˜¯æ‰«æç‰ˆPDFæˆ–æ ¼å¼ç‰¹æ®Šï¼Œå°è¯•å…¶ä»–å¤„ç†å™¨")
        
        elif error_type == "TaskExecutionError":
            print(f"   ğŸ’¡ ä»»åŠ¡æ‰§è¡Œé”™è¯¯: Celeryä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜")
            print(f"   ğŸ“ å»ºè®®: æ£€æŸ¥ä»»åŠ¡é˜Ÿåˆ—å’ŒworkerçŠ¶æ€")
        
        elif error_type == "Unknown":
            print(f"   ğŸ’¡ æœªçŸ¥é”™è¯¯ç±»å‹: å¯èƒ½æ˜¯æ–°çš„é”™è¯¯ç±»å‹æˆ–ç³»ç»Ÿé—®é¢˜")
            print(f"   ğŸ“ å»ºè®®: æ£€æŸ¥å®Œæ•´é”™è¯¯æ—¥å¿—ï¼Œè”ç³»æŠ€æœ¯æ”¯æŒ")
        
        else:
            print(f"   ğŸ’¡ é”™è¯¯ç±»å‹ '{error_type}': éœ€è¦è¿›ä¸€æ­¥åˆ†æ")
            print(f"   ğŸ“ å»ºè®®: æŸ¥çœ‹è¯¦ç»†æ—¥å¿—è·å–æ›´å¤šä¿¡æ¯")
    
    def _calculate_quality_score(self, metadata: Dict[str, Any]) -> int:
        """è®¡ç®—å…ƒæ•°æ®è´¨é‡åˆ†æ•°"""
        score = 0
        
        # æ ‡é¢˜ (å¿…é¡»)
        title = metadata.get("title", "")
        if title and title != "Unknown Title":
            score += 25
        
        # ä½œè€…
        authors = metadata.get("authors", [])
        if authors:
            score += 20
        
        # å¹´ä»½
        year = metadata.get("year")
        if year and year > 1900:
            score += 15
        
        # æœŸåˆŠ/ä¼šè®®
        journal = metadata.get("journal", "")
        if journal:
            score += 15
        
        # æ‘˜è¦
        abstract = metadata.get("abstract", "")
        if abstract and len(abstract) > 50:
            score += 15
        
        # å…³é”®è¯
        keywords = metadata.get("keywords", [])
        if keywords:
            score += 10
        
        return min(score, 100)
    
    def _verify_expected_features(self, test_case: Dict[str, Any], details: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯é¢„æœŸç‰¹æ€§"""
        analysis = {}
        expected_features = test_case.get("expected_features", [])
        
        identifiers = details.get("identifiers", {})
        metadata = details.get("metadata", {})
        
        for feature in expected_features:
            if feature == "doi":
                analysis["has_doi"] = bool(identifiers.get("doi"))
            elif feature == "arxiv_id":
                analysis["has_arxiv_id"] = bool(identifiers.get("arxiv_id"))
            elif feature == "high_quality":
                analysis["is_high_quality"] = self._calculate_quality_score(metadata) >= 80
            elif feature == "title_match":
                analysis["title_available"] = bool(metadata.get("title") and metadata["title"] != "Unknown Title")
            elif feature == "no_doi":
                analysis["correctly_no_doi"] = not bool(identifiers.get("doi"))
            elif feature == "url_parsing":
                analysis["url_processed"] = bool(details.get("identifiers", {}).get("source_urls"))
        
        return analysis
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å…¨é¢æµ‹è¯•æ–°ç€‘å¸ƒæµæ¶æ„")
        print("=" * 60)
        
        start_time = time.time()
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        for test_case in TEST_CASES:
            result = await self.test_single_url(test_case)
            self.results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            await asyncio.sleep(1)
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = self._generate_report(total_time)
        
        return report
    
    def _generate_report(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r.success)
        
        processor_usage = {}
        quality_scores = []
        
        for result in self.results:
            if result.success:
                processor = result.processor_used or "Unknown"
                processor_usage[processor] = processor_usage.get(processor, 0) + 1
                if result.metadata_quality is not None:
                    quality_scores.append(result.metadata_quality)
        
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        report = {
            "summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "success_rate": f"{(successful_tests/total_tests)*100:.1f}%",
                "total_time": f"{total_time:.2f}s",
                "avg_quality_score": f"{avg_quality:.1f}/100"
            },
            "processor_usage": processor_usage,
            "detailed_results": []
        }
        
        # æ·»åŠ è¯¦ç»†ç»“æœ
        for result in self.results:
            detailed = {
                "test_name": result.test_case["name"],
                "url": result.test_case["url"],
                "success": result.success,
                "lid": result.literature_id,
                "processor_used": result.processor_used,
                "metadata_quality": result.metadata_quality,
                "processing_time": f"{result.processing_time:.2f}s" if result.processing_time else None,
                "error_message": result.error_message,
                "analysis": result.analysis
            }
            
            # æ·»åŠ é”™è¯¯ç±»å‹ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if not result.success and result.raw_response:
                detailed["error_type"] = result.raw_response.get("error_type", "Unknown")
            
            report["detailed_results"].append(detailed)
        
        return report
    
    def print_report(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•æŠ¥å‘Šæ€»ç»“")
        print("=" * 60)
        
        summary = report["summary"]
        print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"   æˆåŠŸæµ‹è¯•: {summary['successful_tests']}")
        print(f"   æˆåŠŸç‡: {summary['success_rate']}")
        print(f"   æ€»è€—æ—¶: {summary['total_time']}")
        print(f"   å¹³å‡è´¨é‡åˆ†æ•°: {summary['avg_quality_score']}")
        
        print(f"\nğŸ”§ å¤„ç†å™¨ä½¿ç”¨ç»Ÿè®¡:")
        for processor, count in report["processor_usage"].items():
            print(f"   {processor}: {count}æ¬¡")
        
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for result in report["detailed_results"]:
            status = "âœ…" if result["success"] else "âŒ"
            is_duplicate = result.get("raw_response", {}).get("result_type") == "duplicate"
            duplicate_marker = " (å‰¯æœ¬)" if is_duplicate else ""
            print(f"   {status} {result['test_name']}{duplicate_marker}")
            if result["success"]:
                print(f"      å¤„ç†å™¨: {result['processor_used']}")
                print(f"      è´¨é‡: {result['metadata_quality']}/100")
                print(f"      æ—¶é—´: {result['processing_time']}")
            else:
                print(f"      é”™è¯¯: {result['error_message']}")
                if "error_type" in result:
                    print(f"      ç±»å‹: {result['error_type']}")
                    
        # æ·»åŠ é”™è¯¯ç±»å‹ç»Ÿè®¡
        error_types = {}
        failed_results = [r for r in report["detailed_results"] if not r["success"]]
        
        if failed_results:
            print(f"\nğŸ” é”™è¯¯ç±»å‹ç»Ÿè®¡:")
            for result in failed_results:
                error_type = result.get("error_type", "Unknown")
                error_types[error_type] = error_types.get(error_type, 0) + 1
            
            for error_type, count in error_types.items():
                print(f"   {error_type}: {count}æ¬¡")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Paper Parser æ–°ç€‘å¸ƒæµæ¶æ„å…¨é¢æµ‹è¯•")
    print("=" * 60)
    
    async with ComprehensiveTester() as tester:
        report = await tester.run_all_tests()
        tester.print_report(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # report_file = f"test_report_{timestamp}.json"
        
        # with open(report_file, 'w', encoding='utf-8') as f:
        #     json.dump(report, f, ensure_ascii=False, indent=2)
        
        # print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

if __name__ == "__main__":
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    if sys.version_info < (3, 7):
        print("âŒ éœ€è¦Python 3.7æˆ–æ›´é«˜ç‰ˆæœ¬")
        sys.exit(1)
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)
