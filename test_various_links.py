#!/usr/bin/env python3
"""
å…¨é¢æµ‹è¯•è„šæœ¬ - æµ‹è¯•æ–°ç€‘å¸ƒæµæ¶æ„å¯¹å„ç§å­¦æœ¯ç½‘ç«™é“¾æ¥çš„å¤„ç†èƒ½åŠ›

æµ‹è¯•è¦†ç›–ï¼š
- ArXiv: æœ‰IDå’Œæ— ID
- NeurIPS: ä¸åŒå¹´ä»½
- ACM Digital Library
- IEEE Xplore  
- SpringerLink
- å„ç§edge cases

Author: Paper Parser Team
Date: 2025-08-14
"""

import asyncio
import json
import time
from datetime import datetime
from typing import Dict, List, Any
import aiohttp
import sys

# æµ‹è¯•ç”¨ä¾‹é…ç½®
TEST_CASES = [
    # {
    #     "name": "ArXivç»å…¸è®ºæ–‡ - Transformer",
    #     "url": "https://arxiv.org/abs/1706.03762",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi", "arxiv_id", "high_quality"],
    #     "description": "æœ‰ArXiv IDçš„ç»å…¸è®ºæ–‡ï¼Œåº”è¯¥ä¼˜å…ˆä½¿ç”¨Semantic Scholar"
    # },
    # {
    #     "name": "ArXivæœ€æ–°è®ºæ–‡ - Vision Transformer", 
    #     "url": "https://arxiv.org/abs/2010.11929",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["arxiv_id"],
    #     "description": "è¾ƒæ–°çš„ArXivè®ºæ–‡æµ‹è¯•"
    # },
    # {
    #     "name": "NeurIPS 2012 - AlexNet",
    #     "url": "https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
    #     "expected_processor": "CrossRef",
    #     "expected_features": ["title_match", "no_doi"],
    #     "description": "ç»å…¸çš„NeurIPSè®ºæ–‡ï¼Œæ— DOIï¼Œéœ€è¦æ ‡é¢˜åŒ¹é…"
    # },
    {
        "name": "Acceleration of stochastic approximation by averaging",
        "url": "https://doi.org/10.1137/0330046",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["doi"],
        "description": "ç»å…¸çš„NeurIPSè®ºæ–‡ï¼Œæœ‰DOI"
    },
    # {
    #     "name": "NeurIPS 2017 - Attentionè®ºæ–‡",
    #     "url": "https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html",
    #     "expected_processor": "CrossRef",
    #     "expected_features": ["title_match"],
    #     "description": "å¦ä¸€ç¯‡é‡è¦çš„NeurIPSè®ºæ–‡"
    # },
    # {
    #     "name": "ACM Digital Library - æœ‰DOI",
    #     "url": "https://dl.acm.org/doi/10.1145/3292500.3330958",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi"],
    #     "description": "ACMè®ºæ–‡ï¼Œæœ‰æ˜ç¡®DOI"
    # },
    # {
    #     "name": "IEEE Xploreè®ºæ–‡",
    #     "url": "https://ieeexplore.ieee.org/document/8578335",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["url_parsing"],
    #     "description": "IEEEè®ºæ–‡æµ‹è¯•"
    # },
    # {
    #     "name": "ArXiv PDFç›´é“¾",
    #     "url": "https://arxiv.org/pdf/1706.03762.pdf",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["arxiv_id"],
    #     "description": "ArXiv PDFé“¾æ¥åº”è¯¥èƒ½æå–ArXiv ID"
    # },
    {
        "name": "ResNetåŸè®ºæ–‡ - åº”è¯¥æœ‰DOI",
        "url": "https://arxiv.org/abs/1512.03385",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["doi", "arxiv_id"],
        "description": "ç»å…¸ResNetè®ºæ–‡ï¼Œå‘è¡¨åœ¨CVPRï¼Œåº”è¯¥æœ‰DOI"
    },
    {
        "name": "ArXivè®ºæ–‡ - æœ‰DOIå’ŒArXiv ID",
        "url": "https://arxiv.org/abs/1412.6980",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["doi", "arxiv_id"],
        "description": "æœ‰DOIå’ŒArXiv IDçš„è®ºæ–‡"
    },
    
    # ğŸ†• æ‰©å±•é‡è¦è®ºæ–‡æµ‹è¯•é›†
    {
        "name": "BERT - è‡ªç„¶è¯­è¨€å¤„ç†é‡Œç¨‹ç¢‘",
        "url": "https://arxiv.org/abs/1810.04805",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["doi", "arxiv_id"],
        "description": "BERTè®ºæ–‡ï¼ŒNLPé¢†åŸŸé‡è¦çªç ´"
    },
    # {
    #     "name": "GPT-1 åŸè®ºæ–‡",
    #     "url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["pdf_url"],
    #     "description": "GPT-1åŸè®ºæ–‡ï¼Œç›´æ¥PDFé“¾æ¥"
    # },
    # {
    #     "name": "AlphaGoè®ºæ–‡ - Nature",
    #     "url": "https://www.nature.com/articles/nature16961",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi"],
    #     "description": "AlphaGoçªç ´æ€§è®ºæ–‡ï¼Œå‘è¡¨åœ¨Nature"
    # },
    {
        "name": "GANè®ºæ–‡ - Ian Goodfellow",
        "url": "https://arxiv.org/abs/1406.2661",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["doi", "arxiv_id"],
        "description": "ç”Ÿæˆå¯¹æŠ—ç½‘ç»œåŸå§‹è®ºæ–‡"
    },
    {
        "name": "U-Net - åŒ»å­¦å›¾åƒåˆ†å‰²",
        "url": "https://arxiv.org/abs/1505.04597",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["doi", "arxiv_id"],
        "description": "U-Netæ¶æ„ï¼ŒåŒ»å­¦å›¾åƒåˆ†å‰²ç»å…¸"
    },
    {
        "name": "Batch Normalization",
        "url": "https://arxiv.org/abs/1502.03167",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["doi", "arxiv_id"],
        "description": "æ‰¹æ ‡å‡†åŒ–è®ºæ–‡ï¼Œæ·±åº¦å­¦ä¹ é‡è¦æŠ€æœ¯"
    },
    # {
    #     "name": "Dropoutè®ºæ–‡",
    #     "url": "https://jmlr.org/papers/v15/srivastava14a.html",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["title_match"],
    #     "description": "Dropoutæ­£åˆ™åŒ–æŠ€æœ¯è®ºæ–‡"
    # },
    {
        "name": "Word2Vec - è¯å‘é‡",
        "url": "https://arxiv.org/abs/1301.3781",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["arxiv_id"],
        "description": "Word2Vecè¯å‘é‡è¡¨ç¤ºå­¦ä¹ "
    },
    {
        "name": "Seq2Seq - åºåˆ—åˆ°åºåˆ—",
        "url": "https://arxiv.org/abs/1409.3215",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["doi", "arxiv_id"],
        "description": "åºåˆ—åˆ°åºåˆ—å­¦ä¹ è®ºæ–‡"
    },
    {
        "name": "Adamä¼˜åŒ–å™¨",
        "url": "https://arxiv.org/abs/1412.6980",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["arxiv_id"],
        "description": "Adamä¼˜åŒ–ç®—æ³•è®ºæ–‡"
    },
    # {
    #     "name": "æ·±åº¦å­¦ä¹ Natureç»¼è¿° - LeCun",
    #     "url": "https://www.nature.com/articles/nature14539",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi"],
    #     "description": "æ·±åº¦å­¦ä¹ Natureç»¼è¿°ï¼ŒLeCunç­‰äºº"
    # },
    # {
    #     "name": "ImageNetå¤§è§„æ¨¡è§†è§‰è¯†åˆ«",
    #     "url": "https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Russakovsky_ImageNet_Large_Scale_2015_CVPR_paper.html",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["title_match"],
    #     "description": "ImageNetæ•°æ®é›†å’Œç«èµ›çš„é‡è¦è®ºæ–‡"
    # },
    {
        "name": "YOLOç›®æ ‡æ£€æµ‹",
        "url": "https://arxiv.org/abs/1506.02640",
        "expected_processor": "Semantic Scholar",
        "expected_features": ["doi", "arxiv_id"],
        "description": "YOLOå®æ—¶ç›®æ ‡æ£€æµ‹ç®—æ³•"
    },
    {
        "name": "LSTM - é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ",
        "url": "https://www.bioinf.jku.at/publications/older/2604.pdf",
        "expected_processor": "Site Parser",
        "expected_features": ["pdf_url"],
        "description": "LSTMåŸå§‹è®ºæ–‡ï¼Œ1997å¹´ç»å…¸"
    },
    # {
    #     "name": "imagenet",
    #     "url": "https://doi.org/10.1145/3065386",
    #     "expected_processor": "Semantic Scholar",
    #     "expected_features": ["doi"],
    #     "description": "ReLUæ¿€æ´»å‡½æ•°çš„æ·±å…¥ç ”ç©¶"
    # },
    # {
    #     "name": "ReLUæ¿€æ´»å‡½æ•°ç ”ç©¶",
    #     "url": "https://proceedings.mlr.press/v15/glorot11a.html",
    #     "expected_processor": "Site Parser",
    #     "expected_features": ["title_match"],
    #     "description": "ReLUæ¿€æ´»å‡½æ•°çš„æ·±å…¥ç ”ç©¶"
    # }
]

class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    def __init__(self, test_case: Dict[str, Any]):
        self.test_case = test_case
        self.success = False
        self.literature_id = None
        self.processor_used = None
        self.metadata_quality = None
        self.processing_time = None
        self.error_message = None
        self.raw_response = None
        self.analysis = {}

class ComprehensiveTester:
    """å…¨é¢æµ‹è¯•å™¨ç±»"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results: List[TestResult] = []
        self.session = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def test_single_url(self, test_case: Dict[str, Any]) -> TestResult:
        """æµ‹è¯•å•ä¸ªURL"""
        result = TestResult(test_case)
        url = test_case["url"]
        
        print(f"\nğŸ§ª æµ‹è¯•: {test_case['name']}")
        print(f"   URL: {url}")
        print(f"   é¢„æœŸå¤„ç†å™¨: {test_case['expected_processor']}")
        
        start_time = time.time()
        
        try:
            # å‘é€è§£æè¯·æ±‚
            async with self.session.post(
                f"{self.base_url}/api/resolve",
                json={"url": url},
                timeout=60
            ) as response:
                response_data = await response.json()
                
                if response.status == 202:
                    # å¼‚æ­¥ä»»åŠ¡å·²åˆ›å»ºï¼Œéœ€è¦è½®è¯¢çŠ¶æ€
                    task_id = response_data.get("task_id")
                    if not task_id:
                        result.error_message = "No task_id in 202 response"
                        print(f"   âŒ å¤±è´¥: {result.error_message}")
                        return result
                    
                    print(f"   â³ ä»»åŠ¡å·²åˆ›å»º: {task_id}, ç­‰å¾…å¤„ç†å®Œæˆ...")
                    
                    # è½®è¯¢ä»»åŠ¡çŠ¶æ€ç›´åˆ°å®Œæˆ
                    result = await self._poll_task_completion(result, task_id)
                    
                elif response.status == 200:
                    # åŒæ­¥å“åº”ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    result.success = True
                    result.raw_response = response_data
                    
                    if isinstance(response_data, dict):
                        result.literature_id = response_data.get("lid")
                        if result.literature_id:
                            result = await self._get_literature_details(result)
                    
                    print(f"   âœ… æˆåŠŸ: LID={result.literature_id}")
                else:
                    result.error_message = f"HTTP {response.status}: {response_data}"
                    print(f"   âŒ å¤±è´¥: {result.error_message}")
                    
        except Exception as e:
            result.error_message = str(e)
            print(f"   âŒ å¼‚å¸¸: {result.error_message}")
        
        result.processing_time = time.time() - start_time
        return result
    
    async def _poll_task_completion(self, result: TestResult, task_id: str, max_wait: int = 60) -> TestResult:
        """è½®è¯¢ä»»åŠ¡çŠ¶æ€ç›´åˆ°å®Œæˆ"""
        poll_start = time.time()
        
        while time.time() - poll_start < max_wait:
            try:
                async with self.session.get(
                    f"{self.base_url}/api/tasks/{task_id}"
                ) as response:
                    if response.status == 200:
                        task_status = await response.json()
                        status = task_status.get("status", "").lower()
                        
                        if status == "completed":
                            result_type = task_status.get("result_type")
                            result.literature_id = task_status.get("lid")
                            result.raw_response = task_status
                            result.success = True  # Mark as success regardless of type

                            if result_type == "duplicate":
                                print(f"   âœ… æˆåŠŸ (å‰¯æœ¬): LID={result.literature_id}")
                            else:
                                print(f"   âœ… æˆåŠŸ (åˆ›å»º): LID={result.literature_id}")

                            if result.literature_id:
                                result = await self._get_literature_details(result)
                            
                            return result
                            
                        elif status == "failed":
                            # ä»»åŠ¡å¤±è´¥
                            error_msg = task_status.get("error_message", "Unknown error")
                            result.error_message = f"Task failed: {error_msg}"
                            print(f"   âŒ ä»»åŠ¡å¤±è´¥: {error_msg}")
                            return result
                            
                        elif status in ["pending", "processing"]:
                            # ä»åœ¨å¤„ç†ä¸­ï¼Œç»§ç»­ç­‰å¾…
                            print(f"   â³ å¤„ç†ä¸­... ({status})")
                            await asyncio.sleep(2)
                            continue
                        else:
                            print(f"   âš ï¸  æœªçŸ¥çŠ¶æ€: {status}")
                            await asyncio.sleep(2)
                            continue
                    else:
                        try:
                            error_json = await response.json()
                            print(f"   âš ï¸  æŸ¥è¯¢çŠ¶æ€å¤±è´¥: HTTP {response.status}, å“åº”: {json.dumps(error_json, indent=2)}")
                            print(f"   âš ï¸  å®Œæ•´å“åº”: {await response.text()}")
                        except Exception:
                            error_text = await response.text()
                            print(f"   âš ï¸  æŸ¥è¯¢çŠ¶æ€å¤±è´¥: HTTP {response.status}, å“åº”: {error_text}")
                        await asyncio.sleep(2)
                        continue
                        
            except Exception as e:
                print(f"   âš ï¸  è½®è¯¢å¼‚å¸¸: {e}")
                await asyncio.sleep(2)
                continue
        
        # è¶…æ—¶
        result.error_message = f"Task timeout after {max_wait}s"
        print(f"   âŒ è¶…æ—¶: ä»»åŠ¡åœ¨{max_wait}ç§’å†…æœªå®Œæˆ")
        return result
    
    async def _get_literature_details(self, result: TestResult) -> TestResult:
        """è·å–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯å¹¶åˆ†æ"""
        if not result.literature_id:
            return result
            
        try:
            async with self.session.get(
                f"{self.base_url}/api/literatures/{result.literature_id}"
            ) as response:
                if response.status == 200:
                    details = await response.json()
                    result = self._analyze_literature_details(result, details)
                else:
                    result.analysis["detail_fetch_error"] = f"HTTP {response.status}"
                    
        except Exception as e:
            result.analysis["detail_fetch_error"] = str(e)
            
        return result
    
    def _analyze_literature_details(self, result: TestResult, details: Dict[str, Any]) -> TestResult:
        """åˆ†ææ–‡çŒ®è¯¦ç»†ä¿¡æ¯"""
        metadata = details.get("metadata", {})
        task_info = details.get("task_info", {})
        component_status = task_info.get("component_status", {})
        metadata_status = component_status.get("metadata", {})
        
        # æå–å¤„ç†å™¨ä¿¡æ¯
        result.processor_used = metadata_status.get("source", "Unknown")
        
        # è®¡ç®—å…ƒæ•°æ®è´¨é‡åˆ†æ•°
        result.metadata_quality = self._calculate_quality_score(metadata)
        
        # éªŒè¯é¢„æœŸç‰¹æ€§
        result.analysis = self._verify_expected_features(result.test_case, details)
        
        # æ‰“å°åˆ†æç»“æœ
        print(f"   ğŸ“Š å¤„ç†å™¨: {result.processor_used}")
        print(f"   ğŸ“Š è´¨é‡åˆ†æ•°: {result.metadata_quality}/100")
        print(f"   ğŸ“Š å¤„ç†æ—¶é—´: {result.processing_time:.2f}s")
        
        # éªŒè¯é¢„æœŸ
        expected_processor = result.test_case["expected_processor"]
        if result.processor_used == expected_processor:
            print(f"   âœ… å¤„ç†å™¨åŒ¹é…é¢„æœŸ: {expected_processor}")
        else:
            print(f"   âš ï¸  å¤„ç†å™¨ä¸åŒ¹é…: é¢„æœŸ{expected_processor}, å®é™…{result.processor_used}")
        
        return result
    
    def _calculate_quality_score(self, metadata: Dict[str, Any]) -> int:
        """è®¡ç®—å…ƒæ•°æ®è´¨é‡åˆ†æ•°"""
        score = 0
        
        # æ ‡é¢˜ (å¿…é¡»)
        title = metadata.get("title", "")
        if title and title != "Unknown Title":
            score += 25
        
        # ä½œè€…
        authors = metadata.get("authors", [])
        if authors:
            score += 20
        
        # å¹´ä»½
        year = metadata.get("year")
        if year and year > 1900:
            score += 15
        
        # æœŸåˆŠ/ä¼šè®®
        journal = metadata.get("journal", "")
        if journal:
            score += 15
        
        # æ‘˜è¦
        abstract = metadata.get("abstract", "")
        if abstract and len(abstract) > 50:
            score += 15
        
        # å…³é”®è¯
        keywords = metadata.get("keywords", [])
        if keywords:
            score += 10
        
        return min(score, 100)
    
    def _verify_expected_features(self, test_case: Dict[str, Any], details: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯é¢„æœŸç‰¹æ€§"""
        analysis = {}
        expected_features = test_case.get("expected_features", [])
        
        identifiers = details.get("identifiers", {})
        metadata = details.get("metadata", {})
        
        for feature in expected_features:
            if feature == "doi":
                analysis["has_doi"] = bool(identifiers.get("doi"))
            elif feature == "arxiv_id":
                analysis["has_arxiv_id"] = bool(identifiers.get("arxiv_id"))
            elif feature == "high_quality":
                analysis["is_high_quality"] = self._calculate_quality_score(metadata) >= 80
            elif feature == "title_match":
                analysis["title_available"] = bool(metadata.get("title") and metadata["title"] != "Unknown Title")
            elif feature == "no_doi":
                analysis["correctly_no_doi"] = not bool(identifiers.get("doi"))
            elif feature == "url_parsing":
                analysis["url_processed"] = bool(details.get("identifiers", {}).get("source_urls"))
        
        return analysis
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å…¨é¢æµ‹è¯•æ–°ç€‘å¸ƒæµæ¶æ„")
        print("=" * 60)
        
        start_time = time.time()
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        for test_case in TEST_CASES:
            result = await self.test_single_url(test_case)
            self.results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            await asyncio.sleep(1)
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = self._generate_report(total_time)
        
        return report
    
    def _generate_report(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r.success)
        
        processor_usage = {}
        quality_scores = []
        
        for result in self.results:
            if result.success:
                processor = result.processor_used or "Unknown"
                processor_usage[processor] = processor_usage.get(processor, 0) + 1
                if result.metadata_quality is not None:
                    quality_scores.append(result.metadata_quality)
        
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        report = {
            "summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "success_rate": f"{(successful_tests/total_tests)*100:.1f}%",
                "total_time": f"{total_time:.2f}s",
                "avg_quality_score": f"{avg_quality:.1f}/100"
            },
            "processor_usage": processor_usage,
            "detailed_results": []
        }
        
        # æ·»åŠ è¯¦ç»†ç»“æœ
        for result in self.results:
            detailed = {
                "test_name": result.test_case["name"],
                "url": result.test_case["url"],
                "success": result.success,
                "lid": result.literature_id,
                "processor_used": result.processor_used,
                "metadata_quality": result.metadata_quality,
                "processing_time": f"{result.processing_time:.2f}s" if result.processing_time else None,
                "error_message": result.error_message,
                "analysis": result.analysis
            }
            report["detailed_results"].append(detailed)
        
        return report
    
    def print_report(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•æŠ¥å‘Šæ€»ç»“")
        print("=" * 60)
        
        summary = report["summary"]
        print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"   æˆåŠŸæµ‹è¯•: {summary['successful_tests']}")
        print(f"   æˆåŠŸç‡: {summary['success_rate']}")
        print(f"   æ€»è€—æ—¶: {summary['total_time']}")
        print(f"   å¹³å‡è´¨é‡åˆ†æ•°: {summary['avg_quality_score']}")
        
        print(f"\nğŸ”§ å¤„ç†å™¨ä½¿ç”¨ç»Ÿè®¡:")
        for processor, count in report["processor_usage"].items():
            print(f"   {processor}: {count}æ¬¡")
        
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for result in report["detailed_results"]:
            status = "âœ…" if result["success"] else "âŒ"
            is_duplicate = result.get("raw_response", {}).get("result_type") == "duplicate"
            duplicate_marker = " (å‰¯æœ¬)" if is_duplicate else ""
            print(f"   {status} {result['test_name']}{duplicate_marker}")
            if result["success"]:
                print(f"      å¤„ç†å™¨: {result['processor_used']}")
                print(f"      è´¨é‡: {result['metadata_quality']}/100")
                print(f"      æ—¶é—´: {result['processing_time']}")
            else:
                print(f"      é”™è¯¯: {result['error_message']}")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Paper Parser æ–°ç€‘å¸ƒæµæ¶æ„å…¨é¢æµ‹è¯•")
    print("=" * 60)
    
    async with ComprehensiveTester() as tester:
        report = await tester.run_all_tests()
        tester.print_report(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"test_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

if __name__ == "__main__":
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    if sys.version_info < (3, 7):
        print("âŒ éœ€è¦Python 3.7æˆ–æ›´é«˜ç‰ˆæœ¬")
        sys.exit(1)
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)
